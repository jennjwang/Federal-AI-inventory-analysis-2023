# Summarized GPT Text for Project Descriptions

- [Department_of_Energy (178)](#Department_of_Energy)
- [Department_of_Health_and_Human_Services (157)](#Department_of_Health_and_Human_Services)
- [Department_of_Commerce (49)](#Department_of_Commerce)
- [Department_of_Homeland_Security (41)](#Department_of_Homeland_Security)
- [Department_of_Veterans_Affairs (40)](#Department_of_Veterans_Affairs)
- [Department_of_Agriculture (39)](#Department_of_Agriculture)
- [Department_of_Interior (38)](#Department_of_Interior)
- [National_Aeronautics_and_Space_Administration (33)](#National_Aeronautics_and_Space_Administration)
- [Department_of_State (31)](#Department_of_State)
- [Department_of_Labor (18)](#Department_of_Labor)
- [U.S._Agency_for_International_Development (14)](#U.S._Agency_for_International_Development)
- [Social_Security_Administration (14)](#Social_Security_Administration)
- [Department_of_Treasury (14)](#Department_of_Treasury)
- [Department_of_Transportation (14)](#Department_of_Transportation)
- [U.S._General_Services_Administration (12)](#U.S._General_Services_Administration)
- [National_Archives_and_Records_Administration (5)](#National_Archives_and_Records_Administration)
- [U.S._Office_of_Personnel_Management (4)](#U.S._Office_of_Personnel_Management)
- [Department_of_Justice (4)](#Department_of_Justice)
- [U.S._Environmental_Protection_Agency (3)](#U.S._Environmental_Protection_Agency)
- [Department_of_Housing_and_Urban_Development (1)](#Department_of_Housing_and_Urban_Development)
- [Department_of_Education (1)](#Department_of_Education)

## Department_of_Energy

**Machine Learning for Autonomous Control of Scientific User Facilities**, _Department_of_Energy | Brookhaven National Laboratory_
> BNL and SLAC will collaborate to integrate machine learning algorithms into NSLS-II Operations at scientific user facilities. The goal is to enable intelligent interpretation of accelerator data by training the algorithms with archived device data, records of previous fault causes, and stored beam current spanning over 5 years.

**Automated sorting of high repetition rate coherent diffraction data from XFELS**, _Department_of_Energy | Brookhaven National Laboratory_
> The text discusses the use of automated sorting techniques for coherent diffraction data obtained from synchrotron and X-ray Free-electron Laser Sources. These sources produce interference patterns called "speckles" when diffracted from a crystal containing defects. By analyzing the correlations between the diffraction patterns, algorithms can separate the genuine fluctuations of the material sample from the inherent beam fluctuations, allowing for more accurate data analysis and image resolutions.

**SMMM**, _Department_of_Energy | Brookhaven National Laboratory_
> AI/ML technology is currently being employed to assess real-time measurements during concurrent experiments on two beamlines. It then utilizes this information to optimize data collection on both beamlines, effectively maximizing the scientific output within a given timeframe.

**Streamining intelligent detectors for sPHENIX/EIC**, _Department_of_Energy | Fermi National Accelerator_
> This project focuses on creating real-time algorithms for event filtering in nuclear physics collider experiments using tracking detectors. The goal is to improve the efficiency and accuracy of detection by developing intelligent detectors for sPHENIX/EIC.

**Extreme data reduction for the edge**, _Department_of_Energy | Fermi National Accelerator_
> This project aims to create artificial intelligence algorithms and tools that can efficiently reduce data near the sensor in customized hardware for edge devices. The goal is to enable extreme data reduction for improved performance and efficiency.

**READS: Real-time Edge AI for Distributed Systems**, _Department_of_Energy | Fermi National Accelerator_
> The project aims to create and implement fast-response controls and predictive algorithms for the Fermilab accelerator complex. These measures will be deployed in real-time to enhance the efficiency and performance of the distributed systems.

**Simulation-based inference for cosmology**, _Department_of_Energy | Fermi National Accelerator_
> The project aims to develop and utilize simulation-based inference to estimate cosmological parameters concerning cosmic acceleration in the early and late universe. By analyzing the cosmic microwave background and strong gravitational lensing, an analysis pipeline will be created that can be applied to future cosmic surveys.

**hls4ml**, _Department_of_Energy | Fermi National Accelerator_
> The hls4ml project is focused on creating tools for hardware-software AI codesign on FPGAs and ASICs. These tools are designed specifically for algorithms that operate at the extreme edge.

**In-storage computing for multi- messenger astronomy in neutrino experiments and cosmological surveys**, _Department_of_Energy | Fermi National Accelerator_
> This project focuses on solving the challenges of big-data and time constraints in multi-messenger astronomy in neutrino experiments and cosmological surveys. It proposes a new approach where computation is embedded in the data itself, allowing for faster processing and prompt dissemination of alerts through the use of computational storage accelerators and machine learning algorithms.

**SONIC: AI acceleration as a service**, _Department_of_Energy | Fermi National Accelerator_
> The SONIC project aims to integrate AI hardware to accelerate the inference process for particle physics experiments. It aims to provide AI acceleration as a service, focusing on at-scale implementation for efficient and faster results.

**AI Denoising**, _Department_of_Energy | Fermi National Accelerator_
> The AI Denoising program focuses on creating generative models that can efficiently simulate particle showers in calorimeters used for LHC experiments. Its objective is to provide a faster and more accurate method for simulating these showers using artificial intelligence technology.

**In-pixel AI for future tracking detectors**, _Department_of_Energy | Fermi National Accelerator_
> This project aims to develop AI-on-chip technology for advanced tracking detectors. The focus is on creating intelligent detectors that are equipped with sensing capabilities.

**High-Velocity AI: Generative Models**, _Department_of_Energy | Fermi National Accelerator_
> This project aims to enhance the robustness of AI classification algorithms in astrophysics/cosmology applications by generating adversarial examples and implementing domain adaptation techniques. Additionally, it aims to utilize AI algorithms to enhance the output of low-quality classical simulation engines, enabling high-quality results at a fast pace.

**Next-Generation Beam Cooling and Control with Optical Stochastic Cooling**, _Department_of_Energy | Fermi National Accelerator_
> The program aims to use optical stochastic cooling (OSC) to improve beam control and sensing in colliders and accelerator facilities. By implementing a new OSC system at IOTA, the program plans to achieve turn-by-turn programmability and establish a new standard in beam cooling. This will provide a flexible set of tools for reinforcement learning methods and enhance overall beam control capabilities.

**Machine Learning for Linac Improved Performance**, _Department_of_Energy | Fermi National Accelerator_
> The current emittance optimization procedure in Linacs at FNAL and J-PARC is limited to manual adjustments of a few parameters, which is not feasible for a human operator to handle a larger number of parameters. By incorporating machine learning techniques, this limitation can be overcome, allowing for a greater number of parameters to be adjusted. The objective is to integrate machine learning into linac operation, specifically RF control, in order to achieve improved longitudinal emittance and reduce overall losses.

**Uncertainty Quantification and Instrument Automation to enable next generation cosmological discoveries**, _Department_of_Energy | Fermi National Accelerator_
> This project aims to develop AI-based tools to facilitate advancements in cosmological discoveries. It emphasizes the significance of uncertainty quantification and simulation-based inference for discovery science. Additionally, the project highlights the importance of automated design and control of instrumentation to enhance the efficiency of planning and executing cosmic experiments.

**Machine Learning for Accelerator Operations Using Big Data Analytics / L-CAPE**, _Department_of_Energy | Fermi National Accelerator_
> The use of big data analytics and machine learning in the L-CAPE project at Fermilab LINAC allows for the prediction and classification of anomalies, leading to automatic mitigation, cost savings, and predictive maintenance in accelerator operations. This technology enables operational efficiency and helps in avoiding potential issues in advance.

**Deep Learning Malware Analysis for reusable cyber defenses.**, _Department_of_Energy | Idaho National Laboratory_
> The INL utilizes a feed forward neural network to apply deep learning techniques on a vast dataset of translated malware binaries represented in graph structures. This approach enables the identification of shared characteristics among malware, facilitating the development of reusable cyber defenses.

**Geo Threat Observable for structure cyber threat related to the energy sector**, _Department_of_Energy | Idaho National Laboratory_
> The Geo Threat Observable is a collection of open source threat information specifically focused on cyber issues in the energy sector. This data is stored in a graph database and utilized in machine learning to identify similarities in threats, improving the effectiveness of cyber protections in the industry.

**AI/ML and other software advances**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> The text discusses research in model architecture development for AI/ML and other software advances. This research focuses on improving workflows, optimizing algorithms, and enhancing performance.

**Advanced energy, batteries, and industrial efficiency**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> Data science is being used to improve battery design and energy storage, as well as to scale up various technologies in advanced energy and industrial efficiency. This involves analyzing data to navigate the design space and make advancements in battery technology and energy storage.

**Quantum computing and information systems**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> Quantum computing and information systems utilize machine learning and quantum computing techniques to enhance various fields such as optimization, quantum chemistry, material science, and cryptography. These advanced technologies have the potential to revolutionize these domains by providing more efficient and secure solutions.

**Biology, genomics, and synthetic biology**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> This text discusses the integration of experimental and computational methods in the fields of biology, genomics, and synthetic biology. These methods are used to conduct research in various areas such as genomics, molecular toxicology, nanotechnology, host-pathogen biology, structural biology, genetics, microbial systems, and medical countermeasures.

**High-performance computing, semiconductors, and advanced computer hardware**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> The text discusses the need for novel computer hardware architecture and configurations that can function effectively in high-performance computing, semiconductors, and harsh environments. These advancements aim to enable computers to operate efficiently at the edge and withstand challenging conditions.

**Cyber security, data storage, and data management technologies**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> The text highlights the importance of cyber security and data management technologies in processing and storing large datasets. It emphasizes the need for data-processing pipelines and user interfaces that can handle unstructured data efficiently, while ensuring a secure environment for searching, exporting, and analyzing the data.

**Innovation methods, processes and promising practices that can affect the speed and effectiveness of innovation processes at scale.**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> This text discusses the importance of innovation methods, processes, and practices in achieving faster and more effective innovation at a larger scale. It emphasizes the use of computational approaches to gain quicker insights into the development and implementation of large-scale operations.

**Advanced materials science, engineering, and exploration relevant to the other key technology focus areas**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> The text states that advanced materials science, engineering, and exploration are crucial for enabling machine learning technology to be applied to specialized materials. This will result in improved performance for scientific research and manufacturing systems.

**Natural and anthropogenic disaster prevention and mitigation**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> By utilizing a diverse range of data, both natural and human-caused disasters can be predicted and comprehended, aiding in their prevention and reduction. This approach involves gathering various types of information to anticipate and analyze potential natural disaster situations, with the aim of minimizing their impact.

**Robotics, automation, and advanced manufacturing**, _Department_of_Energy | Lawrence Livermore National Laboratory_
> AI is utilized in robotics, automation, and advanced manufacturing to enhance hardware development and analyze sensor data for increased process reliability.

**To develop a general drag model for assemblies of non-spherical particles created with artificial neural networks**, _Department_of_Energy | National Energy Technology Laboratory_
> The project aims to develop an artificial neural network (ANN) method to model the momentum exchange in fluid-solid mixtures of non-spherical particles. This will improve the accuracy and reduce uncertainty in numerical codes, particularly in the MFiX code, by providing a reliable method for determining drag coefficients for various Reynolds numbers, Stokes numbers, and fluid-solid properties. The research team will use validated CFD code and artificial intelligence techniques to develop the ANN, which will be implemented in TensorFlow and linked with the MFiX code.

**To fill critical data gaps in big data analytics and machine learning applications to inform decision making and improve the ultimate recovery of unconventional oil and natural gas resources.**, _Department_of_Energy | National Energy Technology Laboratory_
> This project aims to enhance big data analytics and machine learning applications by addressing data gaps in the extraction of unconventional oil and natural gas resources. It involves conducting numerical analysis and using all-digital pressure sensing technology to generate a synthetic dataset for each stage, which will be statistically analyzed and integrated with software through data analytics.

**Computational methods for the characterization of CO2 chemisorption in amine- functionalized MOFs.**, _Department_of_Energy | National Energy Technology Laboratory_
> Researchers will use computational methods to analyze databases of metal-organic frameworks (MOFs) and identify potential MOFs that are amine-functionalized. They will also develop software to add amine functional groups to the metal centers and/or ligands of MOFs. The team will calculate the reaction enthalpy for CO2 sorption in amine-functionalized MOFs and develop additional computational methods for characterizing CO2 chemisorption in these MOFs.

**Use ML to reduce high-fidelity physical models to a fast calculation that requires minimal effort to initiate.**, _Department_of_Energy | National Energy Technology Laboratory_
> This text describes the use of machine learning (ML) to simplify and speed up the evaluation of reservoir dynamics and processes. By using ML, experts can quickly generate calculations that require minimal effort to initiate, allowing everyday users with a moderate understanding of the physics to investigate their own scenarios. This platform will be particularly useful for CO2 storage projects, where field experience is limited, as it provides real-time translation of subsurface dynamics.

**Using ML to design sensing materials which can work under harsh environments.**, _Department_of_Energy | National Energy Technology Laboratory_
> The team plans to use ML and established evidence to create a comprehensive ML model for designing gas sensing materials that can withstand harsh environments. This approach involves evaluating materials' performance under specific conditions and predicting temperature dependence of sensing properties to screen existing materials or propose new ones for high-temperature applications.

**To develop drag models for non- spherical particles through machine learning**, _Department_of_Energy | National Energy Technology Laboratory_
> In order to create accurate drag models for non-spherical particles using machine learning, comprehensive experimental and numerical datasets need to be created to understand the aerodynamic drag in dense gas-solid flows. These datasets and the knowledge gained from them will be used to train deep neural networks, which will then be used to develop a general drag model for the NETL MFiX-DEM module. This will improve the accuracy and prediction ability of computational tools used in designing and optimizing fluidized beds and chemical looping reactors.

**To develop an AI-driven integrated autonomous robotic visual inspection (RVI) platform.**, _Department_of_Energy | National Energy Technology Laboratory_
> research aims to create a platform that uses AI technology to enable a robot to autonomously inspect and identify defects in real-time. This platform will also be able to plan its own path and navigate safely in a closed-loop system.

**To leverage ML models to increase the size and complexity of problems that can be optimized within IDAES.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal is to use ML models as substitutes for complex unit operations or to connect different scales in order to expand the range of problems that can be optimized in IDAES. This would allow for larger and more complex models to be handled within IDAES.

**Machine Learning for geophysical data inversion**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning is utilized in geophysical data inversion to create synthetic seismic and gravity data. Additionally, data-driven inversion techniques are employed for the purpose of leak detection.

**To develop and create an autonomous robotic inspection system.**, _Department_of_Energy | National Energy Technology Laboratory_
> The purpose of this project is to create a robotic inspection system that can autonomously monitor coal waste storage facilities. The first objective is to develop a drone that can inspect different structures within the facility. The second objective is to use artificial intelligence and various sensors to detect potential hazards in the storage facility that could lead to environmental damage or structural failure.

**To implement boiler health monitoring using a hybrid first principles-artificial intelligence model**, _Department_of_Energy | National Energy Technology Laboratory_
> The text outlines the steps needed to implement a hybrid first principles-artificial intelligence model for monitoring the health of a boiler. This includes developing algorithms and methodologies for creating the model, using a physics-based approach to assess material damage, and creating an online health-monitoring framework that combines the hybrid models and plant measurements to track key variables and measures for the boiler's health.

**To drive insights on the dependencies between the natural gas and electricity sectors to increase reliability of the NG system**, _Department_of_Energy | National Energy Technology Laboratory_
> Commercially available models will be utilized to create predictive scenarios that will enhance the understanding of the interconnections between the natural gas and electricity sectors. This will ultimately improve the reliability of the natural gas system.

**Geochemically Informed Leak Detection (GILD)**, _Department_of_Energy | National Energy Technology Laboratory_
> A Bayesian Belief Network called Geochemically Informed Leak Detection (GILD) has been created to analyze changes in geochemistry near a possible CO2 leakage area. By utilizing this network and site-specific data, the method can effectively decrease the occurrence of false positives.

**Creation of polymer datasets and inverse design of polymers with targeted backbones having High CO2 permeability and high CO2/N2 selectivity.**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning models were created to accurately forecast the CO2 permeability and CO2/N2 selectivity of polymers. Additionally, innovative approaches were developed to generate polymer datasets and a novel machine learning technique is currently being developed to design polymers with specific properties.

**To implement novel SSC-CCS sensing technology and associated condition-based monitoring (CBM) software for improved understanding of the boiler tube failure mechanisms**, _Department_of_Energy | National Energy Technology Laboratory_
> A condition-based monitoring (CBM) package with a user interface (GUI) will be created to monitor the temperature profiles of boiler tubes. This system will integrate with EPRI's Boiler Failure Reduction Program to assess the health of the tubes, provide warnings and diagnoses for potential failures, and offer suggestions for maintenance.

**To design, proto-type and demonstrate a miniaturized implementation of a multi-process, high-spatial-resolution monitoring system for boiler condition management.**, _Department_of_Energy | National Energy Technology Laboratory_
> The project aims to create a smaller version of a monitoring system for managing the condition of a boiler. It will develop control logic to automate the control of a coal-fired boiler and compare operational data with monitoring data to identify damaging conditions and suggest operational changes. The focus will be on automating soot-blowing and other boiler operations.

**Advanced model to forecast offshore landslide risks and marine geohazards**, _Department_of_Energy | National Energy Technology Laboratory_
> The research aims to develop an advanced model that can accurately predict offshore landslide risks and other marine geohazards. This will be achieved by utilizing intelligent databases, artificial intelligence, big data, and other advanced computing technologies to address various challenges related to subsurface natural-engineered systems in offshore environments.

**To develop artificial intelligence- enabled tools (ArtIT) for cyber hardening of power grids.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal is to create artificial intelligence tools that can strengthen power grids against cyberattacks. This will be achieved by combining various theories and technologies, such as closed-loop controls, security measures, machine learning, and laboratory demonstrations, to develop a resilient framework for wide-area control operations.

**To drive insights on pipeline maintenance and repair strategies to reduce incidents of pipeline leakage; support evaluation of use and reuse strategies**, _Department_of_Energy | National Energy Technology Laboratory_
> A geospatial model powered by machine learning will be developed to assess the risks associated with pipeline maintenance and repair, aiming to minimize incidents of leakage. Additionally, the model will aid in evaluating the potential for using and reusing pipelines as part of sustainability strategies.

**Advanced Image Segmentation**, _Department_of_Energy | National Energy Technology Laboratory_
> The U-Net CNN segmentation technique is used to separate and isolate pores and fluids in computed tomography scans of multiphase transport in cores. This advanced image segmentation method improves the accuracy of analyzing and understanding the structure and behavior of the transport system.

**To evaluate current infrastructure throughout a study area and evaluating future infrastructure needs to accelerate the deployment of CCUS**, _Department_of_Energy | National Energy Technology Laboratory_
> The text states that one of the main tasks is to evaluate current infrastructure and future infrastructure needs in order to speed up the deployment of CCUS (carbon capture, utilization, and storage). LANL will primarily use SimCCS technology for this project, while also providing some minor consulting using NRAP and machine learning algorithms.

**To develop and demonstrate drone- based geophysical and remote- sensing technologies to quantify critical minerals (CM).**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal of this project is to create and showcase drone-based technologies that can accurately measure critical minerals in various energy sources and waste streams. By using artificial intelligence and machine learning, these technologies have the potential to greatly improve the characterization and monitoring of critical minerals from both conventional and secondary sources.

**Initial case study using regulatory compliance (well integrity testing, fluid compositionali data, geographic, and geologic information from oil and gas wells in the Wattenberg Field, Denver Basin, central Colorado, USA**, _Department_of_Energy | National Energy Technology Laboratory_
> Researchers will use AI/ML techniques to analyze well integrity testing, fluid compositional data, and geographical/geological information from oil and gas wells in the Wattenberg Field, Denver Basin, central Colorado, USA. Their goal is to gain new insights into potential leakage issues at a national scale.

**Data platform to expedite access and reuse of carbon ore data for materials, manufacturing and research**, _Department_of_Energy | National Energy Technology Laboratory_
> A data platform has been developed to quickly and efficiently access and reuse carbon ore data for various purposes such as materials, manufacturing, and research. This platform utilizes data science and natural language processing (NLP) techniques and is hosted in a virtual, multi-cloud environment to enable online analytics.

**Machine learning to refine and analyze data for CCS needs**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning techniques are used to analyze and refine data for CCS (carbon capture and storage) requirements. These techniques generate new data products that assist in various areas such as resource evaluation, risk assessment, supply chain analysis, social and environmental justice evaluations, and regulatory compliance.

**Use ML to analyze the existing H2 and natural gas pipelines to identify the key parameters that can enable the H2 transport and storage at a large scale**, _Department_of_Energy | National Energy Technology Laboratory_
> The objective is to utilize machine learning to examine current hydrogen (H2) and natural gas pipelines in order to determine the crucial factors that can facilitate the large-scale transportation and storage of H2. By using methods from geo-data science and geospatial information science, this analysis aims to identify these key parameters and provide insights to support real-time pipeline monitoring, optimized sensor deployment, and the reduction of methane emissions for efficient maintenance.

**AI/ML methodology for rapid design of sorbents tuned to specific ash impoundment and/or landfill requirements.**, _Department_of_Energy | National Energy Technology Laboratory_
> This text outlines the AI/ML methodology for quickly designing sorbents that meet specific requirements for ash impoundments or landfills. The process involves computing descriptors, creating a machine-learned model to predict sorption, optimizing and designing the sorbent for maximum sorption, and generating a force field for an additional pollutant if necessary. The methodology also includes sorption calculations and optimization for the second pollutant based on its concentration in the aqueous solution.

**Machine learning based identification of current hazardous offshore metocean and bathymetric conditions that can impact safe offshore energy operations**, _Department_of_Energy | National Energy Technology Laboratory_
> The text discusses the development of Version 2 of the OGA smart tool, which uses machine learning to identify hazardous offshore conditions that can impact safe offshore energy operations. It mentions the refinement of offshore hazard models, the drafting of manuscripts detailing the tool's models and algorithms, and the assembly of a metocean and seafloor database for release with the tool. Additionally, it suggests strategizing web-hosted versions of the tool and database.

**To develop and validate sensor hardware and analytical algorithms to lower plant operating expenses for the pulverized coal utility boiler fleet**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal is to develop and test sensor hardware and algorithms to decrease operating expenses for coal utility boilers. This will be done using affordable "Internet of Things" technologies. Three specific technologies will be explored, including temperature control, monitoring coal pulverizer operation, and improving air and fuel balance.

**To verify and validate testing of advanced power generation technologies**, _Department_of_Energy | National Energy Technology Laboratory_
> Verification and validation testing of advanced power generation technologies is conducted with the assistance of operating power plants and their systems. This collaboration allows for the use of real-time data, leading to informed plant operators, minimized disruptions, and the ability to meet changing service demands.

**To provide an effective quality assurance method for additively manufactured gas**, _Department_of_Energy | National Energy Technology Laboratory_
> This project aims to develop a cost-effective quality assurance method for gas turbine components produced using laser powder bed fusion. The method will involve using machine learning algorithms to analyze data from in-situ monitoring, ex-situ characterization, and simulation to predict porosity and fatigue properties of the components. The goal is to create a rapid QA tool that can qualify additively manufactured turbine components and provide a process-structure-property database for future use.

**Use of machine learning models to produce surrogates for efficient optimization**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning models can be used to create surrogates that enhance the efficiency of optimization processes. The implementation of this approach, known as IDAES, will be showcased through a practical application related to carbon capture or an integrated energy system.

**AI used to interpret sensor data.**, _Department_of_Energy | National Energy Technology Laboratory_
> AI is currently being utilized to interpret sensor data by employing an algorithm trained with various known sensor conditions. This allows for the automatic classification of sensor data into probable gas concentrations.

**To deploy dynamic neural network optimization to minimize heat rate during ramping for coal.**, _Department_of_Energy | National Energy Technology Laboratory_
> The proposed work aims to use dynamic neural network optimization (D-NNO) to minimize heat rate during all phases of operation at a coal power plant. This will be achieved by developing a dynamic model of the plant to better understand its behavior during ramping and different load conditions, and then using machine learning to fit models to the plant's response data. The ultimate goal is to commercialize D-NNO as a readily-available software application by collaborating with an industry-proven software platform.

**With sensor technologies and network developed, in the future, AI/ML may be used to accelerate data processing of sensor data from the sensor network.**, _Department_of_Energy | National Energy Technology Laboratory_
> AI/ML can be utilized in the future to speed up the processing of sensor data from a sensor network. This can help in identifying and predicting risks and failures in plugged wells.

**To leverage disparate data to update assessments, analytics, and infromation for NATCARB and CS Atlas**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning is used to analyze and generate new data that can be used to update assessments and information for NATCARB and CS Atlas. This allows for the inclusion of data from various sources, enhancing the accuracy of geologic carbon storage assessments.

**To provide integrated boiler management through advanced condition monitoring and component assessment.**, _Department_of_Energy | National Energy Technology Laboratory_
> The Integrated Creep-Fatigue Management System is an online boiler damage monitoring system that can be connected to plant data historian and commercial finite element software. It includes damage mechanism monitoring modules and can calculate combined creep and oxidation mechanisms without the need for finite element analysis.

**To help automate data discovery and preparations to support a range of CS models, tools, and products**, _Department_of_Energy | National Energy Technology Laboratory_
> AI and ML technologies are employed to automate the collection and processing of data from multiple sources for various CS models, tools, and products. This integration and analysis of information helps in providing additional data and information to support different carbon storage tasks.

**To apply machine learning and data analytics techniques to integrated subsurface datasets to predict key reservoir properties and compare various fields across the area of study and to correlate vintage data with new data and address the distribution of fractures and vugs.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text describes a project that aims to use machine learning and data analytics to predict reservoir properties and compare different fields in a study area. Additionally, laboratory experiments will be conducted to optimize a CO2 flood composition for specific rock properties and improve oil recovery in carbonate formations. The results of the project will provide strategies for improving oil recovery in the Michigan Basin and other carbonate plays.

**ML-based proxy models and multi- level data driven fracture network imaging to support rapid decision making.**, _Department_of_Energy | National Energy Technology Laboratory_
> The use of ML-based proxy models and multi-level data driven fracture network imaging enhances decision making by providing visualizations of fracture network, production forecast, and well drainage volume. These models consider factors such as fracture network, HF geometry, HF properties, bottomhole pressure, and drainage volume to generate accurate predictions.

**Machine learning for legacy well evaluation**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning is utilized to detect patterns and attributes that are associated with well integrity problems in legacy wells. This enables the identification and prioritization of monitoring and remediation efforts for these issues.

**To use data analytics and machine learning techniques to advance understanding of the characteristics of the Emerging Paradox Oil Play**, _Department_of_Energy | National Energy Technology Laboratory_
> The text states that data analytics and machine learning techniques will be utilized to enhance knowledge about the characteristics of the Emerging Paradox Oil Play. The goal is to integrate geologic and log-derived models with 3D seismic data to gain a comprehensive understanding of the entire Paradox oil play.

**To provide insights into opportunities to beneficiate and use hydrocarbon infrastructure for alternative uses such as offshore carbon storage.**, _Department_of_Energy | National Energy Technology Laboratory_
> This text describes the use of AI and big data models to analyze various types of information related to hydrocarbon infrastructure, with the goal of identifying opportunities for alternative uses such as offshore carbon storage. The models will use techniques such as natural language processing, Artificial Neural Networks, and potentially bayesian networks to make predictions based on the data.

**Use AI to process large sensor datasets for identification and classification of NG pipeline conditions and methane leaks**, _Department_of_Energy | National Energy Technology Laboratory_
> This text highlights the use of artificial intelligence (AI) to analyze vast sensor datasets and identify and classify conditions and methane leaks in natural gas pipelines. The focus is on developing advanced data analytic techniques, including AI and machine learning (ML), to identify patterns and signatures that represent hazards, defects, and operational parameters of the pipeline network.

**To develop 5G integrated edge computing platform for efficient component monitoring in coal-fired power plants**, _Department_of_Energy | National Energy Technology Laboratory_
> A 5G integrated edge computing platform will be developed to monitor components in coal-fired power plants. This platform will gather and analyze component health data efficiently using on-demand distributed edge computing. The focus will be on investigating the efficiency of edge computing service in terms of dynamic orchestration, resource data collection, and health information monitoring, in order to detect remote faults and perform timely diagnosis.

**To perform reconstruction of the 3D temperature field using Neural Networks with measured and known propagation paths.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text describes the process of reconstructing a 3D temperature field using Neural Networks. The sensor used for testing will be subjected to temperatures up to 300C initially, and then up to 1,800C in high-temperature tests. The results will be compared with data from other methods, and a 3D temperature mapping will be created using the sensor data. Additionally, the system's survivability will be tested in a boiler environment, and a high-temperature sensing array will be used to map temperature distribution within an exhaust pipe.

**To create reduced order models for predicting long term performance degradation behavior of fuel cells and electrolyzers.**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning algorithms are being utilized to analyze extensive datasets of microstructural and performance degradation simulations for different electrode microstructures. This analysis aims to develop reduced order models that can accurately predict long-term performance degradation of fuel cells and electrolyzers, particularly those used in large-scale applications. These reduced order models enable dynamic simulations that effectively replicate the changing loading conditions of the contemporary power grid.

**Development of AI/ML methods**, _Department_of_Energy | National Energy Technology Laboratory_
> The text discusses the development of AI/ML methods for SMART software. This includes the establishment of quality, reliability, and version control standards. Additionally, the text mentions the development of AI/ML methods for activities 2A and 2C, such as modeling anomalies with a CRM and BBN modeling integrated with geochemistry. Advanced computational approaches using NVIDIA's PINN tool and Cerebras Systems Inc.'s WSE are also being developed to accelerate training.

**To help automate data integration and exploration for geologic core properties related information.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text discusses the use of various technologies, including natural language processing, deep learning neural networks, and potentially tensor flow for image analytics, to automate data integration and exploration for geologic core properties related information. These technologies aim to make the process more efficient and accurate.

**Using AI to improve predcitions of subsurface properties, analyze multi- variate inputs, address knowledge and information gaps to improve predictions and modeli**, _Department_of_Energy | National Energy Technology Laboratory_
> AI methods, like fuzzy logic, neural networks, tensor flow, and natural language processing, can be used to enhance predictions of subsurface properties. These methods can analyze multiple inputs, fill in knowledge gaps, and improve modeling and analysis of multi-variate data, ultimately leading to improved predictions and outputs in resource assessment.

**Fluid migration from well-to-well communication will be inputted in AI to determine a costs-benefit analysis**, _Department_of_Energy | National Energy Technology Laboratory_
> An AI system will be used to analyze fluid migration between wells and determine the costs and benefits. The project aims to develop a machine learning algorithm that can predict when a growing fracture will reach a monitored well, based on the distinctive tensile strain signature that precedes it. This algorithm will work alongside an existing fracture warning system to provide early detection of well-to-well communication, predict the depths at which it will occur, and estimate the time until it begins.

**DOE AI Data Infrastructure System**, _Department_of_Energy | National Energy Technology Laboratory_
> The DOE AI Data Infrastructure System aims to enhance the user experience and connectivity of CCS stakeholders by utilizing generative AI and cloud-based data infrastructure. The system creates an adaptive user interface that efficiently connects users to the information that is relevant to them.

**Use ML to enable a geophysical monitoring toolkit, and assimilate real-time modeling and data.**, _Department_of_Energy | National Energy Technology Laboratory_
> Using machine learning, a geophysical monitoring toolkit can be created that incorporates real-time modeling and data assimilation. This toolkit, along with visualization and decision-support frameworks, can greatly enhance pressure and stress imaging.

**Development of new machine learning-based process modeling capabilities that assess the viability and efficiency, with uncertainty quantification, of the chemical processes involved in the carbon fiber production and its output quality**, _Department_of_Energy | National Energy Technology Laboratory_
> The text describes the development of machine learning-based process modeling capabilities to assess the effectiveness and uncertainty of chemical processes in carbon fiber production. This includes verifying lab-scale experiments for producing coal-tar pitch (CTP) and creating a database and tools for analyzing data and economic modeling. The goal is to determine the relationship between process conditions and product quality, as well as assess the economic viability of using different coal regions for high-value product production.

**To apply machine learning methods to explore the inter-well uncertainty in the Goldsmith Landreth San Andres Unit and to update reservoir models.**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning methods will be used to analyze and update reservoir models in the Goldsmith Landreth San Andres Unit. The proposal aims to test the effectiveness of engineered water in reducing interfacial tension and minimizing capillary forces to prevent oil from being pushed out of the matrix.

**To develop high fidelity tools which run in near real time not only help in the field to guide and optimize complex operations but can be used as digital twins**, _Department_of_Energy | National Energy Technology Laboratory_
> High fidelity tools that run in near real time are beneficial in guiding and optimizing complex operations in the field. Additionally, these tools can serve as digital twins for cyber security and cyber-physical modeling.

**To develop innovative biomonitoring and remediation of heavy metals using phytotechnologies.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal of this project is to use algal and cyanobacterial phycotechnologies to tackle heavy metal pollution caused by coal combustion. The project aims to develop new bioindicators that can assess the potential of phytoremediation to restore contaminated sites.

**To realize next generation solid-state power substation.**, _Department_of_Energy | National Energy Technology Laboratory_
> The proposed project aims to create a next generation solid-state power substation that incorporates machine learning, cyber-physical anomaly detection, and multi-agent distributed networked control. The project will enable distributed control and coordination, autonomous control for plug-and-play, automatic reconfiguration, recovery, and restoration, allowing for decoupled, asynchronous, and fractal systems.

**Transform reservoir management decisions through rapid analysis of real time data to visualize forecasted behavior in an advanced control room "human-in-the-loop" format.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text suggests that reservoir management decisions can be improved by analyzing real-time data and visualizing forecasted behavior in an advanced control room format. It also proposes enhancing low-fidelity model performance by using high-fidelity data through transfer-learning and reducing uncertainty by combining high-fidelity and lower-fidelity models for better performance in uncertainty quantification.

**Natural Language Processing**, _Department_of_Energy | National Energy Technology Laboratory_
> The text states that information and articles on energy storage will be collected and analyzed using natural language processing algorithms. These algorithms will help categorize and understand different energy storage efforts in research and development communities, and trends within the selected focus areas will also be examined. Overall, this approach will provide an unbiased and comprehensive view of energy storage R&D beyond known search terms.

**Machine learning to tool and model applications for CCS needs**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning is used to develop and analyze models for CCS applications. It helps optimize CCS infrastructure, visualize data, and interact with large datasets within the EDX multi-cloud ecosystem.

**To create a data-driven multiscale phytotechnology framework for identification and remediation of leached-metals-contaminated soil.**, _Department_of_Energy | National Energy Technology Laboratory_
> The objective of this project is to develop a framework that uses data and technology to identify and remediate soil contaminated with leached metals. This will be achieved by combining satellite remote sensing, machine learning, image processing, geological engineering models, soil science, and plant pathology to identify potential metal leaching and propose effective phytoextraction methods for remediation.

**To drive insights on environmental performance of the natural gas system to inform effective mitigation strategies**, _Department_of_Energy | National Energy Technology Laboratory_
> Life Cycle Analysis models will be utilized to determine and evaluate the environmental aspects and performance of the natural gas system. This information will be crucial in developing successful strategies to mitigate any negative impacts.

**To use ML to help identify promising oxygen carrier materials.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text explains that a machine learning model will be developed using a combination of experimental data and computational results to identify promising oxygen carrier materials. The model will evaluate the compositions based on performance and synthesizability, and the materials identified will be synthesized and compared to existing carriers to refine the model.

**To drive insights on emissions from natural gas production, storage, and transmission to determine how best to reduce emissions**, _Department_of_Energy | National Energy Technology Laboratory_
> Artificial intelligence and machine learning techniques will be utilized to analyze well integrity records in order to identify patterns that can predict potential failure events in natural gas production, storage, and transmission. This approach aims to provide valuable insights and guidance on reducing emissions in these processes.

**To design, develop, and demonstrate an AI-integrated physics-based attack resilient proactive system.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal of the project is to create a proactive system that uses artificial intelligence to enhance the security and resiliency of cyber-physical systems in the distribution grid. This system will be designed to work with various vendors and will be able to manage and protect advanced distribution management and DER management systems.

**To expore and analtze hydrogen- fueled rotating detonation engines using advanced turbulent combustion modeling and high- fidelity simultion tools.**, _Department_of_Energy | National Energy Technology Laboratory_
> This text outlines four main objectives: analyzing the effects of injector design on parasitic combustion in hydrogen-fueled rotating detonation engines (RDEs), understanding the impact of RDE ignition mechanisms and initial transients on the behavior of detonation waves, deploying and assessing machine learning assisted turbulent combustion models for predictive and efficient RDE simulations, and developing a scalable high-order computational fluid dynamics (CFD) modeling framework for simulating full-scale RDEs and investigating the effects of turbulence, combustion, and wall boundary layers.

**ANN Submodels of Reaction Physics**, _Department_of_Energy | National Energy Technology Laboratory_
> The text discusses the development of Artificial Neural Network (ANN) submodels for reaction physics in order to accelerate code. These submodels aim to improve the understanding and prediction of flow physics.

**Using natural language processing to explore and extract information from historical literature/pdfs**, _Department_of_Energy | National Energy Technology Laboratory_
> Natural language processing algorithms can be trained and adapted to enhance the exploration and extraction of information from old scientific literature and pdfs. This enables the extraction of valuable knowledge and data, while also ensuring the preservation of essential information from historical sources.

**To demonstrate multi-gamma based sensor technology for as-fired coal property measurement**, _Department_of_Energy | National Energy Technology Laboratory_
> The purpose of this research is to use multi-gamma based sensor technology to accurately measure coal properties as it is injected into burners. The goal is to develop a database and refine neural network algorithms to improve the accuracy of measuring coal properties using the MGA sensor. This will be integrated into Microbeam's Combustion System Performance Indices program.

**Using time-series classification to assist in automated analysis of sensor data taken during experiments on the MHD test channel.**, _Department_of_Energy | National Energy Technology Laboratory_
> Time-series classification is being utilized to automatically analyze sensor data from experiments on the MHD test channel. This analysis involves combining chemical composition measurements with resistance measurements to validate CFD models of the MHD channel system. The objective is to distinguish the contribution of bulk and boundary layer resistance to the overall resistance of the MHD channel using validated CFD models.

**To detect leaks and creaks.**, _Department_of_Energy | National Energy Technology Laboratory_
> Researchers have been studying new machine learning methods to address the two main challenges in managing large-scale geologic CO2 storage operations: detecting small leaks and detecting small seismic signals that may indicate induced seismicity. The research aims to demonstrate the effectiveness of these approaches in ensuring the safe management of such operations.

**To develop a deep-learning Artificial Intelligence model for analysis of fundamental combustion characteristics**, _Department_of_Energy | National Energy Technology Laboratory_
> A deep-learning AI model will be created to quickly analyze the fundamental combustion characteristics required for the development and troubleshooting of H2-containing fuel combustors. This model aims to support the design process by providing rapid and detailed analysis.

**Computational capabilities to support experimental efforts**, _Department_of_Energy | National Energy Technology Laboratory_
> This subtask aims to use NETL's computational capabilities and university collaborators to assist experimental efforts by conducting atomic-level calculations and microkinetic modeling for catalyst systems. These calculations provide detailed information on reaction energetics and establish important relationships between catalyst structure and properties, ultimately helping to optimize catalyst structure and formulation.

**To identify and characterization REE- CM hot zones using machine learning-aided multi-physics.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text states the goal of developing and demonstrating a machine learning-aided multi-physics approach to quickly identify and characterize areas with high concentrations of rare earth elements (REE) in mine tailings. The focus is on coal and sulfide mine tailings, as well as other byproducts like fly ash and refuse deposits.

**Rokbase Geologic Core Data Tool**, _Department_of_Energy | National Energy Technology Laboratory_
> The Rokbase Geologic Core Data Tool aims to create a platform that allows for easy access, searchability, and description of DOE OGFL data. This will enable future research and development, sustainable resource planning, and responsible management of national resources. NETL's expertise in geo-data science, machine learning, visualization, online data mining, and advanced analytics can be utilized to enhance intelligent analytics for offshore systems.

**To implement sensor-driven deep learning/artificial intelligence for power plant monitoring**, _Department_of_Energy | National Energy Technology Laboratory_
> Implementing sensor-driven deep learning/artificial intelligence in power plant monitoring enables intelligent health monitoring at the sensor or base station level. This provides power plant operators with enhanced prediction tools for maintenance scheduling. The focus is on utilizing chipless RFID technology and various industrial sensors to measure in-situ boiler temperature, as well as monitor temperature, pressure, environmental conditions, and water quality.

**Analysis to Assess Offshore CCS Trends and Gaps**, _Department_of_Energy | National Energy Technology Laboratory_
> This text describes an analysis project that aims to assess trends and gaps in offshore carbon capture and storage (CCS) activities. The project will involve reviewing and analyzing international offshore CCS projects to gather knowledge and data resources. The outcomes of the analysis will include integrating key data and tools into a carbon storage database and providing recommendations on the relevance of international studies to U.S. federal offshore settings.

**Develop and demonstrate reinforcement learning approach for time-varying control for flexible hydrogen and power production.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text discusses the development and demonstration of a reinforcement learning approach for time-varying control in flexible hydrogen and power production. This includes the development of a nonlinear model predictive control framework and reinforcement learning approaches for optimal control policy selection and adaptive control. The goal is to optimize the performance of the system while minimizing degradation over long-term operation.

**To develop a wireless, distributed data acquisition and interpretation system foe seismic monitoring and carbon storage characterization.**, _Department_of_Energy | National Energy Technology Laboratory_
> Resensys is working on a wireless data system that will monitor and interpret seismic activity at carbon storage sites in real time. By using advanced signal processing and AI/ML techniques, this system will help assess the risks of natural seismicity before any CO2 injection, saving time and money in developing the storage project.

**Neural networks used to compensate a drone-mounted magnetic sensor for maneuvering of the drone.**, _Department_of_Energy | National Energy Technology Laboratory_
> The use of neural networks helps to counteract the impact of drone movement on a magnetic sensor attached to the drone. ML algorithms will be developed to compensate for magnetic noise caused by the drone's maneuvering, improving the sensitivity and resolution of magnetic data collected during surveys. This technology aims to optimize electromagnetic technology for cased wells and enable scalable solutions for high-volume drone surveys.

**To develop low cost conversion of coal to graphene**, _Department_of_Energy | National Energy Technology Laboratory_
> This text discusses the goal of developing a cost-effective method for converting coal into high-quality graphene. The plan is to build a manufacturing facility that can process 250 tons of coal per day using flash joule heating technology. Additionally, machine learning algorithms will be used to analyze the relationship between processing parameters and the quality, yield, and dimensions of the graphene produced.

**To drive insights using machine learning-based dynamics, control, and health models and tools developed by NETL to gain valuable operational data, insights, and**, _Department_of_Energy | National Energy Technology Laboratory_
> NETL will utilize machine learning techniques to create models and tools for power generation facilities. These models will provide valuable data and insights for operational purposes.

**Develop, integrate, and automate the reduction of CFD models while preserving acceptable levels of accuracy. In general for CCSI2, this work intends to focuse on CFD applications.**, _Department_of_Energy | National Energy Technology Laboratory_
> This text explains the goal of developing, integrating, and automating the reduction of CFD models while maintaining accuracy. The focus is on using physics-based deep learning models to create surrogates that can replace CFD models for predicting quantities needed for optimization. The results of this work can be used to streamline downstream analyses and there are plans to improve the ML/AI interface in FOQUS by adding support for vector variables and additional surrogate model tools and normalization functions.

**To analyze data and derive insights and improve predictions to forecast wellbore kick events to reduce loss of control events.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text suggests that analyzing data and utilizing neural networks or AI cluster data analysis methods can help improve the detection and forecasting of wellbore kick events. This can lead to better real-time detection and prediction of these events, ultimately reducing the occurrence of loss of control events during drilling.

**Using AI/ML to replace conventional geophysics inversion - does the process quicker than the typical method. Make geophysical results more user-friendly.**, _Department_of_Energy | National Energy Technology Laboratory_
> The project aims to use AI/ML to replace conventional geophysics inversion, which will make the process faster than the typical method and result in more user-friendly geophysical results. This will be done by deploying a high sensitivity atomic magnetometer on a small unmanned aerial system (sUAS) platform and conducting baseline surveys at a CarbonSAFE site. The results from previous modeling will determine which methods, such as MT or CSEM, will be tested.

**To implement unsupervised learning based interaction force model for nonspherical particles in incompressible flows**, _Department_of_Energy | National Energy Technology Laboratory_
> The text describes a plan to develop a neural network-based interaction force model for nonspherical particles in incompressible flows. The model will be trained using a database of interaction forces between the particles and the fluid, obtained through simulation. A variational auto-encoder will be used to improve the diversity of the particle library and extract shape factors, and a multi-layer perceptron will be used for feature extraction.

**To provide combustion performance and emissions optimization through integration of a miniaturized high- temperature multi process monitoring system**, _Department_of_Energy | National Energy Technology Laboratory_
> The project aims to improve combustion performance and reduce emissions by integrating a small, high-temperature multi-process monitoring system. The control logic will be developed to automatically adjust boiler operations based on the data collected, such as initiating soot-blowing to eliminate damaging conditions.

**To use AI to calibrate the simulation model by matching simulation data with production history data.**, _Department_of_Energy | National Energy Technology Laboratory_
> CMG's AI tool, CMOST AI, will be used to calibrate the simulation model by comparing simulation results with production history data. This will involve generating simulation cases and analyzing the results using CMOST AI. An optimizer will then determine parameter values for new simulation jobs.

**Database will be utilized to demonstrate targeted biocide strategies using AI to assess large DNA datasets.**, _Department_of_Energy | National Energy Technology Laboratory_
> The team plans to create a DNA database for produced water management. This project will be carried out in two phases: developing and launching the database, and demonstrating its usefulness through network analysis. The fully characterized streams in the database will be used by other projects to estimate resource recovery and train machine learning models for predicting compositions of produced water.

**Demonstrate the robust performance of our ML method in a commercial-scale synthetic data and integrate image-to-image mapping with convolutional neural networks**, _Department_of_Energy | National Energy Technology Laboratory_
> The ML method we developed performs well in large-scale synthetic data and combines image mapping with convolutional neural networks. It efficiently incorporates real-time observations to provide accurate and timely forecasts for geological carbon storage. The method distributes computation across multiple machines, enabling coupled inversions, ML-driven optimization, and sensitivity analysis using multiple ML models.

**To automate RDE image analysis, machine learning for RDE image analysis is being employed.**, _Department_of_Energy | National Energy Technology Laboratory_
> The project aims to use machine learning for RDE image analysis, which will generate experimental data for insights in RDC design, turbomachinery coupling, model validation, and combustion sensors. NETL is collaborating with the University of Michigan to utilize machine learning in order to accelerate reacting flow CFD modeling and overcome the computational challenges posed by the complex coupling in RDE design.

**To implement machine learning to predict aerodynamic and combustion characteristics in hydrogen turbine**, _Department_of_Energy | National Energy Technology Laboratory_
> The text states that machine learning will be used to predict aerodynamic and combustion characteristics in hydrogen turbines. This will be achieved by combining high fidelity simulations, stochastic modeling, reduced modeling through machine learning, and testing of injector configurations. These predictions can then be used in an industrial setting to make design decisions for hydrogen turbine combustors.

**ML-based approaches to improve site characterization efforts**, _Department_of_Energy | National Energy Technology Laboratory_
> improve site characterization efforts by utilizing data from existing projects or future opportunities. Additionally, show how ML-based rapid forecasting can assist in making reservoir management decisions in the pre-injection phase, even when faced with uncertainties in the data. Finally, demonstrate the benefits of a visualization platform integrated with ML-based models for site characterization.

**AI/ML may be needed to extract data from text, image and tabular- based resources. NEWTS is partnering with university teams to use ML to fill in data gaps using predictive models.**, _Department_of_Energy | National Energy Technology Laboratory_
> NEWTS is using AI/ML to extract data from various sources such as text, images, and tables. They are partnering with university teams to fill in data gaps using predictive models. They will establish data requirements and database structure by reviewing energy-water datasets and literature, and identify data sources from regulatory agencies, government monitoring programs, and open-source literature. They will compile metadata into a data catalog for tracking and reference, and collect and process datasets, potentially using ML techniques like natural language processing to extract data from historical reports or unstructured formats. The data catalog will be released and made available.

**To automate development of proxy models for power generation combustion systems.**, _Department_of_Energy | National Energy Technology Laboratory_
> This text describes a process of automating the development of proxy models for power generation combustion systems. It involves conducting detailed computational fluid dynamics (CFD) analysis of large combustion systems and using the results to create machine learning-based proxy models that can provide similar results in a shorter amount of time. These proxy models will be integrated into real-time digital twin models of power plants to assist operators in identifying instrumentation failures or cyberattacks.

**To drive insights on the power system reliability, cost, and operations during the energy transition with and without FECM technologies**, _Department_of_Energy | National Energy Technology Laboratory_
> Commercially available models will be utilized to create predictive scenarios in order to understand the impact of FECM technologies on power system reliability, cost, and operations during the energy transition. These scenarios will provide valuable insights into the potential benefits and challenges of implementing FECM technologies in the power sector.

**Data discovery, processing, and generation using machine learning for a range of CCS data and information**, _Department_of_Energy | National Energy Technology Laboratory_
> The team will use machine learning to discover, process, and generate data for CCS (carbon capture and storage) efforts. They will utilize the new EDX++ cloud computer capabilities through ArcGIS Enterprise Portal to support geospatial data collection and publishing. This includes developing the Carbon Matchmaker tool and updating GeoCube with the Carbon Storage Open Database and NATCARB. The use of ArcGIS Enterprise Portal will also allow for easier version control and integration of spatial data.

**To leverage machine learning and predictive analytics to advance the state of the art in pipline infrastructure integrity management.**, _Department_of_Energy | National Energy Technology Laboratory_
> The project aims to improve pipeline infrastructure integrity management by using machine learning and predictive analytics. This will be achieved by utilizing forecasted pipeline conditions based on large sets of pipeline integrity data and continuous operational data collected by oil and gas transmission pipeline operators.

**Prediction of gasification gas yield and compositions using machine learning**, _Department_of_Energy | National Energy Technology Laboratory_
> A machine learning model will be created to analyze and improve the gasification process using different types of feedstocks such as waste plastic, waste coal, biomass, and municipal solid waste (MSW). The model will be developed using a database compiled from literature, previous experiments, and new experiments conducted at NETL. This project aims to accelerate the development of gasification technology by combining machine learning with experimental studies, particularly as the size of the database grows.

**Using recursive neural networks and using fiber optic cables to recognize strain patterns and warn operators a fracture is coming.**, _Department_of_Energy | National Energy Technology Laboratory_
> This project aims to develop a machine learning algorithm that can predict the timing of a growing fracture reaching a monitored well. The algorithm will be trained on the unique tensile strain pattern that occurs before a fracture. When used in conjunction with an existing fracture warning algorithm, this new workflow will provide early warning of well-to-well communication, predict the depths where the communication will occur, and estimate the time until the communication begins.

**To research and develop physics- aware and AI-enabled cyber- physical intrusion response for the power grid.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal of the research is to develop a response system for the power grid that can effectively address both cyber and physical threats. This will involve using data from various sensors and models, combined with artificial intelligence, to quickly and efficiently respond to anomalous incidents and maintain the resilience of the power system.

**Online real time system Identification**, _Department_of_Energy | National Energy Technology Laboratory_
> The focus of this project is to use online system identification (SI) to monitor the condition of a power plant boiler during different process states. SI algorithms will be integrated into a Model Predictive Control (MPC) system to ensure continuous adaptability as the power plant operates at various loads. The effectiveness of the control algorithm will be tested on representative models and either a high-fidelity commercial power plant simulator or a real power plant facility. Additionally, the online SI techniques will be tested on historical power plant data, dynamic models, laboratory pilot-scale power systems, and feasible power plants.

**Solving Field Equations on the Wafer Scale Engine**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal is to create a code that can solve field equations on the Wafer Scale Engine (WSE) by developing a collocated, finite volume code with maximum mesh flexibility and advanced CFD capabilities. The code will be expanded with API capabilities to support general purpose CFD applications and will be benchmarked in tests to support problems of interest to FECM.

**To develop an Artificial intelligence- based model for rotating detonation engine designs**, _Department_of_Energy | National Energy Technology Laboratory_
> An artificial intelligence model will be created to design low-loss rotating detonation engines for power generation using natural gas/syngas mixtures. This model will allow for full-scale calculations of the engine over 100-1000 detonation cycles.

**To provide natural gas leak detection and quality control**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning techniques are being used to train sensing systems in order to detect and measure the concentration of natural gas species. These systems can also differentiate between natural gas at various stages of the processing pipeline and distinguish it from other sources such as wetlands and agriculture.

**To use advanced machine learning techniques to analyze static and dynamic measurements of proppant distribution and fracture geometry data.**, _Department_of_Energy | National Energy Technology Laboratory_
> The project aims to analyze proppant distribution and fracture geometry data by utilizing advanced machine learning techniques. This analysis will be performed on static and dynamic measurements collected from numerous microchips that have been injected with proppant near the wellbore.

**To enhance the SimCCS toolset to better account for existent infrastructure and to more broadly engage other user bases to improve toolset performance and applicability.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal is to improve the SimCCS toolset by incorporating existing infrastructure and expanding its user base. This will be achieved through ongoing development to optimize the toolset's performance and make it more applicable, by considering factors such as environmentally sensitive areas, Justice40 considerations, and utilizing existing infrastructure in determining the best placement for CO2 pipeline rights of way and infrastructure.

**To accurately predict alloy & component performance extrapolated to conditions where experimental results to do not exist.**, _Department_of_Energy | National Energy Technology Laboratory_
> AI/ML will use various databases and improved physics-based models to accurately predict the performance of alloys and components under extreme conditions and complex loading. This will be done by generating reduced order models and extrapolating results to conditions where experimental data is lacking.

**To accelerate discovery of protection system and laser processing of protective coatings on CMC for hydrogen turbines.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal of this project is to create a laser-manufactured protection system for hydrogen turbines that will allow for the use of Silicon Carbide fiber/Silicon Carbide (SiCf/SiC) matrix CMCs. This system will consist of a bond coat-environmental barrier coat-thermal barrier coat (BC-EBC-TBC) that effectively protects the CMCs.

**To drive insights on water recovery from cooling tower plumes**, _Department_of_Energy | National Energy Technology Laboratory_
> The text describes a study that aims to understand water recovery from cooling tower plumes. The study involves investigating plume formation and collection on both a laboratory setup and a full-scale industrial cooling tower. Various sensors will be installed on the lab cooling tower, and a computational fluid dynamics model will be used to develop precise plume models. The project will also experiment with electrodes and collectors on the lab cooling tower based on insights from the full-scale plume modeling and sensor data analysis. Ultimately, the findings will be used to design an optimal collection apparatus for specific working conditions.

**To employ machine learning to study the dependence of electrochemical performance on microstructural details**, _Department_of_Energy | National Energy Technology Laboratory_
> The recipient will use machine learning, specifically deep learning methods like RCNN or YOLO, to analyze the relationship between electrochemical performance and microstructural details in a large dataset of images. If there are a limited number of images, the recipient will either use transfer learning with similar images or traditional image processing and computer vision techniques for detection and feature extraction.

**To apply machine learning applications to map carbon ore, rare earth element, and critical mineral resources**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning and GIS applications will be used to map carbon ore, rare earth element, and critical mineral resources. This will involve identifying information gaps, assessing research needs and technology gaps, and targeting resources for sampling and characterization. The goal is to provide a comprehensive assessment of the value chain in the Northern Appalachian region and facilitate the rapid development of commercial projects.

**ML-based reduced order models of reservoir response to Co2 injection into saline and/or hydrocarbon- bearing formations - as the basis for integrated assessment modeling of leakage risk (e.g., SACROC)**, _Department_of_Energy | National Energy Technology Laboratory_
> The NRAP researchers are developing a comprehensive framework that links fast forecasting models of CO2 storage system components. They will expand this framework to demonstrate relevance to industry-standard site risk management methods and to understand containment performance and leakage risk in different scenarios. They will also engage with stakeholders to establish a modeling and risk assessment design basis that is relevant to real geologic storage deployment scenarios.

**Reduce computational cost of CFD simulations that screen for more efficient intensified solvent contactor geometries.**, _Department_of_Energy | National Energy Technology Laboratory_
> The aim is to decrease the computational expense of CFD simulations that evaluate more effective solvent contactor designs. This will be achieved by working with Subtask 4.3 Machine Learning Support to employ Deeper Fluids (DF), graph neural networks (GNNs), or similar machine learning techniques to simplify validated CFD calculations. Additionally, the focus will be on advancing the ongoing process modeling/optimization with input from the CFD reduced order models (ROM).

**To create and apply machine learning algorithms to predict carbon dioxide enhanced oil revoery improvements with rich gas in the Bell Creek Field and other selected fields.**, _Department_of_Energy | National Energy Technology Laboratory_
> The goal is to use machine learning algorithms to predict improvements in carbon dioxide enhanced oil recovery (EOR) using rich gas in the Bell Creek Field and other chosen fields. The predictions made by these algorithms will be compared with the predictions from CMG's reservoir simulation models.

**To drive insights through data-driven predictive modeling to forecast the remaining lifespan and future risk of offshore production platforms.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text describes the development and application of Artificial Neural Network and Gradient Boosted Regression Tree models to predict the remaining lifespan of offshore production platforms. These data-driven models achieved high prediction accuracies of 95-97%.

**To improve control of hybrid SOFC- gas turbine power systems.**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning algorithms are being used to enhance control of hybrid SOFC-gas turbine power systems. These algorithms are being compared with other control methods to determine their effectiveness in improving power generation efficiency.

**Machine learning to process multi- model data and information to aid in the identification of undocumented orphaned wells**, _Department_of_Energy | National Energy Technology Laboratory_
> Machine learning is being used to analyze various types of data and information in order to identify undocumented orphaned wells. By analyzing trends and patterns in known well data, machine learning can predict the existence of undocumented wells. Additionally, machine learning is used to process different types of imagery-based data to classify and characterize additional undocumented orphaned wells in the Appalachain Basin.

**UNET and other approaches for ML- based inversion**, _Department_of_Energy | National Energy Technology Laboratory_
> Researchers are working on developing a design basis for risk-based monitoring in ML-based inversion techniques like UNET. They aim to consider factors like data dimensionality, uncertainty, and inter-tool/module connectivity to optimize the monitoring design using the DREAM tool in NRAP-Open-IAM and the SMART platform.

**To build the first data analytics and artificial intelligence field laboratory for unconventional resources in the Powder River Basin, focusing on optimization of hydraulic fracture stimulations through the use of multiple diagnostic technologies.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text describes the creation of a data analytics and artificial intelligence laboratory in the Powder River Basin, aimed at optimizing hydraulic fracture stimulations for unconventional resources. The laboratory will focus on the development of three major unconventional oil resources through geological characterization and improved models, leading to advancements in well completion and fracture stimulation designs. Multi-variate analysis will be used to understand how completion and stimulation controls affect well productivity.

**To develop a novel platform for secure data logging and processing in fossil fuel power generation systems using blockchain and machine learning to reduce down time for fossil energy power plants, limit reductions of power and reduce cost for repairs.**, _Department_of_Energy | National Energy Technology Laboratory_
> The text discusses the development of a secure data logging and processing platform for fossil fuel power generation systems. This platform will use blockchain and machine learning techniques to reduce downtime, power reductions, and repair costs. The machine learning model development will involve implementing traditional machine learning and deep learning algorithms for anomaly detection using sensor data from a secure sensor network.

**Develop fast predictive models using novel machine-learning based methods.**, _Department_of_Energy | National Energy Technology Laboratory_
> This text highlights the importance of developing accurate and fast predictive machine learning (ML) models for a virtual learning platform. By generating training data and utilizing ML-based models, a Virtual Learning Environment (VLE) can be created to improve reservoir development, management, and monitoring strategies before field activities take place.

**Using ML to build predictive models of branching processes and develop novel algorithms for automated MIP solver tuning**, _Department_of_Energy | National Energy Technology Laboratory_
> The text explains that machine learning will be used to create predictive models of branching processes, which will then be used to improve the optimization of advanced integrated energy systems. Currently, MIP solver tuning is done through trial and error methods, but the proposal suggests developing new algorithms using machine learning to automate this process and solve currently intractable energy systems.

**Demonstrate how ML-based approaches can help operators during active injection and post- injection monitoring**, _Department_of_Energy | National Energy Technology Laboratory_
> Task 4 aims to establish data-sharing protocols between SMART and operators in order to enhance their decision-making process. This includes using ML-based approaches to improve site-monitoring and operations efforts during injection and post-injection phases, such as utilizing IL-ICCS data and developing value of information guidelines.

**To use computational tools to optimize the design of solid CO2 sorbents.**, _Department_of_Energy | National Energy Technology Laboratory_
> This project aims to optimize the design of solid CO2 sorbents by using computational tools. The desired outcome is to determine the best polymer structure and molecular amines that will result in a solid sorbent with optimal CO2 loading capacity, CO2 heat of adsorption, and CO2 mass transfer rate at very low CO2 partial pressures, while minimizing amine leaching.

**To develop and evaluate a general drag model for gas-solid flows via physics-informed deep machine learning**, _Department_of_Energy | National Energy Technology Laboratory_
> This project aims to develop and evaluate a drag model for gas-solid flows using deep machine learning. The project will assess the performance of various deep neural network algorithms, including random forest, BPNN, and XGBoost. The best algorithm will be determined based on their ranking. The developed model will be integrated into the multiphase flow simulation software MFiX-DEM, using the existing software links between MFiX and TensorFlow.

**Data Analytics and Machine Learning (DAMaL) Tools to enhance the analysis of Environment, Safety and Health (ES&H) data: Unsupervised Machine Learning Text Clustering**, _Department_of_Energy | Office of Environment, Health, Safety & Security_
> The DAMaL tools use unsupervised machine learning clustering, NLP, and clustering algorithms to analyze text data from DOE ES&H records. The tools identify important topics that can help analysts investigate recurring safety issues in DOE operations. The tools have been partially deployed and are expected to be maintained, improved, and expanded with more data sources.

**Memorandum of Understanding Between the US DOE and US NRC on Cooperation in the Area of Operating Experience and Applications of Data Analytics (Signed June 2021)**, _Department_of_Energy | Office of Environment, Health, Safety & Security_
> The US DOE and US NRC have signed a Memorandum of Understanding (MOU) in June 2021 to collaborate in the areas of operating experience and data analytics. The purpose of this collaboration is to efficiently utilize resources, share data and technical information, and avoid duplication of effort. The collaboration will focus on areas such as safety data collection and analysis, data visualization, artificial intelligence, and predictive analytics.

**Applications of Natural Language Processing and Similarity Measures for Similarity Ranking**, _Department_of_Energy | Office of Environment, Health, Safety & Security_
> EHSS has developed applications of natural language processing and similarity measures to improve information retrieval and searching of datasets. These applications have been successfully used for tasks such as searching databases, estimating similarities between records, and identifying opportunities for resource prioritization. Currently, the tool is run locally by the principal investigator, but plans to transition to a web-based application were halted due to lack of demand and resources.

**Data Analytics and Machine Learning (DAMaL) Tools for Analysis of Environment, Safety and Health (ES&H) data: Similarity Based Information Retrieval**, _Department_of_Energy | Office of Environment, Health, Safety & Security_
> The EHSS DAMaL tools use natural language processing and cosine similarity to efficiently retrieve important records from DOE ES&H datasets. The tools have no text query restrictions, offer NLP options, and can be used to improve decision-making in various areas such as job planning, hazard identification, and accident investigations. The tool is currently available on the DAMaL tools website and is expected to be continuously maintained, enhanced, and expanded with more data sources.

**Data Analytics and Machine Learning (DAMaL) Tools to enhance the analysis of Environment, Safety and Health (ES&H) data: Classification, Robotic Process Automation and Data Visualization**, _Department_of_Energy | Office of Environment, Health, Safety & Security_
> The EHSS DAMaL tools use classification, robotic process automation, and data visualization to automate the analysis of ES&H data. By leveraging AI and NLP, the tools analyze DOE records and identify important topics for further exploration, helping analysts uncover potential safety issues in DOE operations. The tools have been deployed on the DAMaL website and are expected to be continuously maintained, documented, improved, and expanded with more data sources.

**Groundwater Modeling**, _Department_of_Energy | Office of Legacy Management_
> and simulation of groundwater flow and contaminant transport. It helps in understanding the behavior of groundwater systems and predicting the impact of various factors such as pumping, recharge, and land use changes. This modeling technique plays a crucial role in water resource management and environmental planning.

**Soil Moisture Modeling**, _Department_of_Energy | Office of Legacy Management_
> The text suggests using a combination of different sources and machine learning techniques to create a model that can accurately predict soil moisture levels in the lysimeter located within a disposal cell. This approach aims to better understand and manage soil moisture in this specific area.

**AI-Based Chat Bot**, _Department_of_Energy | Office of the Chief Information Officer_
> The OCIO EITS Service Desk is exploring the use of AI chat bots to improve interactions with end-users. The chat bot would have a specialized architecture to handle IT system languages and provide knowledge and assistance in an accessible way. It would also automate basic functions and provide feedback to the service desk on its performance.

**Managing curb allocation in cities**, _Department_of_Energy | Pacific Northwest National Laboratory_
> The project aims to create a simulation tool and a curb management platform to efficiently handle the growing demand for curb-side parking in cities. These tools will help cities manage the allocation of curbs and optimize the use of available space.

**Laboratory Automation**, _Department_of_Energy | Pacific Northwest National Laboratory_
> Laboratory automation utilizes machine learning to efficiently and accurately identify specific areas of interest in scanning electron microscope (SEM) and transmission electron microscope (TEM) data. This process automates data acquisition, resulting in improved productivity and efficiency in laboratory operations.

**AI techniques for identification of suitable delivery parking spaces in an urban scenario**, _Department_of_Energy | Pacific Northwest National Laboratory_
> AI techniques, specifically Graph Neural Networks, are being employed to assess the significance of parking spaces within a city's network. The aim is to effectively manage curbs and encourage the use of electric vehicles for freight delivery.

**Regional waste feedstock conversion to biofuels**, _Department_of_Energy | Pacific Northwest National Laboratory_
> The text explains that unsupervised machine learning is utilized to categorize waste sources into distinct regions. These regions are then analyzed using calibrated game theoretic models to evaluate the feasibility and economic potential of various waste-to-energy conversion methods.

**Scalable, Efficient and Accelerated Causal Reasoning Operators, Graphs and Spikes for Earth and Embedded Systems (SEA-CROGS)**, _Department_of_Energy | Pacific Northwest National Laboratory_
> The SEA-CROGS project aims to establish a center for scalable and efficient physics-informed machine learning for science and engineering. This will accelerate modeling, inference, causal reasoning, etiology, and pathway discovery for earth systems and embedded systems, ultimately leading to advancements in operator regression for next-generation neuromorphic computers.

**AI used for predictive modeling and real time control of traffic systems**, _Department_of_Energy | Pacific Northwest National Laboratory_
> AI is being utilized to predict traffic patterns and control traffic systems in real-time. Deep learning models are employed to make accurate predictions about traffic and optimize traffic signaling and coordination, leading to better traffic flow and reduced energy consumption and greenhouse gas emissions.

**APT Analytics**, _Department_of_Energy | Pacific Northwest National Laboratory_
> APT Analytics is focused on creating artificial intelligence and machine learning technology to automate the analysis of advanced persistent threat (APT) data. Their goal is to develop innovative solutions that can efficiently process and interpret APT data without human intervention.

**Surrogate models for probabilistic Bayesian inference**, _Department_of_Energy | Pacific Northwest National Laboratory_
> The text explains that AI/ML technologies are being employed to create surrogate models of complex physical systems. These surrogate models will enable the estimation of unknown model parameters by utilizing probabilistic model inversion based on indirect observations.

**Adaptive Cyber-Physical Resilience for Building Control Systems**, _Department_of_Energy | Pacific Northwest National Laboratory_
> Deep learning models are utilized to forecast the performance of building energy systems and identify any potential cyber attacks or health issues. These models also optimize the response of the building energy system to ensure resilient operation and long-term energy efficiency.

**Physics-Informed Learning Machines for Multiscale and Multiphysics Problems (PhILMs)**, _Department_of_Energy | Pacific Northwest National Laboratory_
> Researchers are working on creating physics-informed learning machines, known as PhILMs, which incorporate physics knowledge into deep learning networks. This approach aims to tackle problems that involve multiple scales and multiple physics aspects.

**Elucidating Genetic and Environmental Risk Factors for Antipsychotic-induced Metabolic Adverse Effects Using AI**, _Department_of_Energy | Pacific Northwest National Laboratory_
> This text discusses the development of AI methods to identify and understand the genetic and environmental risk factors associated with metabolic adverse effects caused by antipsychotic drugs. The goal is to use these methods to predict the negative side-effects of these drugs on the human population by analyzing the complex interactions between the individual's genetic makeup, chronic diseases, and the chemical composition of the drug.

**Advancing Market-Ready Building Energy Management by Cost- Effective Differentiable Predictive Control**, _Department_of_Energy | Pacific Northwest National Laboratory_
> This text describes a framework for advancing building energy management through AI-based predictive modeling and control. The framework utilizes domain-aware deep learning models to optimize the response of building systems and components, ensuring resilient operation and sustained energy efficiency. Additionally, the framework includes methods for safety verification and online learning.

**Universal MCEG**, _Department_of_Energy | Thomas Jefferson Laboratory_
> The Universal MCEG is a research and development project focused on creating a machine learning-based Monte Carlo event generator. This generator has the purpose of compressing data and making it compatible for various applications.

**FIMS - Invoice BOT - Employee Reimbursements FIMS - Invoice BOT - Purchase Power**, _Department_of_Energy | Western Area Power Administration_
> The FIMS-Invoice BOT is an automated system that processes invoices sent to the RPA Invoice Intake email box. The bot extracts information from PDF invoices and classifies them as either Employee Reimbursements or Purchase Power Invoices. The extracted information is then reviewed and validated by an Accounts Payable Technician before being loaded into the WAPA Financial Management System.

## Department_of_Health_and_Human_Services

**Information Gateway OneReach Application**, _Department_of_Health_and_Human_Services | ACF | ACF Children's Bureau_
> The Information Gateway hotline is linked to a phone IVR system operated by OneReach AI. Through their database of state hotlines, OneReach can connect callers to the appropriate hotline based on their phone area code. Furthermore, OneReach provides a texting service that uses natural language processing to respond to user queries, which are used for training purposes and to create more FAQs.

**Chatbot**, _Department_of_Health_and_Human_Services | AHRQ | AHRQ_
> The chatbot serves as an interface to enable users to ask questions about AHRQ content in a conversational manner, serving as a replacement for the public inquiry telephone line. Users can easily engage with the chatbot to inquire about AHRQ information and receive prompt responses.

**AHRQ Search**, _Department_of_Health_and_Human_Services | AHRQ | AHRQ_
> The AHRQ search is a comprehensive search tool for an organization that includes various features such as relevancy tailoring, auto-generation synonyms, automated suggestions, suggested related content, auto tagging, and a "did you mean" function. These features aim to help visitors easily find specific content within the organization's website.

**R+2:18eDIRECT: Clarivate**, _Department_of_Health_and_Human_Services | ASPR | BARDA (CBRN & DRIVe)_
> Clarivate, an information services company, has developed an AI technology called R+2:18eDIRECT, which can identify potential drug repurposing candidates. This innovative tool aims to accelerate the drug discovery process by analyzing vast amounts of data and providing valuable insights for researchers and pharmaceutical companies.

**ReDIRECT: AriScience**, _Department_of_Health_and_Human_Services | ASPR | BARDA (CBRN & DRIVe)_
> AriScience has developed an AI technology called ReDIRECT, which can identify potential drug repurposing candidates. This technology uses artificial intelligence to analyze large amounts of data and identify existing drugs that could be repurposed for new therapeutic uses.

**Burn & Blast MCMs: Philips**, _Department_of_Health_and_Human_Services | ASPR | BARDA (CBRN)_
> Philips has developed AI-based algorithms for its Lumify handheld ultrasound system to detect traumatic injuries caused by burns and blasts. These algorithms utilize artificial intelligence to enhance the accuracy and efficiency of diagnosis, enabling healthcare professionals to quickly identify and treat these types of injuries.

**Burn & Blast MCMs: Philips**, _Department_of_Health_and_Human_Services | ASPR | BARDA (CBRN)_
> Philips has developed AI-based algorithms for their Lumify handheld ultrasound system to detect lung injury and infectious diseases. These algorithms help in identifying and monitoring such conditions, making the ultrasound system a valuable tool for healthcare professionals.

**Burn & Blast MCMs: Rivanna**, _Department_of_Health_and_Human_Services | ASPR | BARDA (CBRN)_
> The Rivanna Accuro XV utilizes AI algorithms to effectively identify and emphasize fractures and soft tissue injuries in Burn & Blast MCMs. This advanced technology aids in the detection and highlighting of such injuries, improving the accuracy and efficiency of medical assessments.

**Burn & Bast MCMs: SpectralMD**, _Department_of_Health_and_Human_Services | ASPR | BARDA (CBRN)_
> SpectralMD has developed a technology called Burn & Bast MCMs, which can accurately determine the severity and size of burn injuries. This innovative solution provides a reliable method for assessing burn depth, allowing medical professionals to make informed treatment decisions.

**Digital MCM: Visual Dx**, _Department_of_Health_and_Human_Services | ASPR | BARDA (DRIVe)_
> Digital MCM's Visual Dx is a smartphone app that utilizes artificial intelligence to identify the presence of mPox in images.

**Digital MCM: Virufy**, _Department_of_Health_and_Human_Services | ASPR | BARDA (DRIVe)_
> Virufy is a digital MCM that utilizes forced cough vocalization through a smartphone to detect the presence of COVID-19. This innovative technology combines AI to analyze cough sounds and identify potential cases of the virus.

**Current Health**, _Department_of_Health_and_Human_Services | ASPR | BARDA (DRIVe)_
> Current Health is a platform that provides continuous monitoring and an AI algorithm specifically designed to assess the severity of COVID-19. This platform allows for constant surveillance of patients' health and utilizes artificial intelligence to accurately evaluate the seriousness of their condition related to the virus.

**Digital MCM: Raisonance**, _Department_of_Health_and_Human_Services | ASPR | BARDA (DRIVe)_
> Raisonance has developed a digital MCM (Mobile Cough Monitor) that uses forced cough vocalization (FCV) in smartphones to identify the existence of COVID-19 and Influenza. This technology incorporates artificial intelligence (AI) to analyze the cough sounds and provide accurate detection of these respiratory illnesses.

**Host-Based Diagnostics: Patchd**, _Department_of_Health_and_Human_Services | ASPR | BARDA (DRIVe)_
> Patchd is a wearable device that uses an AI model to predict sepsis in the comfort of one's home. This host-based diagnostic tool aims to detect sepsis early, allowing for timely medical intervention and potentially saving lives.

**Data Modernization**, _Department_of_Health_and_Human_Services | ASPR | Chief Data Officer_
> The text suggests the need to create a data management architecture that allows for efficient utilization of business intelligence and machine learning on all data owned by ASPR. This modernization aims to optimize the analysis and utilization of data within the organization.

**Cyber Threat Detection/ Predictive analytics**, _Department_of_Health_and_Human_Services | ASPR | Office of Critical Infrastructure_
> AI and ML tools are utilized for the processing of vast amounts of threat data in order to detect and predict cyber threats effectively. This approach, known as cyber threat detection or predictive analytics, enables advanced analysis and identification of potential cyber attacks.

**emPOWER**, _Department_of_Health_and_Human_Services | ASPR | Office of Information Management, Data and Analytics_
> The emPOWER program utilizes AI technology to quickly create data tools and a program specifically designed to assist and protect individuals at risk of contracting COVID-19. This initiative aims to provide valuable resources and support to vulnerable populations during the pandemic.

**Community Access to Testing**, _Department_of_Health_and_Human_Services | ASPR | Office of Information Management, Data and Analytics/Division of Supply Chain Control Tower_
> ML models are being used to predict an increase in the pandemic, improving community access to testing.

**Modeling & Simulation**, _Department_of_Health_and_Human_Services | ASPR | Office of Information Management, Data, and Analytics/Division of Modeling and Simulation_
> Modeling and simulation are used to develop tools that can predict and analyze biothreat events before they occur. These tools can be continuously improved and adjusted during actual emergencies.

**Ventilator Medication Model**, _Department_of_Health_and_Human_Services | ASPR | Office of Information Management, Data, and Analytics/Division of Supply Chain Control Tower_
> The ventilator medication model uses a generalized additive model to predict the rate at which COVID inpatients will require ventilators. This model helps to estimate the number of patients who will need ventilator support, aiding in resource allocation and planning for healthcare providers.

**Product redistribution optimization**, _Department_of_Health_and_Human_Services | ASPR | Office of Information Management, Data, and Analytics/ODA_
> The use of AI and models can help partners optimize the redistribution of products. This optimization takes into account factors such as distance, ordering/administration, and equity, allowing for more efficient and equitable distribution of products among jurisdictions, pharmacies, and federal entities.

**Highly Infectious Patient Movement optimization**, _Department_of_Health_and_Human_Services | ASPR | Office of Information Management, Data, and Analytics/ODA_
> The goal is to optimize the location of highly infectious patient transport containers in the US by considering factors such as distance and population. This planning tool will help in making informed decisions regarding the movement of these patients.

**TowerScout:Automated cooling tower detection from aerial imagery for Legionnaires' Disease outbreak investigation**, _Department_of_Health_and_Human_Services | CDC | CSELS_
> TowerScout is an automated system that scans aerial imagery to detect cooling towers, which are potential sources of Legionnaires' Disease outbreaks. By utilizing object detection and image classification models, TowerScout aids in the investigation and prevention of community outbreaks.

**HaMLET: Harnessing Machine Learning to Eliminate Tuberculosis**, _Department_of_Health_and_Human_Services | CDC | CSELS_
> HaMLET is a project that utilizes computer vision models to identify tuberculosis (TB) from chest x-rays. Its purpose is to enhance the accuracy of health screenings for immigrants and refugees who wish to enter the United States.

**Zero-shot learning to identify menstrual irregularities reported after COVID-19 vaccination**, _Department_of_Health_and_Human_Services | CDC | CSELS_
> The researchers used zero-shot learning to identify and categorize menstrual irregularities that were reported by individuals who had received the COVID-19 vaccination. This approach allowed them to analyze and classify these reports without prior training on this specific topic.

**Validation Study of Deep Learning Algorithms to Explore the Potential Use of Artificial Intelligence for Public Health Surveillance of Eye Diseases**, _Department_of_Health_and_Human_Services | CDC | NCCDPHP/DDT_
> This study aims to assess the effectiveness of using deep learning algorithms to detect diabetic retinopathy in retinal photos collected through NHANES. The goal is to determine if these algorithms can replace the need for ophthalmologist grading in public health surveillance of eye diseases.

**Automating extraction of sidewalk networks from street-level images**, _Department_of_Health_and_Human_Services | CDC | NCCDPHP/DNPAO_
> Scientists in the CDC's Data Science Upskilling Program are developing a computer vision model that can automatically extract sidewalk networks from street-level images sourced from Mapillary. This model aims to provide valuable information about the presence of sidewalks in various locations.

**Identifying state and local policy provisions that promote or inhibit creating healthy built environments**, _Department_of_Health_and_Human_Services | CDC | NCCDPHP/DNPAO_
> The Division of Nutrition, Physical Activity, and Obesity at the National Center for Chronic Disease Prevention and Health Promotion wants to use natural language processing and machine learning to make policy surveillance more efficient. They are currently working with a contractor to compare machine learning with traditional methods and identify other groups working on similar projects. As of April 2023, the division has not done this work in-house.

**Identify walking and bicycling trips in location-based data, including global-positioning system data from smartphone applications**, _Department_of_Health_and_Human_Services | CDC | NCCDPHP/DNPAO_
> The Division of Nutrition, Physical Activity, and Obesity is using machine learning to identify walking and bicycling trips in GPS data, such as smartphone applications. They aim to produce geocoded data tables, GIS layers, and maps from commercially-available location-based data sources.

**Identify infrastructure supports for physical activity (e.g. sidewalks) in satellite and roadway images**, _Department_of_Health_and_Human_Services | CDC | NCCDPHP/DNPAO_
> The Division of Nutrition, Physical Activity, and Obesity wants to use machine learning to identify sidewalks, bicycle lanes, and other infrastructure in satellite and roadway images. They will use image-based data as input and produce geocoded data tables, maps, GIS layers, or summary reports as output.

**Use of Natural Language Processing for Topic Modeling to Automate Review of Public Comments to Notice of Proposed Rulemaking**, _Department_of_Health_and_Human_Services | CDC | NCEZID_
> A Natural Language Processing (NLP) tool has been developed to automate the review of public comments for a 'notice of proposed rulemaking' (NPRM). This tool utilizes NLP for topic modeling, improving the efficiency of clustering the comments and streamlining the process.

**Sequential Coverage Algorithm (SCA) and partial Expectation-Maximization (EM) estimation in Record Linkage**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> The CDC's National Center for Health Statistics (NCHS) Data Linkage Program uses the Sequential Coverage Algorithm (SCA) and partial Expectation-Maximization (EM) estimation techniques in their linkage algorithms. SCA is a supervised machine learning algorithm that helps with joining methods for large datasets, while the unsupervised EM estimation is used to estimate the proportion of matching pairs in each block. These methods enhance the accuracy and efficiency of the linkage process.

**Detecting Stimulant and Opioid Misuse and Illicit Use**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> The clinical notes are analyzed to identify instances of illicit use and misuse of stimulants and opioids. This helps in detecting cases where these substances are being used inappropriately or illegally.

**Transcribing Cognitive Interviews with Whisper**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> Current transcription processes for cognitive interviews are limited and time-consuming, with low-quality automated solutions. However, recent open-sourced AI models such as Whisper by OpenAI show promise in automated transcription of video/audio. While Whisper is considered state-of-the-art, it has not been tested with cognitive interviews. A comparison will be conducted against VideoBank and manual transcription, and if successful, all videos from the CCQDER archive will be transcribed.

**Coding cause of death information on death certificates to ICD-10**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> The process involves assigning ICD-10 codes to the cause of death information on death certificates. These codes represent the literal text description provided by the certifier and include both the underlying and contributing causes of death.

**PII detection using Private AI**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> The National Center for Health Statistics (NCHS) has been testing Private AI's NLP solution, which can detect and replace personally identifiable information (PII) in text data. The purpose of this suite of models is to securely identify and remove PII from various text datasets within the CDC network.

**Named Entity Recognition for Opioid Use in Free Text Clinical Notes from Electronic Health Records**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> A team of scientists is working on a Named Entity Recognition model to identify the presence or absence of opioid use in electronic medical records. This model is being developed as part of the CDC's Data Science Upskilling Program and aims to improve the detection of opioid use in the National Hospital Care Survey.

**Semi-Automated Nonresponse Detection for Surveys (SANDS)**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> The NCHS has created SANDS, a model that can detect cases of item nonresponse in open-text survey responses. This model uses Natural Language Processing techniques and has been trained on a custom dataset to improve survey data quality and questionnaire design.

**AI/ML Model Release Standards**, _Department_of_Health_and_Human_Services | CDC | NCHS_
> The NCHS is developing a set of model release standards for AI/ML projects that should be followed within the center. These standards could potentially be used as a basis for creating wider standards for AI/ML development at both NCHS and CDC.

**Nowcasting Suicide Trends**, _Department_of_Health_and_Human_Services | CDC | NCIPC/DIP_
> The text describes the development of an internal interactive dashboard that utilizes various datasets and a machine learning system to predict suicide death trends on a national level on a weekly basis. The purpose of this system is to provide timely information about suicide rates.

**NCIRD SmartFind ChatBots - Public and Internal**, _Department_of_Health_and_Human_Services | CDC | NCIRD_
> NCIRD has developed conversational ChatBots for public flu, public COVID-19 vaccination, and internal knowledge-bot. These ChatBots analyze free text questions from the public, healthcare providers, partners, and internal staff, and provide accurate answers. These ChatBots were developed in collaboration with Microsoft staff using various tools and services.

**Medicaid And CHIP Financial (MACFin) DSH Payment Forecasting model**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Medicaid And CHIP Financial (MACFin) DSH Payment Forecasting model is used to predict future DSH payments based on historical data and trends. The model was trained using time series and machine learning techniques to determine the best performance in terms of average means error. The team had to clean and organize data from over 6 years for all states in order to implement the model effectively and provide meaningful analysis. This forecasting helps with early planning and recommendations related to trends and redistributions, and modified models can also be used for predicting other DSH-related metrics.

**Risk Adjustment Payment Integrity Determination System (RAPIDS)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Risk Adjustment Payment Integrity Determination System (RAPIDS) is an AI system designed to improve the efficiency of classifying and processing risk adjustment payments. It aims to enhance the accuracy and integrity of payment determinations by utilizing advanced technology and automation.

**Central Data Abstraction Tool-Modernized (Modernized-CDAT)- Intake Process Automation (PA) Tool**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Modernized-CDAT Intake Process Automation (PA) Tool utilizes advanced technologies such as NLP, OCR, AI, and ML to automate and modernize the medical record review process in MA RADV audits. This tool aims to reduce the need for manual efforts in this aspect of the audits.

**Relationships, Events, Contacts, and Outreach Network (RECON)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> RECON is an AI-powered platform that specializes in recommender systems and sentiment analysis for relationships, events, contacts, and outreach networks. It uses artificial intelligence to provide recommendations and analyze sentiments in these areas.

**Priority Score Model - ranks providers within the Fraud Prevention System using logistic regression based on program integrity guidelines.**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Priority Score Model uses logistic regression to rank providers within the Fraud Prevention System. It takes inputs such as Medicare Claims data, Targeted Probe and Educate (TPE) Data, and Jurisdiction information to determine the rankings based on program integrity guidelines.

**Federally Facilitated Marketplaces (FFM)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Federally Facilitated Marketplaces (FFM) utilize artificial intelligence (AI) for various tasks including anomaly detection and correction, classification, and forecasting and predicting time series data. This technology helps improve the efficiency and accuracy of the FFM operations and decision-making processes.

**Feedback Analysis Solution (FAS)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Feedback Analysis Solution (FAS) is a system that uses publicly available data to analyze public comments and other information from stakeholders. It utilizes Natural Language Processing (NLP) and machine learning (ML) tools to aggregate, sort, identify duplicates, and extract topics, themes, and sentiment from the dataset, streamlining the comment review process.

**CMS Enterprise Portal Services (CMS Enterprise Portal-Chatbot)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The CMS Enterprise Portal Services, also known as CMS Enterprise Portal-Chatbot, is an AI-powered tool that aims to improve process efficiency and knowledge management. It serves as a platform for users to access information and receive assistance, ultimately enhancing productivity and streamlining operations.

**Fraud Prevention System Alert Summary Report Priority Score**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Fraud Prevention System Alert Summary Report Priority Score is a model that utilizes various data sources to predict the probability of an investigation resulting in an administrative action. Its purpose is to assist CMS in effectively allocating investigation resources. However, the final model type is still under development and has not been finalized.

**Marketplace Learning Management System (MLMS)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Marketplace Learning Management System (MLMS) incorporates artificial intelligence (AI) to provide language interpretation and translation services. It efficiently assists users in understanding and communicating in different languages.

**Performance Metrics Database and Analytics (PMDA)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Performance Metrics Database and Analytics (PMDA) is an AI system that is capable of detecting and correcting anomalies. It also has features for language interpretation and translation, as well as knowledge management.

**Drug Cost Increase Predictions**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> By analyzing past trends in drug costs, it is possible to make predictions about future increases. This method utilizes historical data to forecast the likely rise in drug prices.

**CMS Connect (CCN)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> CMS Connect (CCN) is an artificial intelligence system designed for global search purposes. It utilizes advanced technology to provide efficient and effective search results on a global scale.

**Drug cost anomaly detection**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The purpose of drug cost anomaly detection is to find irregularities in the prices of drugs listed on Part D claims. This process aims to identify any unusual or unexpected variations in drug costs to ensure accuracy and prevent fraudulent activities.

**Artificial Intelligence (AI) Explorers Program Pilot - Automated Technical Profile**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Artificial Intelligence (AI) Explorers Program Pilot is a 90-day research and development initiative. Its aim is to explore the use of AI in creating a machine-readable Automated Technical Profile for CMS systems. The goal is to analyze various data sources at different stages of development to determine the technology used in CMS projects.

**CCIIO Enrollment Resolution and Reconciliation System (CERRS)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The CCIIO Enrollment Resolution and Reconciliation System (CERRS) is an AI system used for classification purposes. It is designed to help with the enrollment process and resolve any discrepancies or issues that may arise.

**Predictive Intelligence - Incident Assignment for Quality Service Center (QSC).**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> Predictive Intelligence is a system used in the Quality Service Center to assign incidents. It analyzes the short descriptions provided by users to find keywords and assigns the ticket to the correct group. The system is regularly re-trained based on incident data every 3-6 months.

**Artificial Intelligence (AI) Explorers Program Pilot - Section 508 accessibility Testing**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Artificial Intelligence Explorers Program Pilot for Section 508 accessibility testing aims to help CMS technical leads and ADOs analyze test result documents more effectively. This 90-day pilot program aims to improve the CMS Section 508 Program.

**Priority Score Timeliness - forecast the time needed to work on an alert produced by Fraud Prevention System (Random Forest, Decision Tree, Gradient Boost, Generalized Linear Regression)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Priority Score Timeliness tool predicts the amount of time required to address alerts generated by the Fraud Prevention System, using various algorithms such as Random Forest, Decision Tree, Gradient Boost, and Generalized Linear Regression. The tool takes inputs such as Medicare Claims data, TPE Data, and Jurisdiction information, and outputs a forecasted time estimate for each alert.

**Medicaid And CHIP Financial (MACFin) Anomaly Detection Model for DSH Audit**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The MACFin AI team created a machine learning model that predicts anomalies in DSH audit data. The model identifies the top outliers in the data, allowing for targeted investigations and minimizing overpayment or underpayment. This helps ensure accurate amounts redistribution.

**Amazon Lex and Amazon Polly for the Marketplace Appeals Call Center**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> Amazon Lex and Amazon Polly are utilized in the Marketplace Appeals Call Center to enhance customer service. Amazon Lex provides self-service options with virtual agents and automated responses, while Amazon Polly converts text into speech, enabling the creation of speech-enabled applications and products.

**Brand vs Generic Market Share**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> This analysis compares the market shares of generic drugs and brand drugs over time and uses Part D claims volume to predict future market shares. It aims to provide insights into the competition between generic and brand drugs in the market.

**Center for Program Integrity (CPI) Fraud Prevention System Models (e.g. DMEMBITheftML, HHAProviderML)**, _Department_of_Health_and_Human_Services | Centers for Medicare & Medicaid Services (CMS) | Centers for Medicare & Medicaid Services (CMS)_
> The Center for Program Integrity (CPI) uses data models like DMEMBITheftML and HHAProviderML to detect fraud, waste, and abuse in Medicare. These models analyze administrative and claims data to identify potential cases and alert investigators for further investigation.

**BEST Platform improves post-market surveillance efforts through the semi-automated detection, validation and reporting of adverse events.**, _Department_of_Health_and_Human_Services | FDA | CBER/OBPV/DABRA_
> The BEST Platform is a system that enhances post-market surveillance by automating the identification, confirmation, and documentation of adverse events. It utilizes advanced technologies such as machine learning and natural language processing to detect possible adverse events from electronic health records and extract relevant information for clinicians to verify.

**To develop novel approaches to expand and/or modify the vaccine AESI phenotypes in order to further improve adverse event detection**, _Department_of_Health_and_Human_Services | FDA | CBER/OBPV/DABRA_
> The goal is to enhance the identification of adverse events in vaccines by creating a BERT-like machine learning model. This model will be trained using clinical documents from UCSF, focusing on language relevant to adverse event detection.

**Process Large Amount of Submitted Docket Comments**, _Department_of_Health_and_Human_Services | FDA | CBER/OBPV/DABRA_
> A system is needed to automate the handling of a high volume of docket comments by utilizing AI/ML technology. This system will transfer the comments, remove duplicates, provide summaries, and group similar comments together.

**Development of Machine Learning Approaches to Population Pharmacokinetic Model Selection and Evaluation of Application to Model-Based Bioequivalence Analysis**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Generic Drugs_
> The text describes the development of two approaches to population pharmacokinetic model selection. The first approach involves using deep learning/reinforcement learning, while the second approach utilizes a genetic algorithm implemented in Python. Both approaches aim to improve model-based bioequivalence analysis.

**Developing Tools based on Text Analysis and Machine Learning to Enhance PSG Review Efficiency**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Generic Drugs_
> The text discusses the development of tools for enhancing PSG (Polysomnography) review efficiency. This involves creating a neural summarization model that combines extractive and abstractive summarization techniques, along with an information retrieval system. The model is evaluated using PSG data and a large dataset from CNN/Daily Mail, and an open-source software package is developed for these tools.

**Machine-Learning based Heterogeneous Treatment Effect Models for Prioritizing Product-Specific Guidance Development**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Generic Drugs_
> The project aims to develop a machine learning algorithm for prioritizing the development of product-specific guidance (PSG) by estimating treatment effects. The algorithm will address the problem of confounders in observational data and use variational autoencoders to estimate both the hidden confounders and treatment effects simultaneously. The model will be evaluated on synthetic datasets, treatment effect benchmarks, and real-world PSG data, including publicly available databases and internal submission data. Collaboration with the FDA team will help validate the model and explain its output for specific PSGs.

**BEAM (Bioequivalence Assessment Mate) - a Data/Text Analytics Tool to Enhance Quality and Efficiency of Bioequivalence Assessment**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Generic Drugs_
> The goal is to create BEAM, a tool that uses data analytics, text mining, and artificial intelligence to improve the quality and efficiency of bioequivalence assessments. By automating labor-intensive tasks and utilizing machine learning, this tool will help regulatory assessments become more streamlined and effective.

**Application of Statistical Modeling and Natural Language Processing for Adverse Event Analysis**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of New Drugs_
> This project aims to develop new tools and methods for monitoring the safety of marketed drugs by using natural language processing and data mining to extract information from approved drug labeling. The goal is to determine when adverse events are generally labeled and identify patterns of detection within the first 3 years of marketing novel drugs. Funding will support an ORISE fellow for this project.

**Centers of Excellence in Regulatory Science and Innovation (CERSI) project - Leveraging AI for improving remote interactions.**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Pharmaceutical Quality (OPQ)_
> The Centers of Excellence in Regulatory Science and Innovation (CERSI) project aims to improve transcription, translation, document and evidence management, and co-working space in the FDA. They plan to use transformer-based sequence to sequence (seq2seq) models for automatic speech recognition and machine translation. To ensure accuracy and adaptability, the models will be manually trained and fine-tuned on specific datasets, including those with different accents and specialized regulatory and scientific terminologies. Additionally, the project includes the implementation of a system to manage documents and evidence, allowing for easy sharing and access among participants.

**Opioid Data Warehouse Term Identification and Novel Synthetic Opioid Detection and Evaluation Analytics**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Strategic Programs (OSP)_
> The Term Identification and Novel Synthetic Opioid Detection and Evaluation Analytics is a system that utilizes social media and forensic chemistry data to identify new references to drug products in social media text. It employs the FastText library to create vector models of known NSO-related terms in a large social media corpus, offering similarity scores and prevalence estimates for lists of terms to improve data gathering in the future.

**MASTER PLAN Y4**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The Master Plan for the Sentinel Innovation Center outlines a five-year roadmap to integrate patient-level EHR data into the Sentinel System. This will allow for more in-depth investigations of medication outcomes using clinical data not typically found in insurance claims. The plan focuses on four strategic areas, including data infrastructure, feature engineering, causal inference, and detection analytics, and aims to enhance Sentinel's capabilities through the use of emerging technologies.

**Information Visualization Platform (InfoViP) to support analysis of individual case safety reports**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The Information Visualization Platform (InfoViP) has been created to enhance the analysis of individual case safety reports (ICSRs). InfoViP utilizes artificial intelligence and advanced visualizations to identify duplicate ICSRs, generate temporal data visualizations, and categorize ICSRs for improved usability, thereby streamlining the review and evaluation process.

**Empirical evaluation of EHR-based signal detection approaches**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The project aims to develop methods to analyze electronic health record (EHR) data and identify signals related to outcomes that can only be identified through EHR data, such as natural language processing and laboratory values. This will involve abstracting and combining both structured and unstructured EHR data, as well as expanding existing methods to include these additional sources of information.

**Augmenting date and cause of death ascertainment in observational data sources**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The objective of this project is to develop algorithms that can improve the assessment of mortality in observational data sources by linking them with electronic health records (EHRs). This is important for ensuring the accuracy of investigations using mortality as an endpoint. The algorithms developed can also be used to supplement death ascertainment in claims data and can detect the date of death and estimate the cause of death using publicly available data and healthcare system information.

**Artificial Intelligence-based Deduplication Algorithm for Classification of Duplicate Reports in the FDA Adverse Event Reports (FAERS)**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The FDA uses an artificial intelligence-based deduplication algorithm to identify duplicate reports in the FAERS. This algorithm processes both structured and unstructured data, including free text narratives, to extract relevant clinical features and link duplicate reports together. This helps optimize data mining in the entire FAERS database.

**Artificial Intelligence (AI) Supported Annotation of FAERS Reports**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The text describes the development of a software application that uses artificial intelligence to categorize FAERS reports, which contain information about medication errors. The application uses computational algorithms and natural language processing to automatically classify reports and identify the type of medication error. An active learning approach is used to continuously improve the accuracy of the algorithms.

**Creating a development network**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The project aims to convert structured data from electronic health records and linked claims into a common data model at each site. Additionally, it seeks to develop a standardized process for storing free text notes and extracting metadata from these notes to make it easier for investigators to access and complete future tasks.

**Using Unsupervised Learning to Generate Code Mapping Algorithms to Harmonize Data Across Data Systems**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> This project aims to evaluate the effectiveness of unsupervised learning in identifying and minimizing coding differences in healthcare systems. The findings will be used to create computational tools that can transfer knowledge between different sites, enabling automated harmonization of electronic health records data.

**Onboarding of EHR data partners**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> In the DI6 project, structured data from electronic health records (EHRs) and linked claims data from two commercial data partners will be converted to the Sentinel Common Data Model (SCDM), which has been successfully used in the Sentinel system for over a decade. This conversion will allow Sentinel investigators to directly work with the data, adapt existing analytic programs, and test algorithms, which is crucial for meeting the goals of the 5-year Sentinel System strategic plan of 2019.

**Label comparison tool to support identification of safety-related changes in drug labeling**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> The label comparison tool is an AI-powered tool designed to help humans review and compare drug labeling in PDF format. It specifically focuses on identifying safety-related changes that occur over time. The tool's BERT natural language processing has been trained to detect potential text related to newly added safety issues in drug labeling.

**Scalable automated NLP-assisted chart abstraction and feature extraction tool**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Surveillance and Epidemiology (OSE)_
> This study aims to demonstrate the usability and value of electronic medical records by using various data sources and techniques. It will utilize real-world longitudinal data from the Cerner Enviza Electronic Health Records, linked to claims, and employ NLP technology to analyze physician notes. The study will focus on examining the connection between the use of montelukast among asthma patients and neuropsychiatric events.

**Community Level Opioid Use Dynamics Modeling and Simulation**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Translational Sciences_
> The OUD project uses artificial intelligence techniques like Agent-Based Modeling (ABM) and Machine Learning (ML) to simulate and analyze the dynamics of opioid use in communities. By examining various factors such as geography and social influences, the project aims to understand how opioids spread and their overall impact. ML is also used to classify data entry types, helping generate accurate training data.

**Automatic Recognition of Individuals by Pharmacokinetic Profiles to Identify Data Anomalies**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Translational Sciences/Office of Biostatistics_
> The text explains that a new R shiny application called DABERS has been created to detect data anomalies in ANDA. However, there is a difficulty in assessing the validity of pharmacokinetic profiles and determining how similar they should be. To address this gap, the project aims to use machine learning and data augmentation to identify potential data manipulations and provide a data-driven method for modeling complex patterns of PK data. This will have regulatory benefits in identifying anomalies under an ANDA and also contribute to public health research and drug development by understanding drug response variability and guiding patient subgroup targeting.

**CluePoints CRADA**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Translational Sciences/Office of Biostatistics_
> The CluePoints CRADA project aims to detect and identify data anomalies in clinical trial data using unsupervised machine learning. It will focus on improving data quality and integrity, aiding site selection for inspection, and helping reviewers identify potentially problematic sites for sensitivity analyses.

**Clinical Study Data Auto-transcribing Platform (AI Analyst) for Generating Evidence to Support Drug Labelling**, _Department_of_Health_and_Human_Services | FDA | CDER/Office of Translational Sciences/Office of Clinical Pharmacology_
> The AI Analyst platform transcribes clinical study data into reports to assess the evidence supporting drug labelling. It mimics the thinking process of subject matter experts and can handle various study designs and data formats. The platform has been tested with numerous submissions and clinical trials and will be used to support the review of certain submissions in the future.

**Data Infrastructure Backbone for AI applications**, _Department_of_Health_and_Human_Services | FDA | CFSAN /OFAS_
> OFAS is creating a data lake called WILEE knowledgebase to integrate data from various sources for advanced analytics in risk-based decision making. This data store allows for automated ingestion of new data and manual curation when needed, and can also incorporate new data sources as they become available. The data lake centralizes information on CFSAN regulated products, food additives, color additives, GRAS substances, and food contact substances, integrating them with stakeholder submission information and cheminformatics data for more efficient knowledge discovery in reviewing premarket submissions and monitoring the US food supply.

**AI Engine for Knowledge discovery, Post-market Surveillance and Signal Detection**, _Department_of_Health_and_Human_Services | FDA | CFSAN/OFAS_
> The use of Artificial Intelligence in post-market surveillance and signal detection will improve CFSAN's ability to detect potential problems with their commodities. The OFAS Warp Intelligent Learning Engine (WILEE) project aims to establish an intelligent knowledge discovery and analytic agent for the Office, providing them with a proactive approach to industry trends and a data-driven decision-making tool.

**Emerging Chemical Hazard Intelligence Platform (ECHIP - completed)**, _Department_of_Health_and_Human_Services | FDA | CFSAN/OFAS_
> The Emerging Chemical Hazard Intelligence Platform (ECHIP) is an AI solution that uses data from news, social media, and scientific literature to identify potential chemical hazards and stakeholder concerns. By automatically reviewing and analyzing data, ECHIP can significantly reduce the time it takes for signal detection and validation from weeks to just a few hours.

**SSTAT**, _Department_of_Health_and_Human_Services | FDA | CTP/OS/DRSI_
> SSTAT is a self-service text analytics tool that allows users to analyze the topics of a group of documents. By submitting documents to the tool, users can generate a list of topics and keywords associated with them. The tool also provides a visual representation of the submitted documents and their respective topics, aiding in quickly understanding the content.

**ASSIST4TOBACCO**, _Department_of_Health_and_Human_Services | FDA | CTP/OS/DRSI_
> ASSIST4Tobacco is a search system that uses semantic technology to assist CTP stakeholders in finding tobacco authorization applications with improved accuracy and efficiency.

**OSCAR**, _Department_of_Health_and_Human_Services | FDA | CTP/OS/DRSI_
> OSCAR is a chatbot designed to assist customers in getting help from the Customer Service Center. It provides a user-friendly interface for users to ask questions, access past responses, and offers an admin dashboard with important metrics.

**Using XGBoost Machine Learning Method to Predict Antimicrobial Resistance from WGS data**, _Department_of_Health_and_Human_Services | FDA | CVM_
> The XGBoost machine learning method is utilized to predict antimicrobial resistance in various bacteria strains such as Salmonella, E. coli, Campylobacter, and Enterococcus. This model aims to enhance predictions by predicting the antimicrobial Minimum Inhibitory Concentrations (MICs) based on genomic data obtained from retail meats, humans, and food producing animals.

**Predictive toxicology models of drug placental permeability using 3D-fingerprints and machine learning**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> This text discusses the importance of predicting the placental permeability of drugs during pregnancy in order to ensure fetal safety and inform regulatory decisions. The proposal suggests using 3D-molecular similarities of placental transporter ligands to known drug substrates to build predictive models for drug transportation across the placenta. This will be validated using blind test sets and empirical assessments.

**Prediction of adverse events from drug - endogenous ligand - target networks generated using 3D-similarity and machine learning methods.**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> The use of molecular similarity has been a valuable tool in various areas of drug development and toxicology. However, predicting adverse drug reactions remains challenging due to the complexity of the biological pathways involved. A new approach using three-dimensional fingerprints encoding both structural and functional similarities between molecules shows promise in predicting adverse events and uncovering unknown mechanisms of toxicity.

**Identification of sex differences on prescription opioid use (POU)-related cardiovascular risks by big data analysis**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> The text discusses the need to identify and analyze sex differences in prescription opioid use (POU)-related cardiovascular risks. Currently, there is a lack of systematic knowledge regarding risk factors associated with increased cardiotoxicity in women. The proposed project aims to develop a statistical model using big data methods and AI-related tools to detect safety signals in POU and identify sex-dependent differences in risk factors for cardiotoxicity. The results of this research may help the FDA and physicians prevent or reduce the risk of POU drug-induced cardiovascular disease in women.

**Assessing and mitigating bias in applying Artificial Intelligence (AI) based natural language processing (NLP) of drug labeling documents**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> This text discusses the concerns about bias in AI systems used for natural language processing of drug labeling documents. It emphasizes the need to properly evaluate AI models trained on different domains to avoid potential bias risks. The proposal suggests conducting a study to understand and assess bias in this application and develop strategies to mitigate it.

**Artificial Intelligence (AI)-based Natural Language Processing (NLP) for FDA labeling documents**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> The FDA generates various unstructured documents during the product-review process, and analyzing the semantic relationships within these documents is crucial for extracting useful information and improving the review process. The use of AI-based NLP, specifically language models like BERT and BioBERT, can help interpret drug properties, provide text summaries, identify anomalies, and retrieve information from FDA labeling documents. This study aims to assess the effectiveness of AI-based NLP compared to the MedDRA approach and provide a benchmark for future development of the FDALabel tool.

**Opioid agonists/antagonists knowledgebase (OAK) to assist review and development of analgesic products for pain management and opioid use disorder treatment**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> The Opioid agonists/antagonists knowledgebase (OAK) is being developed to assist in the review and development of analgesic products for pain management and opioid use disorder treatment. The knowledgebase will curate experimental data on opioid agonist/antagonist activity, test drugs using a quantitative high-throughput screen, and develop predictive models. This knowledgebase will help inform regulatory review and support the development of abuse-deterrent analgesic products to combat the opioid crisis.

**Towards Explainable AI: Advancing Predictive Modeling for Regulatory Use**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> This text discusses the challenges faced by the FDA in assessing and evaluating AI products and implementing AI methods to improve their operations. It highlights the importance of understanding the underlying features driving AI performance and the need for interpretability in regulatory applications. The text proposes investigating representative AI methods to determine their performance and interpretability and provide guidance on developing explainable AI models for decision making in regulatory settings.

**Development of virtual animal models to simulate animal study results using Artificial Intelligence (AI)**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> The development of virtual animal models using AI can simulate animal study results, providing crucial evidence for the safety evaluation of chemicals. By learning from existing animal studies, these models can generate data for new and untested chemicals without the need for further animal experiments, aligning with the FDA's goal to reduce animal testing and enhance toxicology predictive capabilities.

**Informing selection of drugs for COVID-19 treatment by big data analytics and artificial intelligence**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> This text discusses the need to find safe and effective drugs for the treatment of COVID-19. Currently, many drugs are being repurposed, but more evidence is needed to determine their safety and efficacy. The proposed project aims to use artificial intelligence and big data analytics to mine adverse drug events and provide detailed information that can be used to evaluate the safety of repurposed drugs and select the right drugs for COVID-19 treatment.

**Identify sex disparities in opioid drug safety signals in FDA adverse events report systems (FAERS) and social media Twitter to improve women health**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> This proposal seeks to analyze sex disparities in adverse events related to opioid drugs using data from social media (Twitter) and the FDA Adverse Events Report Systems (FAERS). By comparing the findings from both sources, the study aims to determine if Twitter data can serve as an early warning system for identifying opioid-related issues specific to women. The results of this project have the potential to contribute to improving women's health.

**NCTR/DBB-CDER/OCS collaboration on A SafetAI Initiative to Enhance IND Review Process**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> The NCTR/DBB-CDER/OCS collaboration is working on a project called A SafetAI Initiative to improve the IND review process. They are using artificial intelligence (AI) and machine learning (ML) to develop animal-free models for toxicity assessments and to predict safety outcomes based on chemical structure data. Specifically, they will be using deep learning (DL), which is a highly advanced form of ML, to identify safety concerns related to drug-induced liver injury (DILI) and carcinogenicity.

**Development of a Comprehensive Open Access Molecules with Androgenic Activity Resource (MAAR) to Facilitate Assessment of Chemicals**, _Department_of_Health_and_Human_Services | FDA | NCTR_
> The development of a Comprehensive Open Access Molecules with Androgenic Activity Resource (MAAR) is essential for evaluating the safety of drugs and chemicals. The resource will provide open access to androgenic activity data that is currently scattered across various sources, making it difficult to use for research and regulation. MAAR will comply with FAIR principles and support research on androgenic activity and regulatory decision making.

**individual Functional Activity Composite Tool (inFACT)**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) CC_
> The individual Functional Activity Composite Tool (inFACT) is being created to help Social Security Administration adjudicators review extensive case records and find evidence on an individual's functional abilities. inFACT presents a summary of a person's overall functioning, extracted from their medical records and linked to important business factors.

**Assisted Referral Tool**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) CSR_
> The Assisted Referral Tool is designed to help assign suitable scientific areas for grant applications. It offers assistance in the process of determining the most appropriate areas for these applications.

**NanCI: Connecting Scientists**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NCI_
> NanCI is an AI-powered platform that connects scientists by matching scientific content to their interests. Users can collect papers into a folder and use the tool to find similar articles in the scientific literature. They can also connect with other users based on shared interests and receive or make recommendations through the platform's social network.

**Detection of Implementation Science focus within incoming grant applications**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NHLBI_
> A tool utilizing natural language processing and machine learning has been developed to analyze incoming grant applications and determine if they incorporate Implementation Science. The resulting IS score is utilized by NHLBI to assign applications to specific divisions for grants management oversight and administration.

**Division of Allergy, Immunology, and Transplantation (DAIT) AIDS-Related Research Solution**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIAID_
> The Division of Allergy, Immunology, and Transplantation (DAIT) has developed a solution to aid in AIDS-related research. This tool utilizes natural language processing, text extraction, and classification algorithms to predict the priority level and research area for grant applications. By ranking the applications based on these predictions, the tool helps prioritize higher-ranked applications for review.

**Tuberculosis (TB) Case Browser Image Text Detection**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIAID_
> The TB Case Browser Image Text Detection is a tool designed to identify text in images within TB Portals that may contain Personally Identifiable Information or Protected Health Information. Its purpose is to ensure the privacy and security of sensitive data related to tuberculosis cases.

**Scientific Research Data Management System Natural Language Processing Conflict of Interest Tool**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIAID_
> The Scientific Research Data Management System Natural Language Processing Conflict of Interest Tool is designed to help the NIAID's Scientific Review Program team in identifying conflicts of interest between grant reviewers and applicants. This tool uses NLP techniques such as OCR and text extraction to identify entities within grant applications, making the identification process more efficient.

**Research Area Tracking Tool**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIAID_
> The Research Area Tracking Tool is a dashboard that utilizes machine learning to easily identify projects in specific high-priority research areas. With its advanced technology, the tool effectively assists in tracking and monitoring these projects.

**Federal IT Acquisition Reform Act (FITARA) Tool**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIAID_
> The Federal IT Acquisition Reform Act (FITARA) Tool automates the process of identifying IT-related contracts within the National Institute of Allergy and Infectious Diseases (NIAID). It simplifies the task by streamlining the identification process for NIAID contracts that fall under the IT category.

**NIDCR Data Bank**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIDCR_
> The NIDCR Data Bank project allows researchers to transfer large amounts of unorganized data into a cloud-based storage system. This storage system is scalable, cost-effective, and includes metadata for efficient management and governance. Additionally, it offers opportunities for further analysis using artificial intelligence and machine learning tools.

**NIDCR Digital Transformation Initiative (DTI)**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIDCR_
> The NIDCR Digital Transformation Initiative aims to enhance the productivity, clarity, and uniformity for NIDCR employees by developing a chatbot that utilizes natural language processing technology. This initiative intends to streamline operations, enhance communication, and maintain consistency within the organization.

**Splunk IT System Monitoring Software**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIEHS_
> Splunk IT System Monitoring Software is a powerful tool that utilizes machine learning to gather system logs from on-premises IT infrastructure systems and endpoints. It is specifically designed for auditing and cybersecurity monitoring purposes, providing comprehensive monitoring and analysis capabilities.

**SWIFT Active Screener**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIEHS_
> The SWIFT Active Screener is a tool that uses statistical models to help screeners in the Division of Translational Toxicology save time and effort. It utilizes user feedback to automatically prioritize studies and supports literature screening for evidence evaluations.

**Automated approaches for table extraction**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIEHS_
> The project aimed to create automated methods for extracting data from tables, reducing the need for manual extraction. Tables in published research are complex and contain important data, making them a challenging but valuable source of information.

**Similarity-based Application and Investigator Matching (SAIM)**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIGMS_
> The SAIM system utilizes natural language processing to detect non-NIH grants received by NIGMS Principal Investigators. It helps identify if a grant application has excessive overlap with another agency's funded grant.

**Clinical Trial Predictor**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIGMS_
> The Clinical Trial Predictor utilizes various natural language processing and machine learning algorithms to forecast whether applications may contain clinical trials based on the content of their titles, abstracts, narratives, specific aims, and research strategies.

**Stem Cell Auto Coder**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIGMS_
> The Stem Cell Auto Coder is a tool that uses advanced technology to determine the specific type of stem cell research an application is related to. It can accurately predict whether the research involves human or non-human embryonic cells, human or non-human induced pluripotent cells, or human or non-human non-embryonic cells.

**JIT Automated Calculator (JAC)**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NIGMS_
> The JIT Automated Calculator (JAC) utilizes natural language processing to analyze Just-In-Time (JIT) Other Support forms and calculate the amount of external support received by Principal Investigators (PIs) apart from the pending application.

**SingleCite: Improving single citation search in PubMed**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> SingleCite is an automated algorithm developed by NLM to improve single citation searches in PubMed. It establishes a query-document mapping by predicting the probability of a retrieved document being the target based on three variables. SingleCite has demonstrated superior performance in benchmarking experiments and is used to rescue queries that would otherwise fail.

**NLM-Gene: towards automatic gene indexing in PubMed articles**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> NLM-Gene is an automatic tool developed by NLM to assist in the manual gene indexing process in PubMed articles. It uses advanced natural language processing and deep learning methods to find gene names in biomedical literature. The tool's performance has been evaluated on gold-standard datasets and is planned to be integrated into the production MEDLINE indexing pipeline.

**NLM-Chem: towards automatic chemical indexing in PubMed articles**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The NLM has developed an automatic tool called NLM-Chem to assist in the manual process of chemical indexing in PubMed articles. This tool uses advanced natural language processing and deep learning methods to find chemical names in biomedical literature. After being assessed on evaluation datasets, NLM-Chem will be integrated into the production MEDLINE indexing pipeline.

**Best Match: New relevance search for PubMed**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> PubMed, a widely-used search engine for biomedical literature, has introduced a new relevance search algorithm called Best Match. This algorithm, developed by NLM, uses machine-learning technology and user input to help users find the most relevant papers for their queries, overcoming the challenge posed by the increasing volume of biomedical literature. Best Match replaces the traditional date sort order, offering an alternative approach to retrieving information.

**Remediate Adobe .pdf documents to be more accessible**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The NLM is using AI to make Adobe .pdf documents adhere to Section 508 accessibility standards, allowing them to be released to the public. This initiative aims to improve accessibility for individuals who are blind and rely on assistive technology for reading.

**Biomedical Citation Selector (BmCS)**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The Biomedical Citation Selector (BmCS) automates the selection of articles, enabling the National Library of Medicine (NLM) to efficiently index and provide relevant information to the public. This automation helps NLM standardize article selection and decrease the processing time for MEDLINE articles.

**MetaMap**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> MetaMap is a program that allows access to concepts in the UMLS Metathesaurus from biomedical text. It uses NLP to link the text to the knowledge in the Metathesaurus, including synonymy relationships. MTI uses MetaMap to generate indexing terms.

**CLARIN: Detecting clinicians' attitudes through clinical notes**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The NLM is utilizing AI techniques to detect and comprehend the emotions, biases, and burnout of clinicians through clinical notes. This is crucial for promoting equity, diversity, and quality of care and decision support in the healthcare field.

**Pangolin lineage classification of SARS-CoV-2 genome sequences**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The PangoLEARN machine learning tool can classify SARS-CoV-2 genome sequences into specific lineages. This classification helps users find sequences based on their lineage and track lineages that may have mutations affecting treatments and vaccine effectiveness.

**MTIX**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> MTIX is a machine learning system that automates the indexing of MEDLINE articles with MeSH terms. It uses a multi-stage neural text ranking approach to achieve automated indexing, resulting in cost-effective and timely indexing of the articles.

**MEDIQA: Biomedical Question Answering**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The MEDIQA project focuses on using AI techniques to automate question answering in the biomedical field. By utilizing NLM knowledge sources and traditional as well as neural machine learning, the project aims to enhance accessibility to NLM resources by providing a single-entry access point.

**Computed Author: author name disambiguation for PubMed**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The National Library of Medicine (NLM) has developed a machine-learning method to accurately distinguish between authors with the same name in PubMed. They use a scoring system to identify and classify ambiguous pairs of papers, and then employ clustering to group all papers belonging to the same authors. This method has been integrated into PubMed to improve the accuracy of author name searches.

**CylanceProtect**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> CylanceProtect is a software that offers protection for Windows and Mac endpoints against cyberthreats. It safeguards these devices from potential security breaches and ensures their safety from malicious attacks.

**ClinicalTrials.gov Protocol Registration and Results System Review Assistant**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) NLM_
> The research project seeks to determine if incorporating AI into the review process of study records on ClinicalTrials.gov would improve efficiency and effectiveness. It aims to assist ClinicalTrials.gov in evaluating the potential benefits of AI integration.

**HIV-related grant classifier tool**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/DPCPSI/OAR_
> The HIV-related grant classifier tool is a front-end application that allows scientific staff to input grant information and automatically classifies them using an algorithm. It also includes interactive data visualization, such as a heat map, to display the confidence level of predicted grants using the Plotly Python library.

**Person-level disambiguation for PubMed authors and NIH grant applicants**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/DPCPSI/OPA_
> The accurate identification of individual researchers is crucial for analyzing grants, articles, and other outputs at a person-level. A new and enhanced method has been developed to disambiguate authors in PubMed articles and NIH grant applications, providing valuable information for making data-based decisions.

**Machine learning system to predict translational progress in biomedical research**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/DPCPSI/OPA_
> A machine learning system has been developed to determine the likelihood of a research paper being cited by a future clinical trial or guideline, enabling real-time assessment and prediction of translational progress in biomedicine. The system analyzes the early reaction of the scientific community to a paper to gather information.

**Automated approaches to analyzing scientific topics**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/DPCPSI/OPA_
> A validated approach has been developed and implemented to analyze scientific topics using natural language processing and AI/ML. This approach effectively groups similar documents such as grants, publications, or patents and extracts accurate AI labels that reflect the scientific focus of each topic. This aids in the analysis of NIH research portfolios.

**Machine learning pipeline for mining citations from full-text scientific articles**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/DPCPSI/OPA_
> The NIH Office of Portfolio Analysis has developed a machine learning pipeline that identifies freely available scientific articles on the internet. This pipeline converts full-text pdfs to xml and uses a Long Short-Term Memory (LSTM) recurrent neural network model to distinguish reference text from other text in the articles. The identified references are then processed through the Citation Resolution Service.

**Identification of emerging areas**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/DPCPSI/OPA_
> An AI/ML-based approach has been developed to determine the age and rate of progress of topics in NIH portfolios. This method can effectively identify emerging areas of research and contribute to speeding up scientific progress.

**Prediction of transformative breakthroughs**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/DPCPSI/OPA_
> The ability to predict scientific breakthroughs on a large scale would speed up the process of discovery and make research investments more effective. Through the identification of common signatures in co-citation networks, this initiative has successfully predicted breakthroughs in biomedicine more than 5 years before they are announced through subsequent publications. A patent application has been filed for this approach.

**NIH Grants Virtual Assistant**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/OER_
> The NIH has developed a virtual assistant called Chat Bot to help users in locating grant-related information through OER resources. This virtual assistant aims to provide assistance and guidance to individuals seeking information about grants.

**Internal Referral Module (IRM)**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/OER_
> The Internal Referral Module (IRM) is a system that uses Artificial Intelligence and Natural Language Processing to automate the prediction of grant applications to NIH Institutes and Centers Program Officers. This technology helps Program Officers make more informed decisions about which applications to consider.

**Research, Condition, and Disease Categorization (RCDC) AI Validation Tool**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/OER_
> The RCDC AI Validation Tool aims to verify the accuracy and comprehensiveness of RCDC categories for public data reporting. Its purpose is to ensure that research, condition, and disease categorization are reliable and complete.

**Chiller Plant Optimization**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/ORF_
> The goal of the Chiller Plant Optimization project is to decrease the amount of energy required to produce chilled water and cool the NIH campus. By implementing efficiency measures, the project aims to reduce energy usage and improve overall sustainability.

**Tool for Nature Gas Procurement Planning**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/ORF_
> The tool for natural gas procurement planning allows NIH to create a plan for acquiring natural gas and determine feasible price goals using up-to-date long-term projections. This tool helps in efficiently managing the procurement process for natural gas.

**NIH Campus Steam Demand Forecaster**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/ORF_
> The NIH Campus Steam Demand Forecaster predicts the steam demand for the next four days on the NIH campus. This helps the stakeholders at the NIH Central Utilities Plant to efficiently plan and optimize their operations and maintenance tasks.

**Natural Language Processing Tool for Open Text Analysis**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/ORF_
> This project aims to enhance facility readiness and minimize downtime by enabling software to analyze previously inaccessible data stored in open text. With the help of a Natural Language Processing tool, valuable insights can be extracted from this data, improving overall operational efficiency.

**NIH Campus Cooling Load Forecaster**, _Department_of_Health_and_Human_Services | NIH | National Institutes of Health (NIH) OD/ORF_
> The NIH Campus Cooling Load Forecaster predicts the amount of chilled water that will be needed on the NIH campus for the next four days. This helps the management of the central utilities plant to efficiently plan and optimize the operation and maintenance of the chiller plant.

**Contracts and Grants Analytics Portal**, _Department_of_Health_and_Human_Services | OIG | OIG_
> The Contracts and Grants Analytics Portal is an AI-powered tool that helps HHS OIG staff access grants data effortlessly. It enables them to quickly find relevant audit findings, discover similar patterns, analyze trends, compare data between different divisions, and assess potential anomalies among grantees.

**Text Analytics Portal**, _Department_of_Health_and_Human_Services | OIG | OIG_
> The text analytics portal enables non-analytics professionals to efficiently analyze text documents using search, topic modeling, and entity recognition technologies. It is primarily designed for HHS-OIG specific scenarios in its initial implementation.

## Department_of_Commerce

**Chatbot Pilot**, _Department_of_Commerce | International Trade Administration (ITA)_
> A chatbot will be integrated into trade.gov, providing assistance to ITA clients by answering frequently asked questions, helping them locate information and content, and suggesting events and services. By inputting questions or responding to prompts, ITA clients can receive answers and suggestions tailored to their specific roles as exporters, foreign buyers, or investors, as the chatbot scans through ITA content libraries and information from ITA staff.

**Market Diversification Toolkit**, _Department_of_Commerce | International Trade Administration (ITA)_
> The Market Diversification Toolkit is a tool that helps users identify potential new export markets based on their current trade patterns. By entering the products they make and the markets they currently export to, the tool utilizes a ML algorithm to compare and identify markets that should be considered. The tool provides a comprehensive analysis by combining product-specific trade and tariff data with macroeconomic and governance data, allowing users to narrow down the results and modify the indicators to fit their needs. Additionally, users have the option to export all the data to a spreadsheet for further analysis.

**AD/CVD Self Initiation**, _Department_of_Commerce | International Trade Administration (ITA)_
> The AD/CVD program investigates dumping and countervailing duties based on complaints filed by US entities. Self-initiation allows the ITA to proactively monitor trade patterns and initiate investigations before the harmed entities are even aware of the harm.

**B2B Matchmaking**, _Department_of_Commerce | International Trade Administration (ITA)_
> The B2B matchmaking system utilizes algorithms and AI technology to analyze data and create matches between event participants based on their individual requirements and available opportunities. The system takes in information about the participants and provides suggested B2B matches along with a scorecard indicating the strength of the match.

**Consolidated Screening List**, _Department_of_Commerce | International Trade Administration (ITA)_
> The Consolidated Screening List is a compilation of 13 different export screening lists maintained by the US Government. It restricts certain exports, reexports, or transfers of items to parties on the list. The CSL search engine has a "Fuzzy Name Search" feature that allows users to search for entities without knowing the exact spelling of their name, which is useful for names translated into English from non-Latin alphabet languages.

**Azure Chatbot**, _Department_of_Commerce | Minority Business Development Administration (MBDA)_
> Azure Chatbot is used to automate and streamline user responses to potential questions on the MBDA website. It incorporates AI-based chatbot responses along with Machine Learning and Natural Language Processing capabilities.

**Coastal Change Analysis Program (C-CAP)**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The Coastal Change Analysis Program (C-CAP) has been using geographic object-based image analysis and ML algorithms to classify coastal land cover from high-resolution imagery since 2015. They have recently started using a CNN approach for deriving the impervious surface component of their land cover products. Prior to this, C-CAP focused on developing moderate resolution land cover using Landsat data, adopting a methodology that employed Classification and Regression Trees in 2002.

**CoralNet: Ongoing operational use, improvement, and development, of machine vision point classification**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> CoralNet is a point annotation software used for classifying benthic photos. The development of classifiers has reduced the need for human annotation, and ongoing improvements and developments are being made to the software.

**Fisheries Electronic Monitoring Image Library**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The Fisheries Electronic Monitoring Library (FEML) is a central database that stores electronic monitoring data on marine life. It serves as a repository for information related to the monitoring of fisheries.

**Automated detection of hazardous low clouds in support of safe and efficient transportation**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> This project aims to maintain and improve the automated detection of dangerous low clouds to ensure safe and efficient transportation. It involves using machine learning to combine satellite imagery and NWP data to create FLS products that are regularly used by NWS Aviation Weather Center and Weather Forecast Offices.

**Picky**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The Picky system utilizes CNN (Convolutional Neural Network) to identify objects of a specific size in side scan imagery. It provides users with a probability that enables automated selection of contacts in real-time. Side scan imagery, being a one-channel intensity image, is well-suited for basic CNN techniques.

**Coral Reef Watch**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> NOAA Coral Reef Watch (CRW) has been using remote sensing, modeled, and in situ data for over 20 years to operate a Decision Support System (DSS) that helps resource managers, researchers, decision makers, and stakeholders worldwide prepare for and respond to coral reef ecosystem stressors caused by climate change and warming oceans. CRW offers the world's only global early-warning system for coral reef ecosystem changes, monitoring conditions that can lead to coral bleaching, disease, and death, and providing near real-time information and early warnings to users. Their products primarily focus on sea surface temperature but also include other variables such as light and ocean color.

**The Development of ProbSevere v3 - An improved nowcasting model in support of severe weather warning operations**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> ProbSevere v3 is an improved nowcasting model that uses various data sources to predict severe weather events such as wind, hail, and tornadoes. It has already been proven to enhance severe weather warnings and the development of the next version, ProbSevere v3, aims to further improve upon its capabilities by incorporating additional data sets and better machine learning techniques. The project has received funding from GOES-R and has successfully demonstrated its effectiveness in the 2021 Hazardous Weather Testbed, with a proposal submitted for an operational update.

**Edge AI survey payload development**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The development of a multispectral aerial imaging payload is being supported, which includes nine cameras and on-board computers with GPUs. The payload is capable of running detection models in real-time, allowing for immediate processing of imagery and reducing the amount of data and time needed for analysis of arctic mammals.

**SUVI Thematic Maps**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The SUVI Thematic Maps are produced by NOAA's National Centers for Environmental Information and are derived from Level 2 High Dynamic Range composite images from the GOES-16 Solar Ultraviolet Imager. These maps use a machine learning classifier to create a detailed map of solar features from all six SUVI spectral channels. Although considered experimental, future releases will improve these files, and users can contact the NCEI SUVI team for assistance.

**Steller sea lion brand sighting**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The text discusses the process of detecting and identifying branded steller sea lions from remote camera images in the western Aleutian Islands, AK. The objective is to simplify photo processing and decrease the workload needed for image review.

**Drought outlooks by using ML techniques**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> ML techniques, specifically simple neural networks and deep learning techniques, are being employed to generate drought outlooks using NCEP models. These techniques are focused on predicting precipitation and temperature over the CONUS region for a period of 1-5 weeks.

**Passive acoustic analysis using ML in Cook Inlet, AK**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> Passive acoustic data in Cook Inlet, AK is analyzed using machine learning techniques to detect and classify signals emitted by beluga whales. This analysis, done with a set of 4 CNN models and weighted scoring developed in collaboration with Microsoft, is providing valuable information on the seasonal distribution, habitat use, and impact of human disturbance on beluga whales in Cook Inlet. The project also aims to extend its scope to include other cetacean species and anthropogenic noise.

**Robotic microscopes and machine learning algorithms remotely and autonomously track lower trophic levels for improved ecosystem monitoring and assessment**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> Robotic microscopes and machine learning algorithms are used to track and monitor phytoplankton, which are crucial for marine food webs. The Imaging Flow Cytobot (IFCB) collects images of phytoplankton, and a random forest algorithm is used to identify and classify them. The IFCB is deployed on both fixed and roving platforms to monitor phytoplankton communities in different areas and assess their distribution, abundance, and changes in relation to ocean and climate variability.

**NN Radiation**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> NN radiation for GFS and GEFS, specifically in the LW and SW regions, has been successfully developed in previous versions of GFS. The stability and robustness of this approach have been demonstrated, and now efforts are underway to develop NN LW and SW radiations for the current versions of GFS and GEFS.

**NN training software for the new generation of NCEP models**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The NN training software is designed to enhance the NCEP EMC Training and Validation System by efficiently managing the high-resolution model data from the new NCEP operational models. This software aims to optimize the training process and ensure effective handling of the data produced by these advanced models.

**BANTER, a machine learning acoustic event classifier**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> BANTER is a machine learning acoustic event classifier that utilizes hierarchical random forests. This supervised system is designed to accurately classify and categorize acoustic events based on their audio characteristics.

**Ice seal detection and species classification in multispectral aerial imagery**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The aim of this study is to enhance the accuracy of ice seal detection and species classification in multispectral aerial imagery. This will be achieved by refining and improving the existing pipelines to decrease false positive rates to below 50%, while maintaining a high accuracy of over 90%. Additionally, the goal is to minimize the need for labor-intensive post-survey review by significantly reducing or eliminating it altogether.

**AI-based automation of acoustic detection of marine mammals**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The software INSTINCT was developed to automate the detection of marine mammals in passive acoustic data. Its purpose is to quickly process data and adapt mitigation measures to climate change impacts on Arctic marine mammals. Skilled operators familiar with INSTINCT, machine learning, and the acoustic repertoire of Alaska region marine mammals are needed for continued integration of AI methods into existing processes.

**Developing automation to determine species and count using optical survey data in the Gulf of Mexico**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The VIAME project aims to develop automation tools for analyzing optical survey data in the Gulf of Mexico. This includes building an image library of landed catch, developing machine learning and deep learning algorithms to identify and count species from underwater imagery, and creating automated processes to analyze imagery in near real time and store the information in a central database.

**First Guess Excessive Rainfall Outlook**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The First Guess Excessive Rainfall Outlook is a machine learning tool used by the WPC to predict excessive rainfall. It is based on data from the ERO and atmospheric variables, and is specifically designed for the Day 4-7 forecast period.

**VIAME: Video and Image Analysis for the Marine Environment Software Toolkit**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> VIAME is an open-source software toolkit called Video and Image Analysis for the Marine Environment. It allows users to use deep-learning algorithms to automatically annotate imagery using a graphical interface. It is free for all NOAA users and is supported by the NOAA Fisheries Office of Science and Technology.

**Steller sea lion automated count program**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The NOAA Fisheries Alaska Fisheries Science Center's Marine Mammal Laboratory conducts annual aerial surveys to monitor the endangered western Steller sea lion population in Alaska. They use visual imagery and manually process the images to count and classify individuals by age and sex class. They are now working with Kitware to develop detection and image registration pipelines to improve the counting method.

**EcoCast: A dynamic ocean management tool to reduce bycatch and support sustainable fisheries**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> EcoCast is an operational tool that uses boosted regression trees to model the distribution of swordfish and bycatch species in the California Current. Its purpose is to reduce bycatch and support sustainable fisheries by providing dynamic ocean management.

**ENSO Outlooks using observed/analyzed fields**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The ENSO Outlooks utilize a LSTM model that incorporates ocean and atmospheric predictors in the tropical Pacific to predict ONI values for up to one year in advance. A proposed extension of this model involves incorporating a CNN layer that utilizes reforecast data to enhance the accuracy of ONI forecasts.

**Using community-sourced underwater photography and image recognition software to study green sea turtle distribution and ecology in southern California**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> This project aims to study green sea turtles in the highly populated region of La Jolla Cove in southern California. By collaborating with local photographers, underwater images of the turtles are collected and analyzed using facial recognition software. This allows for the identification of individual turtles, helping to determine population size, residency patterns, and foraging habits.

**FathomNet**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> FathomNet offers training data, including annotated and localized imagery, for the development of machine learning algorithms. They have employed interns and incorporated college class curriculums to localize annotations on NOAA video data and train their own algorithms.

**ANN to improve CFS T and P outlooks**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The study by Fan et al. (2021) proposes the use of Artificial Neural Networks (ANN) to enhance the accuracy of Climate Forecast System (CFS) Week 3-4 forecasts for precipitation and temperature. The researchers suggest that incorporating ANN into the forecasting model can lead to improved predictions in these weather variables.

**First Guess Excessive Rainfall Outlook**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The First Guess Excessive Rainfall Outlook is a machine learning product used by the WPC to predict excessive rainfall. It is based on atmospheric variables and provides outlooks for the Day 1, 2, and 3 periods.

**The VOLcanic Cloud Analysis Toolkit (VOLCAT): An application system for detecting, tracking, characterizing, and forecasting hazardous volcanic events**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The VOLcanic Cloud Analysis Toolkit (VOLCAT) is an application system that uses artificial intelligence and satellite technology to detect, track, characterize, and forecast hazardous volcanic events, particularly volcanic ash. It includes several applications that are used by Volcanic Ash Advisory Centers to issue volcanic ash advisories. The project aims to further develop the VOLCAT products and integrate them into the NESDIS Common Cloud Framework to meet the new International Civil Aviation Organization requirements.

**Replacing unstructured WW3 in the Great Lakes with a Recurrent neural network and a boosted ensemble decision tree**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> The possibility of replacing unstructured WW3 in the Great Lakes with a Recurrent Neural Network (RNN) and a boosted ensemble decision tree (XGBoost) was investigated. The RNN, specifically an LSTM developed by EMC, and the XGBoost developed by GLERL were trained on two decades of wave observations in Lake Erie. The performance of these AI models was then compared to the operational unstructured WW3 in the Great Lakes.

**An Interactive Machine Learning Signals in Passive Acoustic Recordings Toolkit for Classifying Species Identity of Cetacean Echolocation**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> A toolkit is being developed to automate the detection and classification of toothed whale and dolphin echolocation clicks in the Gulf of Mexico. The project is funded from June 2018 to May 2021 and the toolkit will be used to analyze long-term recordings from passive acoustic instruments to study trends in marine mammal density and distribution.

**Using k-means clustering to identify spatially and temporally consistent wave systems**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> K-means clustering is used for postprocessing to identify consistent wave systems in terms of their location and time. This method has been tested and approved by NWS marine forecasters across the country and has been in operational use since February 3, 2021.

**Fast tracking the use of VIAME for automated identification of reef fish**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> Researchers are using VIAME to develop automated detection and classification models for the SEAMAP Reef Fish Video survey in the Gulf of Mexico. These models are performing well enough that they will be incorporated into the annotation process for video reads this spring, as part of a supervised quality control process.

**A Hybrid Statistical-Dynamical System for the Seamless Prediction of Daily Extremes and Subseasonal to Seasonal Climate Variability**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> This text discusses the development of a hybrid statistical-dynamical prediction system that can accurately forecast daily extremes, as well as subseasonal to seasonal temperature and precipitation patterns. The system combines a Bayesian statistical method with machine learning to improve the accuracy of seasonal forecasts from the North American Multi-Model Ensemble (NMME). The goal is to create a seamless and reliable prediction system that can represent daily extremes in line with climate conditions.

**ProbSR (probability of subfreezing roads**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> ProbSR is an algorithm that uses machine learning to predict the likelihood of roads being subfreezing, ranging from 0% to 100%. It helps determine the probability of encountering icy conditions while driving.

**Deep learning algorithms to automate right whale photo id**, _Department_of_Commerce | National Oceanic and Atmospheric Administration (NOAA)_
> Deep learning algorithms have been developed to automate the identification of right whales based on their photos. The algorithms can match the whales from different angles and body parts, and the system is currently being used on the Flukebook platform for both North Atlantic and southern right whales. A paper discussing this system is currently under review at Mammalian Biology.

**Data Science: Clutter**, _Department_of_Commerce | National Telecommunications and Information Administration (NTIA)_
> The NTIA's Institute for Telecommunication Sciences (ITS) is using AI to automatically identify and classify clutter that obstructs radio frequency propagation paths. Clutter refers to vegetation, buildings, and other structures that cause radio signal loss through dispersion, reflection, and diffraction. They have developed a convolutional neural network (CNN) that is trained using lidar data and radio frequency propagation measurements, allowing it to predict clutter classification labels when given new radio path lidar data.

**WAWENETS**, _Department_of_Commerce | National Telecommunications and Information Administration (NTIA)_
> The WAWENETS algorithm analyzes speech recordings from telecommunications systems and provides a numerical rating for speech quality or speech intelligibility. This rating is typically on a scale of 1 to 5 for speech quality and 0 to 1 for speech intelligibility.

**Enriched Citation**, _Department_of_Commerce | United States Patent and Trade Office (USPTO)_
> The Enriched Citation system is a data dissemination system that identifies the references cited in patent office actions. It provides detailed information about these references, including bibliographic information, the claims they were cited against, and the sections relied upon by examiners. The system extracts this information from unstructured office actions and offers it through a structured public API.

**AI use for CPC classification**, _Department_of_Commerce | United States Patent and Trade Office (USPTO)_
> The AI system is used to classify incoming patent applications according to the cooperative patent classification scheme. This classification helps assign work and recommend symbols for AI search. The system also processes incoming patent applications and generates classification symbols as output.

**AI retrieval for TM design coding and Image search**, _Department_of_Commerce | United States Patent and Trade Office (USPTO)_
> Clarivate has developed an AI retrieval system that helps trademark examiners identify similar trademark images, assign appropriate design codes, and evaluate the acceptability of goods and services. The system utilizes both incoming and registered trademark images to generate design codes and related images as output.

**AI retrieval for patent search**, _Department_of_Commerce | United States Patent and Trade Office (USPTO)_
> The AI retrieval for patent search is an advanced tool that aids examiners in finding relevant documents and expanding their search. It analyzes both published and unpublished applications and suggests additional areas to explore based on the user's preference for similar concepts.

**Inventor Search Assistant (iSAT)**, _Department_of_Commerce | United States Patent and Trade Office (USPTO)_
> The Inventor Search Assistant (iSAT) is a service that assists inventors in conducting novelty searches. By entering a brief description of their invention, users can receive a selection of recommended documents, figures, and classification codes to aid in their search.

## Department_of_Homeland_Security

**Use of technology to identify proof of life**, _Department_of_Homeland_Security | Customs and Border Protection_
> The CBP One app uses technology called Liveness Detection to verify the identity of users. This technology uses Artificial Intelligence and the user's mobile device camera to determine if the face presented is of the actual person and not a photo or mask. This feature is crucial for the app's functionality within the agency.

**Integrated Digital Environment**, _Department_of_Homeland_Security | Customs and Border Protection_
> The Integrated Digital Environment improves managers' understanding of end user workflows and identifies opportunities for improvement. It uses AI/ML models to analyze end user activity data and provides tailored analytics, metrics gathering, and workflow suggestions. This creates a connected and secure environment for CBP applications and allows administrators to optimize their use.

**Vessel Detection**, _Department_of_Homeland_Security | Customs and Border Protection_
> Vessel Detection utilizes integrated technologies and analytics to enhance maritime detection and the sensor network. This includes machine-assisted and AI-enhanced detection and tracking, which improves the identification of illicit vessels in areas with high levels of legitimate trade and recreational water vessel traffic. It allows for setting search criteria and transmitting them to sensors, which use AI algorithms to filter and recognize objects of interest, sharing them with other detection systems.

**Autonomous Aerostat**, _Department_of_Homeland_Security | Customs and Border Protection_
> The Autonomous Aerostat is an advanced system that uses three tethers instead of one, along with weather sensors and analytic capabilities. It employs AI and robotics to launch and recover the aerostat during bad weather without the need for on-site staff, making it able to operate autonomously and saving time and manpower.

**AI for Autonomous Situational Awareness**, _Department_of_Homeland_Security | Customs and Border Protection_
> The AI for autonomous situational awareness system aims to detect and monitor illegal cross-border traffic in remote areas using IoT sensor kits. It will utilize motion image/video technology enhanced with Artificial Intelligence to detect and determine the direction of vehicles. The system will also incorporate a motion sensor and high-resolution camera to capture images, which will be processed by AI models to classify objects and provide sufficient imagery for re-identification. The goal is to create an efficient, affordable, and covert system for situational awareness and detection.

**Automated Item of Interest Detection - ICAD**, _Department_of_Homeland_Security | Customs and Border Protection_
> The ICAD system is a software used by USBP agents to analyze photographs taken by field imaging equipment. It uses the Matroid software to determine if any of the images contain human subjects. The goal is to expand the system's models to include vehicles and subjects with long-arm rifles, while excluding animals or items of little interest.

**Autonomous Maritime Awareness**, _Department_of_Homeland_Security | Customs and Border Protection_
> The Autonomous Maritime Awareness system is a combination of surveillance towers, ocean data solutions, unmanned autonomous surface vehicles (ASV), and AI. The system can autonomously detect, identify, and track items of interest in a maritime environment using radars, cameras, and sensor suites. It can send alerts to monitoring agencies for potential interdictions or intel collections.

**AI Curated Synthetic Data**, _Department_of_Homeland_Security | Customs and Border Protection_
> AI Curated Synthetic Data is a technology that generates realistic synthetic data for computer vision. It produces visually accurate X-ray scan images and virtual 3D assets of vehicles and narcotics containers. This data is used to improve the development of Anomaly Detection Algorithms for Non-Intrusive Inspection, aiding in the detection of contraband in conveyances and cargo using AI/ML technology.

**Data and Entity Resolution**, _Department_of_Homeland_Security | Customs and Border Protection_
> Data and Entity Resolution is a system that automatically merges and identifies connections between different data sources. It uses Machine Learning to develop models that can associate records and identify commonalities. Non-technical users can train these models through a user-friendly interface.

**RVSS Legacy Overhauled System Project (INVNT)**, _Department_of_Homeland_Security | Customs and Border Protection_
> The RVSS Legacy Overhauled System Project (INVNT) introduces the Video Computer Aided Detection (VCAD) software, also known as Matroid AI. This software allows CBP end users to create and share vision detectors, which are trained computer vision models that can recognize objects, people, and events in images or videos. Users can monitor streaming video in real time or search through pre-recorded data to identify objects, people, and events of interest, and access detection information through reports, alerts, and an API for integration with CBP applications.

**Geospatial imagery utilizing annotation**, _Department_of_Homeland_Security | Customs and Border Protection_
> Geospatial imagery with annotation uses SAR satellites to capture images of any location on Earth, regardless of cloud cover or time of day. It utilizes AI technologies like machine vision and object detection to identify airframes, military vehicles, marine vessels, and can also detect changes for disaster response missions.

**Autonomous Surveillance Towers (Anduril)**, _Department_of_Homeland_Security | Customs and Border Protection_
> The Autonomous Surveillance Towers by Anduril are capable of autonomously detecting, identifying, and tracking items of interest using AI. With a hybrid command and control capability, the system can be accessed remotely via URL on various devices. It is solar powered, easily deployable, and can provide visibility for up to 1.5 miles for people and 3 miles for vehicles. The tower constantly scans, detects movement, slews the camera to the object, identifies it, and alerts the user while autonomously tracking the object.

**Entity Resolution**, _Department_of_Homeland_Security | Customs and Border Protection_
> Entity Resolution is a software system that uses global trade data to enhance agency investigations by analyzing trade flows and risks associated with cross-border trade. It utilizes AI/ML models to manage and structure data, perform entity resolution and network analysis, and contribute to the software's knowledge graph and user interface.

**Cyber Incident Reporting**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> Cyber incident handling specialists use automation tools powered by Machine Learning and Natural Language Processing to process data from threat intelligence and cyber incident channels. These tools improve the accuracy and relevance of filtered data presented to human analysts and decision-makers, as well as help aggregate information in reports for analysis. The data received includes information from covered CIRCIA entities.

**Security Information and Event Management (SIEM) Alerting Models**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> Threat hunting and SOC analysts receive a large amount of log data daily. Although they use manual and automatic detection alerts, they are not enough to detect all cyber attacks. Analysts use automated tooling to refine and produce additional alerts based on aggregated information and expertise, allowing them to detect anomalies accurately and quickly.

**AI Security and Robustness**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> AI Security and Robustness involves the creation of frameworks, processes, and testing tools to govern the acquisition, development, deployment, and maintenance of AI technologies. These tools, which utilize Machine Learning and Natural Language Processing, are used by technology integrators in CISA and other federal entities to ensure the reliable, strong, and secure functioning of their AI systems by expediting data processing.

**Automated Indicator Sharing (AIS) Automated PII Detection**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The Automated PII Detection and Human Review Process uses analytics and natural language processing to automatically detect potential personally identifiable information (PII) in submissions to Automated Indicator Sharing (AIS). If potential PII is detected, the submission is reviewed by analysts who can confirm or deny the identification of PII and redact the information if necessary. Privacy experts oversee the process and provide feedback for improvement, ensuring compliance with privacy requirements and maintaining the integrity and trust in the system.

**Cyber Threat Intelligence Feed Correlation**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> Cyber Threat Intelligence Feed Correlation is a system that utilizes AI technology to quickly analyze and correlate information from various sources. This helps to enhance the quality of shared information and allows the algorithm to learn and improve its performance. It also has the potential to create customized algorithms for continuous monitoring of threat actor tactics, techniques, and procedures (TTPs).

**Cyber Vulnerability Reporting**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> Advanced automation tools are needed by vulnerability analysts to process data from multiple reporting channels and share it automatically. These tools utilize Machine Learning and Natural Language Processing to improve the accuracy and relevance of filtered data for human analysts and decision-makers. Machine Learning techniques are also used to aggregate information from databases like KEV and CVE for further analysis and presentation.

**Critical Infrastructure Anomaly Alerting**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The Cyber Sentry program monitors critical infrastructure networks and employs threat hunting analysts who need advanced anomaly detection and machine learning capabilities to analyze cyber-physical data on IT and OT networks, including ICS/SCADA. The Critical Infrastructure Anomaly Alerting model offers AI-assistance to process this information.

**Malware Reverse Engineering**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> Malware reverse engineering is crucial for CISA's cyber defense mission. Threat Focused Reverse Engineering (TFRE) uses advanced engineering, formal methods, and deep learning techniques to improve cyber threat intelligence. Automated techniques are necessary to disrupt adversaries' malware development and target them more effectively, allowing analysts to focus on hunting and taking down threats. TFRE includes tool hardening, enhanced computational abilities, and understanding of deployment environments to provide important capabilities in the development pipeline.

**Advanced Analytic Enabled Forensic Investigation**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> CISA uses forensic specialists to investigate cyber events in various government and critical infrastructure organizations. These specialists utilize advanced analytic tools, including Artificial Intelligence, to analyze data and identify anomalies and potential threats efficiently. This automated process ensures timely detection of high-quality anomalies using mathematical and probabilistic models.

**Advanced Network Anomaly Alerting**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> Advanced Network Anomaly Alerting enables SOC analysts to receive large amounts of data from NCPS's Einstein sensors. By using automated tooling and mathematically-based models, analysts can refine and produce accurate alerts based on aggregated information, ensuring timely detection of high fidelity anomalies.

**Operational Activities Explorer**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The Operations Center at CISA utilizes a dashboard powered by artificial intelligence to make sense of ongoing operational activities. This AI system leverages real-time event data, historical cybersecurity information, and previous operational responses to suggest appropriate actions and engagement strategies with government entities and critical infrastructure owners and operators, considering the potential impact on National Critical Functions.

**AIS Scoring and Feedback**, _Department_of_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> AIS Automated Scoring & Feedback (AS&F) uses descriptive analytics to classify indicators of compromise (IOCs) based on organizational-centric intelligence. It checks if the indicator is present in a known-good list, if there are sightings of the indicator, if it has been verified by an analyst, and if there are other reports about the indicator. Participants can use the opinion and confidence values to filter and prioritize the data, and the confidence score and opinion value to compare with other organizations. This helps with actioning and investigating Indicator objects.

**Text Analytics for Survey Responses (TASR)**, _Department_of_Homeland_Security | HQ_
> Text Analytics for Survey Responses (TASR) is a tool used by DHS OCHCO to analyze and extract important topics from survey responses. It helps DHS Leadership understand employees' needs and improve job satisfaction by providing insights from open-ended questions in the quarterly DHS Pulse Surveys.

**RelativityOne**, _Department_of_Homeland_Security | HQ, Customs and Border Protection, Cybersecurity and Infrastructure Security Agency, Countering Weapons of Mass Destruction, Immigration and Customs Enforcement, Intelligence and Analysis, Science and Technology_
> RelativityOne is a document review platform designed for efficient and large-scale document review and production in arenas like litigation and FOIA. It helps organizations streamline their review processes and improve productivity in handling vast amounts of documents.

**Normalization Services**, _Department_of_Homeland_Security | Immigration and Customs Enforcement_
> HSI uses Artificial Intelligence to normalize and validate addresses, phone numbers, names, and ID numbers. This helps to correct data entry errors, identify purposeful misidentification, connect information across datasets, and reduce the time and resources needed for investigations. These services are provided through RAVEn, a project that supports ICE's mission to enforce and investigate violations of U.S. laws and analyze trends and criminal patterns.

**Email Analytics**, _Department_of_Homeland_Security | Immigration and Customs Enforcement_
> The Email Analytics application allows users to analyze and review email data obtained legally. It utilizes AI to classify spam messages and extract entities such as names, organizations, and locations. Additionally, it integrates machine translation capabilities with a commercial product.

**Barcode Scanner**, _Department_of_Homeland_Security | Immigration and Customs Enforcement_
> The Barcode Scanner is a tool developed for the RAVEn GO's Encounter Card that can scan and input information from MRZ and PDF417 barcodes found on travel documents and US Driver's Licenses. This project is a part of the DHS HSI Innovation Lab and RAVEn, which supports ICE's mission to enforce and investigate violations of US laws and provides tools for analyzing trends and identifying criminal patterns. For more information, refer to the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for RAVEn.

**Mobile Device Analytics**, _Department_of_Homeland_Security | Immigration and Customs Enforcement_
> Mobile Device Analytics (MDA) has been developed to help investigators analyze large amounts of data extracted from mobile devices following court orders. The goal of MDA is to improve the efficiency of agents and analysts in identifying relevant evidence, relationships, and criminal networks. The project involves developing machine learning for object detection in photos and videos, and is part of the DHS HSI Innovation Lab / RAVEn project, which supports ICE's mission to enforce and investigate violations of U.S. laws.

**Machine Translation (Previously Language Translator)**, _Department_of_Homeland_Security | Immigration and Customs Enforcement_
> Systran offers machine translation for more than 100 different language combinations, including Chinese, Spanish, Arabic, Farsi, Russian, German, Ukrainian, and Filipino to English. It can translate plain text, word documents, and PDFs, and provides a web-based user interface and API endpoint.

**Facial Recognition Service**, _Department_of_Homeland_Security | Immigration and Customs Enforcement_
> The Facial Recognition Service, developed by the DHS HSI Innovation Lab, is utilized by HSI agents and analysts to identify known individuals and extract faces from perpetrators involved in child exploitation offenses, human rights atrocities, and war crimes. This service is supported by RAVEn, a virtualized environment that enables large analytical projects and helps ICE enforce and investigate violations of U.S. laws. For more details, refer to the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for RAVEn.

**Predicted to Naturalize**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> The Predicted to Naturalize model is designed to predict the eligibility of Legal Permanent Residents for naturalization and provides their current address. It can be utilized to send important information to USCIS customers regarding their resident status and inform others about potential USCIS benefits.

**Identity Match Option (IMO) Process with DBIS Data Marts**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> The Identity Match Option (IMO) process is used by USCIS to create a single identity for each applicant or beneficiary across multiple systems. This process uses standard entity matching algorithms to combine person-centric data attributes from various systems, allowing analysts to view an individual's immigration history, detect fraud, and resolve data quality issues.

**Person-Centric Identity Services A-Number Management Model**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> The Person-Centric Identity Services (PCIS) aims to be the trusted source of comprehensive immigration history and status information, using biographical and biometric data. The A-Number Management model is responsible for training and evaluating the PCIS system by ingesting various datasets and using machine learning to ensure accurate search results and high confidence in entity resolution. This model is critical in surfacing and resolving a person's records, even when dealing with fuzzy matches and varying data quality.

**I-539 approval prediction**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> This project aims to develop a machine learning model that can accurately predict the approval timeline for I-539 cases submitted through eProcessing. The model has the potential to enhance the approval process of "Application to Extend or Change Nonimmigrant Status" by providing valuable insights and predictions.

**Person-Centric Identity Services Deduplication Model**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> The Person-Centric Identity Services (PCIS) aims to be the trusted source of biographical and biometric information, allowing real-time access to an individual's immigration history and status. The Deduplication model is crucial in identifying and combining records from various systems, using machine learning to accurately match and consolidate data for a comprehensive view of a person's immigration history.

**Sentiment Analysis - Surveys**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> The Sentiment Analysis - Surveys system analyzes survey results and assigns sentiments to different categories using Natural Language Processing modeling software. This helps survey administrators extract valuable information from employee satisfaction surveys by combining quantitative and qualitative data. This capability is currently accessible on demand.

**I-485 Family Matching**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> The I-485 Family Matching system uses AI to create connections between family members and their underlying I-485 petitions, which determine if the petition is employment-based or family-based. This system helps predict visa usage by comparing the petition's visa classification and priority date with the Department of State's Visa Bulletin. It also aids in identifying and grouping I-485s filed by family members, even when the only available matching field is a common address. Overall, the goal is to improve the reliability and efficiency of processing this data.

**Topic Modeling on Request For Evidence data sets**, _Department_of_Homeland_Security | United States Citizenship and Immigration Services_
> Topic modeling on Request For Evidence data sets involves creating models that can identify and group documents based on related topics. This technique is useful for organizing, understanding, searching, and summarizing text data by discovering hidden themes and classifying documents accordingly.

## Department_of_Veterans_Affairs

**VA-DoE Suicide Exemplar Project**, _Department_of_Veterans_Affairs_
> The VA-DoE Suicide Exemplar Project is using artificial intelligence to enhance the VA's capability to identify veterans who may be at risk of suicide. This project includes three interconnected initiatives that involve partnerships with the Department of Energy.

**Behavidence**, _Department_of_Veterans_Affairs_
> Behavidence is an app that helps veterans track their mental health. By comparing their phone usage to a digital phenotype of individuals with confirmed mental health diagnoses, the app provides valuable insights and support.

**Interpretation/triage of eye images**, _Department_of_Veterans_Affairs_
> Artificial intelligence is used to triage and interpret eye images, helping healthcare providers assess health risks and improve diagnosis for various eye conditions such as glaucoma, macular degeneration, and diabetic retinopathy, particularly in telehealth settings.

**Reinforcement learning evaluation of treatment policies for patients with hepatitis C virus**, _Department_of_Veterans_Affairs_
> A machine learning model is utilized to forecast disease progression in veterans with hepatitis C virus. This reinforcement learning evaluation aims to assess treatment policies for these patients.

**Predicting hepatocellular carcinoma in patients with hepatitis C**, _Department_of_Veterans_Affairs_
> This study examined whether deep learning recurrent neural network (RNN) models, using raw longitudinal data from electronic health records, outperformed conventional regression models in predicting the risk of developing hepatocellular carcinoma (HCC) in patients with hepatitis C virus (HCV)-related cirrhosis. The study used data from patients in the national Veterans Health Administration with at least 3 years of follow-up after their cirrhosis diagnosis.

**Acute kidney injury (AKI)**, _Department_of_Veterans_Affairs_
> This project, in partnership with Google DeepMind, aims to identify acute kidney injury (AKI) which can range from mild to severe kidney function loss and even kidney failure. The artificial intelligence system developed can also identify AKI caused by other illnesses.

**Nediser reports QA**, _Department_of_Veterans_Affairs_
> Nediser is an AI system designed to help radiologists analyze X-ray properties accurately. It can identify normal templates, detect hardware, assess patella alignment and leg length and angle discrepancy, as well as measure Cobb angles.

**Machine learning (ML) for enhanced diagnostic error detection and ML classification of protein electrophoresis text**, _Department_of_Veterans_Affairs_
> Researchers are using machine learning to improve the detection of diagnostic errors and classify protein electrophoresis text. They are collecting true/false positive annotations through chart review and creating a vector embedding of patient records. This embedding will be used to retrieve unlabeled records similar to the labeled ones, and machine learning will be used as a filter to enhance specificity after rules-based retrieval. The selected inputs for embedding will include structured data related to stroke risk and potentially previous text notes.

**Artificial intelligence coach in cardiac surgery**, _Department_of_Veterans_Affairs_
> The artificial intelligence coach in cardiac surgery is able to detect when team members have different understandings or approaches to complex healthcare tasks. This is particularly important in safety-critical fields like aviation and healthcare, where differing mental models can result in errors and harm. By identifying these differences, the coach can assist in improving teamwork and enhancing human cognitive abilities during surgeries.

**Medication Safety (MedSafe) Clinical Decision Support (CDS)**, _Department_of_Veterans_Affairs_
> The MedSafe Clinical Decision Support system analyzes clinical data to provide evidence-based recommendations for primary care providers managing diabetes, hypertension, and chronic kidney disease. It examines multiple factors such as comorbidities, lab results, medications, and adverse drug events to generate patient-specific recommendations.

**Digital command center**, _Department_of_Veterans_Affairs_
> The Digital Command Center is a system that aims to gather and analyze all data within a medical center. By utilizing predictive and prescriptive analytics, hospital leaders can make more informed decisions to improve the overall performance of the hospital.

**Assessing lung function in health and disease**, _Department_of_Veterans_Affairs_
> Artificial intelligence can assist health professionals in evaluating lung function and sleep parameters, enabling them to identify factors that indicate normal or abnormal lung function. This technology aids in assessing respiratory health and can provide valuable insights for diagnosing and monitoring lung diseases.

**Computer-aided detection and classification of colorectal polyps**, _Department_of_Veterans_Affairs_
> This study aims to improve the clinical management of colorectal polyps by using artificial intelligence models. These models analyze video frames from colonoscopy videos in real time to detect the presence of polyps and predict their potential malignancy.

**Artificial Intelligence physical therapy app**, _Department_of_Veterans_Affairs_
> The Artificial Intelligence physical therapy app is a versatile tool that supports physical therapy. It collects data from wearable sensors and uses AI to analyze the information, providing explainable feedback to the therapist.

**VA /IRB approved research study for finding colon polyps**, _Department_of_Veterans_Affairs_
> This text describes an approved research study that aims to find colon polyps using artificial intelligence. The study has been approved by the VA/IRB and will employ a randomized trial methodology.

**Use of machine learning to predict surgery in Crohn’s disease**, _Department_of_Veterans_Affairs_
> Machine learning was utilized to analyze patient data from 2001 to 2015 in order to predict the occurrence of surgery in individuals with Crohn's disease. The study focused on patient demographics, medication usage, and longitudinal laboratory values to create a model that could forecast surgical outcomes within a year.

**Using machine learning to predict perfusionists’ critical decision-making during cardiac surgery**, _Department_of_Veterans_Affairs_
> Machine learning is utilized to create predictive models of perfusionists' decision-making during critical cardiac surgery situations. The findings have the potential to enhance patient safety and surgical outcomes by guiding the development of computerized clinical decision support tools for use in the operating room.

**Machine learning models to predict disease progression among veterans with hepatitis C virus**, _Department_of_Veterans_Affairs_
> Researchers have developed a machine learning model that accurately predicts the progression of hepatitis C virus in veterans. This model can aid in early detection and intervention for better patient outcomes.

**AI Cure**, _Department_of_Veterans_Affairs_
> AICURE is a phone app that tracks how well patients follow their prescribed medications in clinical or pharmaceutical sponsor drug studies. It uses artificial intelligence to monitor and ensure adherence to oral medications.

**Automatic speech transcription engines to aid scoring neuropsychological tests.**, _Department_of_Veterans_Affairs_
> Automatic speech transcription engines are used to analyze the cognitive decline of older VA patients by transcribing their digitally recorded speech responses using AI-based speech-to-text engines. This eliminates the need for manual transcription of patient speech in order to score the neuropsychological tests.

**Provider directory data accuracy and system of record alignment**, _Department_of_Veterans_Affairs_
> AI is utilized in provider directories to enhance accuracy and align with the system of record. It serves as a transactor for intelligent identity resolution and linking, with additional functions such as domain cache for Clinical Decision Support and state reconstruction, as well as discrepancy detection. AI acts as a synchronizer, performing intelligent propagation and semi-automated discrepancy resolution, while adapters enable inference through OWL and logic programming. Lastly, AI offers long-term storage for extensive machine learning and business intelligence applications.

**Predicting hospitalization and corticosteroid use as a surrogate for IBD flares**, _Department_of_Veterans_Affairs_
> This study analyzed data from over 20,000 Veterans Health Administration patients diagnosed with irritable bowel disease (IBD) to predict hospitalizations and the use of corticosteroids as a proxy for IBD flares. The researchers used longitudinal lab data and various predictors to develop random forest models for making these predictions.

**Machine learning tools to predict outcomes of hospitalized VA patients**, _Department_of_Veterans_Affairs_
> A study has been approved to use machine learning tools to predict health outcomes of VA patients. The study will specifically focus on predicting Alzheimer's disease, rehospitalization, and Chlostridioides difficile infection.

**Prediction of Veterans' Suicidal Ideation following Transition from Military Service**, _Department_of_Veterans_Affairs_
> Machine learning is being used to determine factors that can predict veterans' thoughts of suicide after they transition from military service. The data used for this analysis is gathered from surveys conducted online, where veterans share their experiences within three months of leaving the military and continue to do so every six months for the first three years after their service.

**Disentangling dementia patterns using artificial intelligence on brain imaging and electrophysiological data**, _Department_of_Veterans_Affairs_
> A deep learning framework is being developed to predict dementia patterns observed on MRI and EEG, with the aim of using these imaging modalities as biomarkers for different forms of dementia and epilepsy disorders. The VA is conducting a retrospective chart review as part of this collaborative effort.

**Predicting corticosteroid free endoscopic remission with Vedolizumab in ulcerative colitis**, _Department_of_Veterans_Affairs_
> The researchers used a random forest model to predict whether patients with ulcerative colitis would achieve corticosteroid-free remission after 52 weeks of Vedolizumab treatment. They created models using either baseline data or data up until week 6 of therapy.

**Prediction of biologic response to thiopurines**, _Department_of_Veterans_Affairs_
> Artificial intelligence is utilized to predict the biologic response to thiopurines in Veterans with irritable bowel disease by analyzing data from CPRS and CDW. This allows for a more accurate prediction of how individuals will respond to this treatment.

**Seizure detection from EEG and video**, _Department_of_Veterans_Affairs_
> Machine learning algorithms analyze EEG and video data to detect seizures in a VHA epilepsy monitoring unit. This automated process eliminates the need for human intervention in identifying seizures.

**Gait signatures in patients with peripheral artery disease**, _Department_of_Veterans_Affairs_
> Machine learning is being utilized to enhance the treatment of functional issues in patients with peripheral artery disease. By analyzing previously collected biomechanics data, researchers are able to identify characteristic gait signatures of PAD and assess the effectiveness of limb acceleration measurements in modeling important biomechanical measures from this data.

**Automated eye movement analysis and diagnostic prediction of neurological disease**, _Department_of_Veterans_Affairs_
> Artificial intelligence is used to analyze eye movements and predict neurological diseases by continuously improving automated algorithms and identifying markers of conditions like traumatic brain injury, Parkinson's, and stroke. This automated analysis enhances the quality and accuracy of diagnostic predictions.

**Precision medicine PTSD and suicidality diagnostic and predictive tool**, _Department_of_Veterans_Affairs_
> The Precision Medicine PTSD and Suicidality Diagnostic and Predictive Tool uses real-time data to analyze and predict episodes of PTSD and suicidality. It helps diagnose these conditions early and accurately, while also providing insights into the effects of stress on the development of PTSD, particularly in extreme situations.

**SoKat Suicidial Ideation Detection Engine**, _Department_of_Veterans_Affairs_
> The SoKat Suicide Ideation Engine (SSIE) utilizes natural language processing (NLP) to enhance the identification of suicide ideation among veterans. It analyzes survey data gathered by the Office of Mental Health (OMH) Veteran Crisis Line (VCL) support team called VSignals.

**Prediction of health outcomes, including suicide death, opioid overdose, and decompensated outcomes of chronic diseases.**, _Department_of_Veterans_Affairs_
> This tool utilizes electronic health records (EHR) to generate deep phenotypes and predict various health outcomes such as suicide death, opioid overdose, and worsening conditions of chronic diseases. It takes both structured and unstructured data from EHR as inputs.

**Extraction of family medical history from patient records**, _Department_of_Veterans_Affairs_
> A pilot project is being conducted to extract family medical history data and identify African American Veterans aged 45-50 who are at risk of prostate cancer but have not undergone screening. This is done by analyzing TIU documentation from patient records.

**CuraPatient**, _Department_of_Veterans_Affairs_
> CuraPatient is an AI-powered remote tool that enables patients to effectively manage their health conditions without the need for in-person medical appointments. It offers features like health tracking, program enrollment, insurance management, and appointment scheduling through a personalized patient profile.

**Screening for esophageal adenocarcinoma**, _Department_of_Veterans_Affairs_
> National VHA administrative data is utilized to modify tools that utilize electronic health records in order to forecast the likelihood of esophageal adenocarcinoma.

**GI Genius (Medtronic)**, _Department_of_Veterans_Affairs_
> The GI Genius, developed by Medtronic, is an artificial intelligence tool that helps in identifying colon polyps. It uses advanced technology to enhance the detection process and improve accuracy.

**PredictMod**, _Department_of_Veterans_Affairs_
> PredictMod is an artificial intelligence system that analyzes the gut microbiome to determine if predictions about diabetes can be made. It utilizes advanced technology to evaluate the relationship between the gut microbiome and diabetes, potentially providing valuable insights and predictions in this area.

**Predictor profiles of OUD and overdose**, _Department_of_Veterans_Affairs_
> Machine learning prediction models are used to analyze both known and new risk factors for opioid use disorder (OUD) and overdose among Post-9/11 Veterans. These models help create predictor profiles for OUD and overdose through various classification-tree modeling approaches.

**Social determinants of health extractor**, _Department_of_Veterans_Affairs_
> AI technology is utilized to extract social determinants of health (SDOH) information from clinical notes. This extracted data can be utilized in health-related analysis to assess if SDOH factors like disease risks or healthcare inequality contribute to health outcomes.

## Department_of_Agriculture

**Predictive modeling of invasive pest species and category at the port of entry using machine learning algorithms**, _Department_of_Agriculture | USDA | APHIS_
> Machine learning algorithms are utilized to create predictive models that enhance the accuracy of detecting invasive or quarantine significant pest species at the port of entry. These models are developed by analyzing inspection data and are aimed at improving the prediction ability of identifying such pests.

**Detection of pre-symptomatic HLB infected citrus**, _Department_of_Agriculture | USDA | APHIS_
> The text states that the goal is to detect citrus trees infected with HLB before symptoms appear. This is done by identifying specific pixels in multispectral and thermal imagery that indicate HLB infection.

**Detection of aquatic weeds**, _Department_of_Agriculture | USDA | APHIS_
> The process of detecting aquatic weeds involves identifying and locating these plants in water bodies. This is done in order to effectively manage and control their growth, as weeds can have negative impacts on ecosystems and water quality.

**High throughput phenotyping in citrus orchards**, _Department_of_Agriculture | USDA | APHIS_
> High throughput phenotyping in citrus orchards involves efficiently identifying, tallying, and classifying citrus trees to effectively track the overall health of the orchard. This method enables quick and accurate assessment of the orchard's condition.

**Standardization of cut flower business names for message set data**, _Department_of_Agriculture | USDA | APHIS_
> This text discusses the process of standardizing cut flower business names for message set data using natural language processing. The data is cleaned by removing punctuation, allowing for easier matching. Similar terms are identified using cosine similarity and the results are then output.

**Artificial Intelligence for correlative statistical analysis**, _Department_of_Agriculture | USDA | APHIS_
> Artificial Intelligence is employed to perform correlative statistical analysis by employing various modeling techniques such as random forest, artificial neural networks, k-nearest neighbor clustering, and support vector machines. These techniques enable the prediction of relationships between variables.

**Automated Detection & Mapping of Host Plants from Ground Level Imagery**, _Department_of_Agriculture | USDA | APHIS_
> This text explains the process of automatically detecting and mapping host plants using ground-level imagery. The aim is to create maps specifically showing the location of target trees using streetview images.

**Training machine learning models to automatically read file attachments and save information into a more convenient Excel format.**, _Department_of_Agriculture | USDA | APHIS_
> Artificial intelligence is employed to automate the process of document processing and information extraction. This technology helps program managers extract required information from specific fields in PDF email attachments, saving time by eliminating the need for manual opening and copying of information from each attachment.

**Approximate string or fuzzy matching, used to automate matching similar, but not identical, text in administrative documents**, _Department_of_Agriculture | USDA | APHIS_
> Approximate string or fuzzy matching is a technique used to automatically match similar but not exact text in administrative documents. By calculating a similarity metric, this algorithm can classify similar strings into one category, reducing the need for duplicate information and manual error-checking.

**NAL Automated indexing**, _Department_of_Agriculture | USDA | ARS_
> The NAL Automated indexing system, Cogito, utilizes artificial intelligence to automatically add subject indexing to around 500,000 peer-reviewed journal articles every year. This software uses the National Ag Library Thesaurus concept space (NALT) to annotate the articles, with only NALT concepts being used as metadata in the Library's bibliographic citation databases such as AGRICOLA, PubAg, and Ag Data Commons.

**4% Repair Dashboard**, _Department_of_Agriculture | USDA | ARS_
> The 4% Repair Dashboard is a model that analyzes expense descriptions to determine whether they are related to repairs and maintenance. It uses keywords in the context to classify expenses as either "repair" or "not repair".

**ARS Project Mapping**, _Department_of_Agriculture | USDA | ARS_
> The ARS Project Mapping allows national program leaders to analyze and cluster research project plans using NLP. This helps them identify patterns and synergies in the ARS research program portfolios through an interactive dashboard.

**Democratizing Data**, _Department_of_Agriculture | USDA | ERS_
> The project aims to make data and evidence accessible to all by utilizing AI tools, machine learning, and natural language processing. It seeks to understand how publicly-funded data and evidence contribute to scientific advancements and societal benefits.

**Westat**, _Department_of_Agriculture | USDA | ERS_
> Westat held a competition to discover automated methods for connecting USDA nutrition information to a large dataset of 750,000 food items. Teams competing in the challenge utilized various AI techniques like Natural Language Processing, random forest, and semantic matching.

**Retailer Receipt Analysis**, _Department_of_Agriculture | USDA | FNS_
> The Retailer Receipt Analysis is a Proof of Concept (POC) that uses OCR on a sample of FNS receipt and invoice data to automate the manual process, save time, ensure accuracy, and detect difficult patterns. The goal is to develop a review system with an automated workflow that learns from analyst feedback, can identify known SNAP fraud patterns, detect new patterns, and visually highlight alerts on retailer invoices and receipts.

**Nutrition Education & Local Access Dashboard**, _Department_of_Agriculture | USDA | FNS_
> The Nutrition Education & Local Access Dashboard aims to visually present data on FNS nutrition support at the county level, including nutrition education and local food access, as well as other metrics related to hunger and nutritional health. The dashboard includes a K-means clustering script that groups states into seven different clustering options, such as Farm to School Intensity & Size and Fresh Food Access, allowing users to identify states with similar characteristics for potential partnerships that they may not have previously considered.

**Land Change Analysis Tool (LCAT)**, _Department_of_Agriculture | USDA | FPAC_
> The Land Change Analysis Tool (LCAT) uses a random forest machine learning classifier to create detailed land cover maps using aerial and/or satellite imagery. They collect training data through a web application and process it using a 192-node docker cluster. The results are published through an Image service, and they have already mapped over 600 million acres and generated over 700 thousand training samples.

**OCIO/CDO Council Comment Analysis Tool**, _Department_of_Agriculture | USDA | Federal CDO Council_
> The Comment Analysis Tool, developed by the OCIO/CDO Council, utilizes Natural Language Processing (NLP) to assist in regulatory comment analysis. This toolset helps reviewers identify comment topics and themes, as well as group similar comments together. By providing novel insights and streamlining the comment processing process, these tools offer significant value by reducing duplicate efforts and ultimately saving costs for government agencies and the USG.

**Wildland Urban Interface - Mapping Wildfire Loss**, _Department_of_Agriculture | USDA | Forest Service_
> This study aims to assess the potential of machine learning and object-based image classification methods in identifying buildings, building loss, and defensible space in areas at risk of wildfires. The study focuses on pre- and post-wildfire events in wildland-urban interface settings to determine the effectiveness of these techniques in mapping wildfire damage.

**Landscape Change Monitoring System (LCMS)**, _Department_of_Agriculture | USDA | Forest Service_
> The Landscape Change Monitoring System (LCMS) is a data system created by the USDA Forest Service that uses remote sensing to monitor changes in vegetation canopy cover and land cover and use. It employs a supervised classification process with temporal change classifications and training data to identify vegetation gain, loss, and changes in land cover and use.

**TreeMap 2016**, _Department_of_Agriculture | USDA | Forest Service_
> TreeMap 2016 is a model that represents the forests of the conterminous United States by matching forest plot data to a 30x30 meter grid. It is used in various sectors for projects such as fuel treatment planning and estimating carbon resources. The model was created using a random forests machine-learning algorithm, which imputed the forest plot data to target rasters provided by LANDFIRE, using predictor variables such as forest cover, height, vegetation type, topography, location, biophysical variables, and disturbance history.

**CLT Knowledge Database**, _Department_of_Agriculture | USDA | Forest Service_
> The CLT knowledge database is an information system that uses data aggregator bots and intelligent software to collect and update cross-laminated timber (CLT) information. It has cataloged over 3,600 publications and helps stakeholders in the mass timber industry by disseminating knowledge, facilitating collaboration, and reducing duplication of efforts. This tool supports the use of mass timber, which benefits forest health by increasing the economic value of forests.

**RMRS Raster Utility**, _Department_of_Agriculture | USDA | Forest Service_
> The RMRS Raster Utility is a library designed for data acquisition, raster sampling, and statistical and spatial modeling. It is built on the .NET framework and offers machine learning capabilities to improve efficiency and reduce storage space in raster analysis processes.

**Forest Health Detection Monitoring**, _Department_of_Agriculture | USDA | Forest Service_
> Machine learning models are utilized to scale up training data by using various types of imagery, such as Sentinel-2, Landsat, MODIS, and lidar, collected from both field and high-resolution sources. These models are used to map and monitor forest mortality and defoliation stages across the United States and to convert raster outputs into vector polygons.

**Ecosystem Management Decision Support System (EMDS)**, _Department_of_Agriculture | USDA | Forest Service_
> The Ecosystem Management Decision Support System (EMDS) is a tool for landscape analysis and planning. It is integrated with ArcGIS and QGIS and allows users to create applications tailored to their specific needs. These applications can utilize four AI engines for various purposes, such as logic processing, multi-criteria decision analysis, Bayesian networks, and Prolog-based decision trees.

**Geospatial and Remote Sensing Training Courses**, _Department_of_Agriculture | USDA | Forest Service_
> There are various geospatial and remote sensing training courses available that focus on teaching software and scripting techniques for machine learning. These courses cover a range of topics such as change detection, using software packages like eCognition, geospatial scripting for Google Earth Engine, and utilizing Collect Earth Online.

**List Frame Deadwood Identification**, _Department_of_Agriculture | USDA | NASS_
> The deadwood identification process uses a model that combines various data inputs to predict the likelihood of a farm operation going out of business. By analyzing common tree splits identified by the model and incorporating expert knowledge, a recurring process for removing deadwood is developed.

**Census of Agricuilture Response Propensity Scores**, _Department_of_Agriculture | USDA | NASS_
> Response propensity scores for the Census of Agriculture (COA) are calculated using random forest models that incorporate historical data, control data, and other survey data. These scores aid in optimizing data collection efforts by identifying areas or individuals with higher likelihoods of responding to the COA.

**Cropland Data Layer**, _Department_of_Agriculture | USDA | NASS_
> The Cropland Data Layer (CDL) uses a machine learning algorithm to analyze satellite data and determine the type of crop or activity in each 30 square meter pixel on the ground. The algorithm is trained using USDA's Farm Services Agency data and other sources to ensure accuracy. The CDL has been produced since 2008 and has proven to be highly accurate for commodities like corn and soybeans.

**Climate Change Classification NLP**, _Department_of_Agriculture | USDA | NIFA_
> A model is used to classify NIFA funded projects as either climate change related or not using natural language processing techniques. The model takes into account various text fields like project title, non-technical summary, objectives, and keywords. The output is a classification of projects as climate change related or not.

**Digital Imagery (no-change) for NRI program**, _Department_of_Agriculture | USDA | NRCS_
> The NRI program utilizes neural networks and AI technologies to identify no-changes in digital imagery. This allows for efficient monitoring and assessment of national resources.

**Operational water supply forecasting for western US rivers**, _Department_of_Agriculture | USDA | NRCS_
> The USDA National Water and Climate Center has developed a next-generation prototype for generating operational water supply forecasts using a multi-model machine-learning system. This system has improved accuracy in predicting river flow volumes and has been tested in various locations across the western US and Alaska. The use of AI and other data-science technologies has allowed for automated training and operation, and the results are explainable in terms of known hydroclimatic processes, providing relatable forecast storylines for customers.

**Conservation Effects Assessment Project**, _Department_of_Agriculture | USDA | NRCS_
> The Conservation Effects Assessment Project aims to forecast the benefits of conservation at the individual field level. It achieves this by utilizing farmer survey data, APEX modeling outcomes, and environmental data in its model.

**Ecological Site Descriptions (machine learning)**, _Department_of_Agriculture | USDA | NRCS_
> The text describes the use of machine learning to analyze a large amount of data, including over 20 million records of soils data and 20,000 text documents on ecological state and transition information. This analysis aims to provide ecological site descriptions based on the collected information.

**Artificial Intelligence SPAM Mitigation Project**, _Department_of_Agriculture | USDA | OASCR_
> The Artificial Intelligence SPAM Mitigation Project aims to tackle the issue of spam and marketing emails in civil rights complaints email channels. By combining Robotic Process Automation with AI/ML models, the project automatically identifies and eliminates these unwanted emails, which make up a significant proportion of incoming OASCR emails.

**Acquisition Approval Request Compliance Tool**, _Department_of_Agriculture | USDA | OCIO_
> The Acquisition Approval Request Compliance Tool is a natural language processing model that analyzes text in procurement descriptions to determine the likelihood of an award being IT-related and requiring an AAR. It uses the characteristics of awards with AAR numbers to calculate the probability of IT-relatedness for procurements without an AAR number.

**Predictive Maintenance Impacts**, _Department_of_Agriculture | USDA | OCIO_
> Predictive maintenance is used to forecast the effects of DISC maintenance on infrastructure items. This is done by utilizing various tools such as einblick, mysql, python, linux, and tableau.

**Intelligent Ticket Routing**, _Department_of_Agriculture | USDA | OCIO_
> Intelligent Ticket Routing is a system that automatically directs BMC Remedy tickets to the appropriate work group. It uses various technologies including python, jupyterhub, scikit learn, gitlab, flask, gunicorn, nginx, and erms.

**Video Surveillance System**, _Department_of_Agriculture | USDA | OSSP_
> The Video Surveillance System (VSS) will consist of various components such as cameras, management systems, and network equipment. It will integrate different video surveillance subsystems, allowing for easy management and viewing of video footage from compatible devices. Operators can easily track targets by selecting cameras and accessing advanced features through a unified configuration platform.

## Department_of_Interior

**Land Use Plan Document and Data Mining and Analysis R&D**, _Department_of_Interior | BLM_
> The Land Use Plan Document and Data Mining and Analysis R&D aims to uncover patterns and conflicts in planning documents related to resource management. This research explores the potential to identify these conflicts and map them, allowing for the exclusion, restriction, or stipulation of proposed action locations as defined in the planning documents.

**Improving UAS-derived photogrammetric data and analysis accuracy and confidence for high-resolution data sets using artificial intelligence and machine learning**, _Department_of_Interior | BOR_
> The use of artificial intelligence and machine learning can improve the accuracy and confidence of high-resolution data obtained from unmanned aerial systems (UAS) in photogrammetry. By establishing a standard reference protocol and utilizing these technologies, the potential information contained in UAS-derived photogrammetric products can be analyzed in detail, enabling better-informed decision-making regarding Reclamation's assets.

**Seasonal/Temporary Wetland/Floodplain Delineation using Remote Sensing and Deep Learning**, _Department_of_Interior | BOR_
> Reclamation wanted to assess if advancements in machine learning, particularly deep learning with convolutional neural networks, could enhance the mapping of seasonal/temporary wetlands/floodplains using high-resolution remote sensing data. If successful, these improved mappings could support the management of protected species and offer valuable information for decision-making in operations and planning scenarios.

**Data Driven Sub-Seasonal Forecasting of Temperature and Precipitation**, _Department_of_Interior | BOR_
> Reclamation has conducted two year-long prize competitions to improve sub-seasonal forecasting of temperature and precipitation in the western US. Participants were able to outperform benchmark forecasts from NOAA. Reclamation is now collaborating with Scripps Institute of Oceanography to evaluate and implement the most promising methods from these competitions, as improving sub-seasonal forecasts can greatly benefit water management outcomes.

**Photogrammetric Data Set Crack Mapping Technology Search**, _Department_of_Interior | BOR_
> The project aims to use photogrammetric products to analyze and map cracks on Reclamation facilities. Traditionally, this process was time-consuming and required physical access, but with the use of drones and other devices, it can now be done more efficiently. By developing a standard protocol and utilizing machine learning and AI, this data will be used to enhance decision-making regarding Reclamation assets.

**PyForecast**, _Department_of_Interior | BOR_
> PyForecast is a software designed by Reclamation for water supply forecasting. It utilizes various data-driven methods from statistics and machine learning to achieve accurate predictions.

**Data Driven Streamflow Forecasting**, _Department_of_Interior | BOR_
> Reclamation and CEATI hydropower industry group conducted a year-long evaluation of 10-day streamflow forecasting technologies and held a public competition for the same. The top performing forecast product was from UpstreamTech, an AI/ML forecasting company, while other competitors also outperformed NOAA's benchmark forecasts. Reclamation is now focusing on evaluating UpstreamTech's forecast products and the top performers from the competition.

**Improved Processing and Analysis of Test and Operating Data from Rotating Machines**, _Department_of_Interior | BOR_
> This project aims to improve the analysis of DC ramp test data from rotating machines. Currently, engineering expertise is required to recognize characteristic curves from the test plots. By using machine learning and AI tools, such as linear regression, computer software can analyze the test plots and identify characteristic curves, leading to faster and more reliable analysis of field-performed DC ramp testing.

**Snowcast Showdown**, _Department_of_Interior | BOR_
> Reclamation collaborated with various organizations to organize the Snowcast Showdown Prize Competition. The competition aimed to create efficient algorithms using machine learning methods to estimate distributed snow information by combining data from different sources. The winning methods are currently being assessed and incorporated into a future project with NOAA's River Forecast Centers.

**Well Activity Report Classification**, _Department_of_Interior | BSEE_
> The text discusses the classification of significant well events using data from well Activity Reports. The research focuses on the use of self-supervised and supervised deep neural networks to identify classification systems for these events.

**Well Risk**, _Department_of_Interior | BSEE_
> NASA's Advanced Supercomputer Division will use the research on sustained casing pressure to develop machine learning models for identifying risk factors in wells. This will assist BSEE engineers in detecting potential issues at different stages of well development.

**Level 1 Report Corrosion Level Classification**, _Department_of_Interior | BSEE_
> Level 1 reports obtained from BSEE assess the condition of well platforms and include images that can estimate coating and structural condition. These reports are manually reviewed to determine safety concerns and the need for additional audits. To reduce processing time, BSEE has partnered with NASA to research the use of AI techniques in identifying excessive corrosion in well platform components.

**Autonomous Drone Inspections**, _Department_of_Interior | BSEE_
> The Bureau of Safety and Environmental Enforcement (BSEE) is considering using autonomous drones to detect methane and inspect unsafe platforms on the outer continental shelf. This technology would enable inspections to be conducted safely and efficiently, without the need for extensive training of human operators.

**Sustained Casing Pressure Identification**, _Department_of_Interior | BSEE_
> BSEE and NASA's Advanced Supercomputing Division have partnered to research the use of AI techniques in quickly identifying well platforms experiencing sustained casing pressure (SCP) problems. SCP, which can lead to safety issues, is typically caused by gas migration or defects in tube connections, downhole accessories, or seals. The collaboration aims to mitigate accidents by efficiently identifying wells with SCP.

**Data Mining, Machine Learning and the IHS Markit Databases**, _Department_of_Interior | USGS_
> The use of data mining, machine learning, and IHS Markit databases can assist in identifying potential conflicts between energy development and other priorities. This will lay the foundation for introducing new skills, analyses, and products for the ERP and Mission Area, while also increasing understanding of the capabilities of machine learning in the ERP.

**WRA.HIHR.WAIEE Building capacity for assessment and prediction of post-wildfire water availability**, _Department_of_Interior | USGS_
> The text outlines several objectives aimed at building capacity for assessing and predicting post-wildfire water availability in the western US. These objectives include collecting and analyzing datasets, developing standardized plans and rapid response strategies, establishing the state of the science on water quality impairment, cataloging measurement methods and data needs, and integrating predictive approaches for post-fire hazards across different scales.

**Aluminum Criteria Development in Massachusetts**, _Department_of_Interior | USGS_
> The USGS and MassDEP will gather water-quality data at freshwater sites in Massachusetts to establish aluminum criteria. They will use a multiple linear regression model developed by the EPA in 2017, which considers water chemistry factors such as pH, DOC, and hardness to calculate the criteria.

**WOS.OS.NHM National Temperature Observations**, _Department_of_Interior | USGS_
> The WOS.OS.NHM National Temperature Observations project aims to lessen the workload of Science Centers by streamlining the collection, storage, analysis, and processing of quality assurance data. This will likely result in a greater number of sensors being used in the water temperature network. The project will involve modifying software to accommodate discrete water temperature data, implementing new temperature policies and procedures in data collection software, and creating a pilot program to assist Science Centers with 5-point temperature checks.

**Vegetation and Water Dynamics**, _Department_of_Interior | USGS_
> The text discusses various activities related to vegetation and water dynamics, such as tracking vegetation phenology for drought monitoring and identifying phenological signatures associated with irrigated agriculture and invasive species. It mentions the use of tools like VegDRI and QuickDRI for drought mapping and monitoring, as well as a livestock forage assessment tool for quantifying drought effects on livestock forage deficits. Additionally, understanding surface water dynamics and differentiating between irrigated and non-irrigated systems provides valuable geospatial data for water management and ecological comparisons.

**DO NOT USE (21st Century IMT Applicaitons Analysis AIML)**, _Department_of_Interior | USGS_
> The text discusses the implementation of AI and ML cloud services, such as SageMaker and Rekognition, to provide cloud-based tools and services for investigations using machine learning or artificial intelligence approaches. These capabilities are important for predictive science and facilitating the transition towards actionable intelligence.

**TMDL and Data Mining Investigations**, _Department_of_Interior | USGS_
> Data mining techniques, such as artificial neural network models, are used in TMDL investigations to analyze hydrologic data. These techniques help in understanding and predicting the impact of pollutants on water bodies.

**WRA.NWC.IWAA National Extent Hydrogeologic Framework for NWC**, _Department_of_Interior | USGS_
> The aim of the WRA.NWC.IWAA National Extent Hydrogeologic Framework for NWC project is to deliver consistent predictions of groundwater quality and its impact on surface water, as well as strategies for integrating these predictions into water availability assessments. This involves predicting groundwater salinity and nutrient concentrations, and incorporating these predictions into comprehensive assessments of water availability by coupling them with groundwater flow and flux simulations.

**Twitchell Rice AFRI**, _Department_of_Interior | USGS_
> A study led by UC Davis and in collaboration with UC Berkeley, the USGS, and private consultants will investigate the effects of converting drained row crops to flooded rice production on Twitchell Island in the Sacramento-San Joaquin Delta. The USGS's research objective is to assess the water quality effects, specifically MeHg production, under different rice management practices such as tillage, flooding, and fertilization, and determine the relative methylation potential of each practice.

**WRA.NWC.IWAA National-Extent Groundwater Quality Prediction for the National Water Census and Regional Integrated Water Availability Assessments**, _Department_of_Interior | USGS_
> The project aims to predict groundwater quality nationwide, focusing on salinity and nutrients and their impact on surface water for human and ecological purposes. Additionally, it aims to find ways to incorporate these predictions into comprehensive water availability assessments like the National Water Census and regional Integrated Water Availability Assessments.

**WRA.NWC.EF Economic Valuation of Ecosystem Services in the Delaware River Basin**, _Department_of_Interior | USGS_
> This project aims to evaluate the economic value of ecosystem services in the Delaware River Basin. It involves creating a plan to assess existing data and models, developing a database for fish data, using AI/ML models to predict fish abundances and size under different conditions, and creating economic valuation models for the fishery resource. The project also includes evaluating the validity of the economic models, linking the models to assess tradeoffs between water use and the fisheries resource, and creating a web application for internal use that promotes understanding of the models and allows for assessment of resource tradeoffs.

**Multi-scale modeling for ecosystem service economics**, _Department_of_Interior | USGS_
> The ARIES modeling framework is being expanded to incorporate artificial intelligence and decision rules in order to select appropriate models and data for ecosystem service (ES) economics. This intelligent modeling system will use national and global datasets to accurately map ES and will be implemented in the United States to provide nationwide, AI-supported ES assessment and valuation. The system will also integrate national economic accounts data with ecosystem services data to offer more timely and integrated data at both national and subnational levels.

**WRA.NWC.IWAA Model Application for the National IWAAs and NWC**, _Department_of_Interior | USGS_
> The Model Application for the National IWAAs and NWC (MAPPNAT) project aims to develop and apply models for the National IWAAs reports and the National Water Census. It will provide initial applications, long-term projections, routine updates, and short-term forecasts for various hydrologic sub-disciplines. The project will involve a combination of modeling activities and strategic planning to accommodate multiple sub-disciplines and different areas of expertise.

**SWFL Habitat GIS Model**, _Department_of_Interior | USGS_
> The SWFL Habitat GIS Model aims to update and maintain a digital library of predicted flycatcher breeding habitat, as well as provide a habitat viewer for users to access and display habitat maps. The model also collaborates with other efforts to improve and extend the utility of the flycatcher satellite model through cutting-edge modeling techniques. Additionally, the model participates in regional workgroups and conferences to inform potential and existing users about the SWFL Habitat Viewer.

**21st Century Prospecting: AI-assisted Surveying of Critical Mineral Potential (Reimbursable)**, _Department_of_Interior | USGS_
> The US Mineral Resources Program (MRP) has partnered with USGS and DARPA to use AI-assisted surveying to assess the distribution of critical minerals in the US. This partnership aims to advance scientific understanding of these minerals, identify unknown resources, and enhance national mineral security.

**WRA.WPID.IWP.PUMP ExaSheds stream temperature projections with process-guided deep learning**, _Department_of_Interior | USGS_
> The WRA.WPID.IWP.PUMP project aims to enhance stream temperature models using process-guided deep learning. By incorporating new forms of process guidance and techniques from previous projects, the project seeks to improve the reliability of projections into future decades, specifically in the Delaware River Basin, by ensuring robustness in not-previously-seen conditions like future climates.

**WRA.WPID.IWP.PUMP Turbidity Forecasting**, _Department_of_Interior | USGS_
> The WRA.WPID.IWP.PUMP Turbidity Forecasting project focuses on improving national hydrological forecast models to provide water quality forecasts that are useful for water resource managers. The project aims to enhance the delivery of these forecasts for effective management of water resources.

**DOMESTIC WELL VULNERABILITY SES INDICATORS NEW HAMPSHIRE**, _Department_of_Interior | USGS_
> This text discusses the goals of a study in New Hampshire on the statistical associations between private well data and socio-economic status (SES) data. The study aims to identify indicators of vulnerability to private well water availability and quality and disseminate the findings to various audiences.

**WRA.NWC.WU Gap analysis for water use**, _Department_of_Interior | USGS_
> The USGS Water Use Program is conducting a gap analysis of water-use data in order to improve understanding of uncertainty in water-use estimates and inform future data collection and modeling efforts. The objectives of the analysis include identifying dominant water-use categories, identifying gaps in available data, and finding methods for data estimation to improve modeling efforts. Other objectives include understanding data quality, collaborating with model developers, and improving data related to important water use categories in different regions.

**Two-Dimensional Detailed Hydraulic Analysis**, _Department_of_Interior | USGS_
> The USGS plans to conduct a detailed hydraulic analysis and develop a two-dimensional hydraulic model to assist in decision-making for floodplain management and protection of life and property. This includes objectives such as conducting topographic surveys, analyzing the hydrology of Joachim Creek, and creating flood maps to show inundation extents and water-surface elevation.

**GEMSC Geospatial Modernization and Machine Learning Integration**, _Department_of_Interior | USGS_
> The USGS has a vision for the next decade that includes using advanced technologies like sensor technologies, AI, ML, and high-performance computing to observe and understand change in real-time. The proposed GEMSC project aims to integrate these technologies into their workflows and data services to develop a strategic framework for combining science with IT platforms.

**WRA.HIHR.WQP Process-guided Deep Learning for Predicting Dissolved Oxygen on Stream Networks**, _Department_of_Interior | USGS_
> The aim of this project is to develop a model that can accurately forecast the daily minimum, mean, and maximum levels of dissolved oxygen in stream segments within the Lower Delaware River Basin. This will be achieved by utilizing nationally accessible datasets and implementing a process-guided deep learning approach.

**Kaguya TC DTM Generation**, _Department_of_Interior | USGS_
> The primary objectives for FY21 are to create a processing pipeline to produce Kaguya TC DTMs, generate a set of 100 Kaguya TC DTMs using Ames Stereo Pipeline (ASP), and assess the outcomes of this process.

**AI/ML for aquatic science**, _Department_of_Interior | USGS_
> This project aims to use AI and deep learning to develop computational frameworks and algorithms for recognizing individual fish. The objectives include developing baseline AI models, improving recognition performance in dynamic environments, evaluating AI detection of diseased fish, and evaluating deep learning models for individual recognition and respiration rate using video data.

## National_Aeronautics_and_Space_Administration

**Pre-trained microscopy image neural network Encoders**, _National_Aeronautics_and_Space_Administration | Ames Research Center_
> Pre-trained microscopy image neural network Encoders have been developed and trained on over 100,000 microscopy images of materials. These encoders outperform ImageNet encoders when used in downstream microscopy tasks, such as semantic segmentation, instance segmentation, and regression tasks. The technology allows for the rapid quantification of material microstructure from microscope images, which is crucial for designing and developing new materials.

**High-Performance Quantum-Classical Hybrid Deep Generative Modeling Parameterized by Energy-based Models for Flight-Operations Anomaly Detection**, _National_Aeronautics_and_Space_Administration | Ames Research Center_
> The project aims to develop a high-performance machine learning model for detecting anomalies in flight operations. The model combines classical and quantum computing techniques to improve performance and reduce costs. It uses data from flight recorders to predict anomalies during take-off and landing phases.

**Prediction of Mass Level in Radio Frequency Cryogenics**, _National_Aeronautics_and_Space_Administration | Ames Research Center_
> The ML model predicts the level of fluid in a tank in radio frequency cryogenics by analyzing the radio frequency signature of the fluids. This method is particularly useful in microgravity environments where standard fluid level detection methods are ineffective due to the fluid's unrestricted shape and definition.

**AdaStress**, _National_Aeronautics_and_Space_Administration | Ames Research Center_
> AdaStress is a method that uses reinforcement learning to efficiently identify low-probability but highly important faults in complex systems. It replaces computationally intensive Monte Carlo sampling approaches, which may not be feasible for systems with a small number of critical faults.

**Biological and Physical Sciences (BPS) RNA Sequencing Benchmark Training Dataset**, _National_Aeronautics_and_Space_Administration | Ames Research Center_
> The Biological and Physical Sciences RNA Sequencing Benchmark Training Dataset consists of RNA sequencing data from both spaceflown and control mouse liver samples. The dataset includes real data from NASA GeneLab as well as synthetic data generated using a generative adversarial network. The dataset is used to identify predictive genes through classification methods and hierarchical clustering.

**Biological and Physical Sciences Microscopy Benchmark Dataset**, _National_Aeronautics_and_Space_Administration | Ames Research Center_
> This study utilizes a dataset of fluorescence microscopy images obtained from the Biological and Physical Sciences Open Science Data Repositories. The dataset comprises 93,488 images of mouse fibroblast cell nuclei, which were exposed to Fe particles or X-rays and labeled for DNA double strand breaks. The dataset has been modified to be AI compatible and is publicly accessible on the Registry of Open Data on AWS, along with AI tools developed by the researchers.

**Application that provides bio-inspired solutions to engineering problems (PeTaL)**, _National_Aeronautics_and_Space_Administration | Glenn Research Center_
> PeTaL is an open source AI design tool that uses data from nature and technology to advance biomimicry research and development. It streamlines the bio-inspired design process by integrating various tools and methodologies, and requires curated data for predictive solutions. It is intended for use by designers, engineers, and biologists seeking nature's solutions to design and engineering problems. Currently, it classifies biology journal articles into functional categories, and is being developed to involve text summarization and named entity recognition tasks using large language models.

**Inverse Design of Materials**, _National_Aeronautics_and_Space_Administration | Glenn Research Center_
> This project aims to expedite the process of discovering and optimizing materials by using supervised ML models and Bayesian optimization. It is currently being used to improve weld quality in an NESC investigation and will be applied to designing insulating materials for electrified aircraft. The project's outputs include custom-tailored material recipes and approaches, with a 4x speedup in the overall discovery/design process and a potential 10x increase in throughput by parallelizing the discovery of multiple materials.

**Europa Ice Floe Detection (GSFC Planetary Sciences Lab)**, _National_Aeronautics_and_Space_Administration | Goddard Space Flight Center_
> The Planetary Sciences Lab at GSFC used Machine Learning techniques to identify and categorize ice blocks in the chaotic areas of Jupiter's moon Europa using imagery from the Galileo space probe. Additionally, they employed GANs to create simulated training data for their analysis.

**Titan Methane Cloud Detection (GSFC Planetary Sciences Lab)**, _National_Aeronautics_and_Space_Administration | Goddard Space Flight Center_
> The GSFC Planetary Sciences Lab is using Machine Learning to analyze images from the Cassini space probe in order to identify and study methane clouds on Saturn's moon Titan. This approach allows for the detection and understanding of these clouds in a more efficient and detailed manner.

**Hybrid On-Board and Ground-Based Processing of Massive Sensor Data (HyspIRI IPM)**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> Future space missions will generate large amounts of science data, creating a challenge in transmitting and processing this data. Ground-based tools determine the spacecraft's orbit and prioritize onboard-processing modes based on the type of terrain being overflown. The intelligent onboard processing concept of the HyspIRI mission includes various algorithms such as decision tree classifiers, SVM classifiers, and spectral band indices for data analysis.

**SensorWeb: Volcano, Flood, Wildfire, and Others.**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> The Sensor Web Project utilizes a network of sensors connected through software and the internet to an autonomous satellite observation system. This system allows for expansion of sensors, customization of trigger conditions, and customization of responses. The project is currently being used to monitor the Earth's 50 most active volcanos, as well as conduct experiments on flooding, wildfires, and cryospheric events.

**Mexec Onboard Planning and Execution**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> MEXEC is a software designed to enhance the autonomy and efficiency of robotic explorers by scheduling and executing activities. It was developed as a solution to the fail-operational requirements of the Europa Clipper project, specifically addressing issues caused by the radiation environment around Jupiter. The software uses AI planning, scheduling, and execution technologies to monitor and react to unexpected events, reconstructing command sequences to ensure the safety of the spacecraft while minimizing science loss.

**Autonomous Marine Vehicles (Single, Multiple)**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> Autonomous Marine Vehicles are necessary for studying hydrothermal venting on Ocean Worlds, such as Enceladus, which may hold potential for extraterrestrial life. The focus is on achieving scientific goals autonomously, specifically the localization of features like hydrothermal venting, with limited human interaction. Field programs and simulations have been conducted to develop and test methods for locating these vents.

**Onboard Planner for Mars2020 Rover (Perseverance)**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> The onboard planner for the Mars2020 Rover (Perseverance) uses an incremental scheduling approach to create a feasible schedule based on activity priorities. It considers factors like preheating, maintenance, and rover's wake/sleep cycles when determining valid time intervals for activity placement. To address potential brittleness, the Copilot systems perform a stochastic analysis to set meta parameters for the scheduler. Additionally, research and experiments are being conducted to enhance future rovers' autonomy in various aspects such as planning, execution, image processing, and fault diagnosis, with on-site demonstrations at JPL's simulated mars navigation yard.

**ASPEN Mission Planner**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> ASPEN Mission Planner is an AI-based application framework that supports various planning and scheduling applications. It offers reusable software components such as modeling language, resource management, temporal reasoning, and graphical interface. ASPEN has been successfully utilized in several space missions, including the Modified Antarctic Mapping Mission, Orbital Express, Earth Observing One, and ESA's Rosetta Orbitter.

**CLASP Coverage Planning & Scheduling**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> The CLASP project is a scheduler for space-based or aerial instruments that are dragged across the surface of the body being observed. It helps choose the orientation and on/off times of these instruments to cover as many target points as possible without using too much memory and energy. It is currently being used for optimized scheduling for various missions and has been used for over 100 missions analysis and studies.

**TRN (Terrain Relative Navigation)**, _National_Aeronautics_and_Space_Administration | Jet Propulsion Laboratory_
> Terrain Relative Navigation (TRN) is a system that helps determine the position of a spacecraft during landing on Mars by comparing descent images with a map created from orbital imagery. It was successfully used by the Mars 2020 mission to select a safe landing site amidst potential hazards, and will also be utilized for the Mars Sample Return Lander.

**Lessons Learned Bot (LLB)**, _National_Aeronautics_and_Space_Administration | Langley Research Center_
> The Lessons Learned Bot (LLB) is a Microsoft Excel add-in application that uses a trained Machine Learning (ML) model to search for lessons learned (LL) documents relevant to the text in selected Excel cells. It can find and rank LL records based on just a few words or entire paragraphs of text, providing users with streamlined access to relevant information. In addition to the Excel add-in, there is also an API version of the software available for use within the Agency firewall.

**Autonomous WAiting Room Evaluation (AWARE)**, _National_Aeronautics_and_Space_Administration | Langley Research Center_
> The AWARE system utilizes a security camera and YOLO Machine Learning model to identify and count individuals waiting for service at Langley's Badge & Pass Office. It automatically sends texts and emails requesting extra assistance when the number of people surpasses a predefined limit.

**Geophysical Observations Toolkit for Evaluating Coral Health (GOTECH)**, _National_Aeronautics_and_Space_Administration | Langley Research Center_
> In 2021-2022, Georgia Tech and the University of Rochester collaborated on three capstone projects to create machine learning models that can assess the condition of coral reefs using satellite LIDAR imagery. These projects were supported by Coral Vita and the National Institute of Aerospace, and their findings were shared at the United Nations COP27.

**Pedestrian Safety Corridors for Drone Test Range**, _National_Aeronautics_and_Space_Administration | Langley Research Center_
> The NASA Langley Research Center is conducting experiments with drones and their safety mechanisms. They are expanding their test range to include areas where people walk and drive, and are using image recognition technology to detect pedestrian traffic in real time. The data collected is used to create maps showing the density of human pedestrian traffic, which is then integrated into the GRASP flight risk simulation tool.

**Airplane detection**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> A deep learning algorithm has been developed to detect airplanes in high-resolution satellite imagery. This technology utilizes advanced neural networks to accurately identify and locate airplanes, which can have various applications in surveillance, transportation, and security sectors.

**Deep Learning-based Hurricane Intensity Estimator**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> The Deep Learning-based Hurricane Intensity Estimator is a web tool that utilizes deep learning algorithms to accurately estimate the windspeed of a hurricane. This is achieved by analyzing satellite images, allowing for an objective and reliable assessment of the hurricane's intensity.

**Similarity Search for Earth Science Image Archive**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> The text describes a method called Self Supervised Based Learning for searching image archives in the field of Earth Science. This approach allows users to search for images by using a query image.

**Forecasting Algal Blooms With Ai In Lake Atitlán**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> By analyzing image datasets from various satellites, artificial intelligence (AI) can forecast algal blooms in Lake Atitlán. Machine learning will identify the variables that can predict future blooms, allowing for proactive measures to prevent them not only in Lake Atitlán but also in similar freshwater bodies in Central and South America.

**GCMD Keyword Recommender (GKR)**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> The GCMD Keyword Recommender (GKR) is a tool that utilizes Natural Language Processing to suggest science keywords. It helps researchers and scientists find relevant keywords for their work more efficiently.

**ImageLabeler**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> ImageLabeler is a collaborative web-based tool used for generating training data for machine learning. It allows multiple users to label and tag images collectively, making the process more efficient and accurate.

**Deep Learning Approaches for mapping surface water using Sentinel-1**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> The text discusses the use of deep learning techniques for mapping surface water using Sentinel-1 SAR images. It focuses on a specific architecture called U-Net, which is employed for this purpose.

**Ship detection**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> The process of ship detection involves using deep learning techniques to identify and locate ships in high-resolution satellite imagery. This method relies on advanced algorithms and neural networks to analyze the visual features of the images and accurately detect the presence of ships.

**Mapping sugarcane in Thailand using transfer learning, a lightweight convolutional neural network, NICFI high resolution satellite imagery and Google Earth Engine**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> A lightweight convolutional neural network, called U-Net, combined with transfer learning, is used to map sugarcane pixels in Thailand. The network utilizes a MobileNetV2 based encoder and is trained using NICFI high resolution satellite imagery and Google Earth Engine, specifically the NICFI mosaic.

**Automatic Detection of Impervious Surfaces from Remotely Sensed Data Using Deep Learning**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> The study proposes a method for automatically detecting impervious surfaces using remotely sensed data. It utilizes a U-Net based architecture, with VGG-19 as an encoder block and a custom decoder block, to accurately map impervious surfaces by analyzing Landsat and OSM data patches.

**Predicting streamflow with deep learning**, _National_Aeronautics_and_Space_Administration | Marshall Space Flight Center_
> A long short-term memory model is utilized to forecast streamflow at USGS gauge sites. The model incorporates data from the NASA Land Information System and precipitation forecasts to make accurate predictions.

## Department_of_State

**Within Grade Increase Automation**, _Department_of_State | A_
> A combination of Natural Language Processing and Intelligent Character Recognition is utilized to extract information from the JF-62 form for payroll actions related to within grade increase. The extracted data is then cross-verified with existing reports using Robotic Process Automation, and a formatted file is generated for approval and further processing.

**Tailored Integration Logistics Management System (ILMS) User Analytics**, _Department_of_State | A_
> The Tailored Integration Logistics Management System (ILMS) User Analytics by A/LM aims to enhance user experiences and simplify interactions with the system. By analyzing real system actions and clicks, meaningful information can be extracted to meet the specific needs of users, reducing the time required to complete daily tasks.

**Supply Chain Fraud and Risk Models**, _Department_of_State | A_
> A/LM plans to develop AI/ML models to detect suspicious activity in the Integrated Logistics Management System. These models will focus on key supply chain functions like Asset Management, Procure-to-Pay, and Fleet Management, in order to expand the existing risk analytics and identify potential fraud or malfeasance.

**Conflict Forecasting**, _Department_of_State | A_
> CSO/AA is creating conflict and instability forecasting models that utilize open-source data to predict outcomes such as interstate war, mass mobilization, and mass killings. AI is employed in statistical models, employing machine learning techniques like tree-based methods, neural networks, and clustering approaches.

**Verified Imagery Pilot Project**, _Department_of_State | A_
> The Bureau of Conflict and Stabilization Operations conducted a pilot project using a technology service called Sealr. Sealr utilizes blockchain encryption to secure photos taken on smartphones and uses artificial intelligence to detect tampering or spoofs. The pilot showed that Sealr and similar technologies can enhance remote monitoring of foreign assistance in conflict-affected or inaccessible regions.

**Tailored Integration Logistics Management System (ILMS) Automated User Support Bot**, _Department_of_State | A_
> ILMS has created an automated support bot to assist customers with their support desk inquiries. The bot aims to streamline interactions and reduce the workload of costly support desk agents by resolving simple issues.

**Federal Procurement Data System (FPDS) Auto-Populate Bot**, _Department_of_State | A_
> A bot was developed by A/LM and A/OPE to automate data entry in the Federal Procurement Data System (FPDS), reducing the workload for procurement staff and improving compliance with DATA Act reporting. The bot is currently being used to update approximately 300 FPDS awards each week. Additionally, A/LM collaborated with WHA to create a bot that automates closeout reminders for federal assistance grants and is working on developing bots for receiving report validation and customer service inbox monitoring.

**Product Service Code Automation ML Model**, _Department_of_State | A_
> A/LM created a machine learning model that can analyze unstructured procurement data entered by users, like Requisition Title and Line Descriptions. This model automatically identifies the types of commodities and services being purchased, improving the categorization of procurement items.

**Automatic Detection of Authentic Material**, _Department_of_State | CGFS_
> The Foreign Service Institute School of Language Studies is creating a tool that can automatically find authentic texts in native languages. These texts will be classified based on their topic and proficiency level, helping with the development of foreign language curriculum and language testing kits.

**Automated Damage Assessments**, _Department_of_State | CSO_
> The Conflict Observatory program utilizes AI and machine learning to analyze satellite images and record war crimes and human rights violations in Ukraine. It specifically focuses on conducting automated assessments of various structures, such as critical infrastructure, hospitals, schools, and crop storage facilities, to determine the extent of damage caused by the conflict.

**ServiceNow AI-Powered Virtual Agent (Chatbot)**, _Department_of_State | CSO_
> IRM's BMP Systems intends to integrate ServiceNow's Virtual Agent into their current applications to facilitate user interaction with support and data inquiries. The Virtual Agent utilizes ServiceNow's AI technology, which is offered as part of their Platform as a Service (PaaS) solution.

**Automated Burning Detection**, _Department_of_State | CSO_
> The Village Monitoring System program utilizes AI and machine learning to analyze commercial satellite imagery on a daily basis. Its purpose is to detect anomalies by examining the near-infrared band, enabling automated identification of burning incidents.

**Apptio**, _Department_of_State | CSO_
> Apptio is used by Apptio: Working Capital Fund to bill bureaus for consolidated services and help them budget for future service costs. It has the ability to extrapolate future values using different formulas.

**NLP for Foreign Assistance Appropriations Analysis**, _Department_of_State | F_
> The use of natural language processing (NLP) allows for the automation of earmark and directive extraction from the annual appropriations bill in Foreign Assistance Appropriations (F/RA) analysis. This eliminates the need for manual processing, making the task more efficient and streamlined.

**eRecords M/L Metadata Enrichment**, _Department_of_State | FSI_
> The Department's eRecords archive uses machine learning models to enhance metadata, making it easier to find and review records. The models help extract entities, analyze sentiment, classify documents, and identify document types.

**Global Audience Segmentation Framework**, _Department_of_State | GPA_
> The Global Audience Segmentation Framework is a prototype system that gathers and examines media clips reports from approximately 70 Embassy Public Affairs Sections on a daily basis. It aims to segment and categorize the global audience based on the collected data.

**Machine-Learning Assisted Measurement and Evaluation of Public Outreach**, _Department_of_State | GPA_
> GPA has developed a machine-learning assisted system to collect, analyze, and summarize the digital content footprint of the Department for public outreach purposes. This system helps in measuring and evaluating the effectiveness of their outreach efforts on a global scale.

**GPATools and GPAIX**, _Department_of_State | GPA_
> GPATools and GPAIX are production systems developed by GPA that are used to test potential messages on a large scale. These systems help determine the most effective way to reach and engage specific target audiences by conducting tests across segmented foreign sub-audiences.

**Facebook Ad Test Optimization System**, _Department_of_State | GPA_
> The Facebook Ad Test Optimization System is a media collection and analysis system developed by GPA. It gathers data from various open and commercial media clips services to provide a real-time overview of media coverage worldwide.

**AI Capabilities Embedded in SMART**, _Department_of_State | IRM_
> The SMART system on OpenNet incorporates AI models in its backend to perform various tasks such as extracting objects within cables, analyzing the sentiment of cables, extracting keywords of identified topics, and analyzing historical data to provide recommendations to users when composing cables.

**NLP to pull key information from unstructured text**, _Department_of_State | PM_
> NLP can be utilized to extract important details like country names and agreement dates from lengthy unstructured pdf documents. This allows for efficient retrieval of key information from large volumes of text.

**K-Means clustering into tiers**, _Department_of_State | PM_
> The text suggests using K-Means clustering to categorize countries into tiers. This can be achieved by analyzing data gathered from open source and bureau sources.

**Deepfake Detector**, _Department_of_State | R_
> The Deepfake Detector is a deep learning model that can determine if an image contains a real person's face or a synthetically generated face created using Generative Adversarial Networks (deepfakes). It analyzes the image and classifies it as either real or fake.

**Louvain Community Detection**, _Department_of_State | R_
> Louvain Community Detection is a method that groups nodes in a social network into communities based on their similarity. It clusters similar nodes together, forming distinct groups within the network.

**SentiBERTIQ**, _Department_of_State | R_
> SentiBERTIQ is a tool that uses deep contextual AI to analyze text and extract subjective information. It was trained using a multilingual BERT model on 2.2 million labeled tweets in multiple languages, and it can assign a sentiment to each text document and provide a confidence interval in a CSV format for user review.

**Image Clustering**, _Department_of_State | R_
> The text explains that image clustering technique involves utilizing a pre-trained deep learning model to create image embeddings. These embeddings are then employed for hierarchical clustering, enabling the identification of images that are similar to each other.

**Topic Modeling**, _Department_of_State | R_
> Topic modeling is a technique that groups text into themes by analyzing the frequency of words used in documents. This method has been used to analyze digital media articles and social media posts and can be easily implemented using Python libraries.

**Optical Character Recognition – text extraction**, _Department_of_State | R_
> The text explains that Optical Character Recognition (OCR) is a method to extract text from images. This can be achieved by utilizing standard python libraries. The input for this process is websites from which data is collected.

**Text Similarity**, _Department_of_State | R_
> GEC A&R's Text Similarity capability compares texts and determines if they are the same or very similar by calculating cosine similarity. If texts have a high cosine similarity, they are grouped together and can be reviewed by analysts for further examination.

**forecasting**, _Department_of_State | R_
> Forecasting involves using statistical models to predict future outcomes. It has been used to project the expected number of COVID cases and predict the occurrence of violent events by analyzing tweets.

**TOPIQ**, _Department_of_State | R_
> The TOPIQ tool by GEC A&R is designed to classify text into topics automatically. It utilizes Latent Dirichlet Allocation (LDA), a natural language processing technique, to identify topics from a collection of documents and determine the probability of each document belonging to a specific topic.

## Department_of_Labor

**Form Recognizer for Benefits Forms**, _Department_of_Labor_
> Form Recognizer for Benefits Forms is a custom machine learning model that extracts data from intricate forms and assigns data entries to field headers. This model takes a document or scanned image of the form as input and generates a JSON response with extracted key/value pairs by utilizing the custom trained model.

**Scanner Data Product Classification**, _Department_of_Labor_
> The Bureau of Labor Statistics (BLS) receives data from corporations about the cost of their goods and services. BLS has started using machine learning to label this data with Entry Level Item (ELI) codes based on word frequency counts from item descriptions. Logistic regression is used to estimate the probability of each item being classified in each ELI category, and any classifications below a certain probability threshold are flagged for human review.

**Claims Document Processing**, _Department_of_Labor_
> Claims Document Processing involves training custom natural language processing models to determine if a physician's note contains causal language. This is done to identify any statements that establish a cause-and-effect relationship in the note.

**DOL Intranet Website Chatbot Assistant**, _Department_of_Labor_
> The DOL Intranet Website Chatbot Assistant is a conversational chatbot that is available on DOL intranet websites. It is designed to assist users by answering common procurement questions and providing information about specific contracts.

**Automatic Data Processing Workflow with Form Recognizer**, _Department_of_Labor_
> Form Recognizer offers automatic data processing workflow, efficiently extracting the necessary information from complex forms. This technology simplifies the task of data extraction from various documents, streamlining the overall workflow.

**OEWS Occupation Autocoder**, _Department_of_Labor_
> The OEWS Occupation Autocoder is a tool that takes state submitted response files containing occupation titles and job descriptions and assigns up to two Standard Occupational Classification (SOC) codes to each title. These codes, along with their probabilities, are sent back to the states to assist human coders in assigning SOC codes to the responses.

**Automatic Document Processing**, _Department_of_Labor_
> Automatic Document Processing involves using technology to extract pre-defined selection boxes from a continuation of benefits form. This process is done automatically without manual intervention.

**Official Document Validation**, _Department_of_Labor_
> AI technology can be used to validate official documents by detecting discrepancies in addresses and identifying text that is unreadable or distorted in letters sent to recipients of benefits. This helps ensure the accuracy and clarity of important communications.

**Text to Speech Conversion**, _Department_of_Labor_
> Text to speech conversion refers to the process of converting written text into spoken words with the help of neural networks. This technique aims to create more lifelike and human-sounding applications by utilizing natural language processing models.

**Language Translation**, _Department_of_Labor_
> The text describes a service that offers language translation for published documents and websites. This service utilizes natural language processing models to achieve accurate translations.

**Audio Transcription**, _Department_of_Labor_
> The text refers to the process of converting spoken words into written text using natural language processing models. This transcription is done for the purpose of keeping records.

**Website Chatbot Assistant**, _Department_of_Labor_
> The website chatbot assistant provides assistance to users by offering basic information about the program, providing contact details, and helping in tracking petition case status.

**Case Recording summarization**, _Department_of_Labor_
> An open source large language model is utilized to summarize case recording documents that do not contain any personal identifiable information or sensitive data. The process is performed outside of the DOL technical environment and is carefully reviewed by human note takers.

**Hololens**, _Department_of_Labor_
> The Hololens is an artificial intelligence (AI) technology that allows inspectors to visually examine hazardous and elevated spaces from a secure position. This innovative tool ensures the safety of inspectors while conducting inspections in high-risk areas.

**Call Recording Analysis**, _Department_of_Labor_
> The text refers to the analysis of recorded calls made to Benefits Advisors in the DOL Interactive Voice Response center. This analysis is done automatically, most likely using AI or machine learning technology, to gain insights from the recorded calls.

**Expenditure Classification Autocoder**, _Department_of_Labor_
> The Expenditure Classification Autocoder is a machine learning model that categorizes reported expense descriptions from Consumer Expenditure Diary Survey participants into item codes. This custom model streamlines the process of assigning expense classification categories to improve efficiency and accuracy.

**Data Ingestion of Payroll Forms**, _Department_of_Labor_
> A custom machine learning model is used to extract data from complex forms and label the data entries according to field headers. By inputting a document or scanned image of the form, the model generates a JSON response containing extracted key/value pairs based on the form's analysis with the trained model.

**Electronic Records Management**, _Department_of_Labor_
> The text discusses the use of AI and NLP in electronic records management to meet NARA metadata standards for permanent federal documents. It suggests that using AI can help identify data within the documents, while NLP can aid in classifying and summarizing them.

## U.S._Agency_for_International_Development

**Project Vikela**, _U.S._Agency_for_International_Development | Bureau for Development, Democracy, and Innovation (DDI)_
> Project Vikela aims to utilize artificial intelligence technology to identify and flag illegal rhino horn in X-ray scanners used for airplane luggage. The goal of this initiative is to enhance the detection and prevention of illegal wildlife trafficking by implementing AI solutions in airport security systems.

**Morogoro youth empowerment through establishment of social innovation (YEESI) lab for problem-centered training in machine vision**, _U.S._Agency_for_International_Development | Bureau for Development, Democracy, and Innovation (DDI)_
> The text describes a project that aims to establish a social innovation lab in Morogoro, Tanzania, to provide machine vision training to young people in the area. The project aims to address agricultural problems in the region, such as disease detection, weed classification, pest detection, crop yield estimation, and crop vigor estimation, by developing experts and algorithms in machine vision. Its goal is to contribute to the improvement of farming practices and the livelihoods of farmers in Tanzania.

**Long-term impacts of land-use/land-cover dynamics on surface water quality in Botswana’s reservoirs using satellite data and artificial intelligence methods: Case study of the Botswana’s Limpopo River Basin (1984-2019)**, _U.S._Agency_for_International_Development | Bureau for Development, Democracy, and Innovation (DDI)_
> This study examines the long-term impacts of land-use/land-cover dynamics on surface water quality in Botswana's reservoirs, specifically in the Limpopo River Basin. The study aims to understand the relationships between land-use change, socioeconomic development, climate change, and their effects on water quality and availability in the basin. It will utilize satellite data, artificial intelligence methods, and in-situ water quality measurements to develop models for estimating water quality and mapping water quality indexes.

**Machine Learning for Peace**, _U.S._Agency_for_International_Development | Bureau for Development, Democracy, and Innovation (DDI)_
> Machine Learning for Peace is an initiative that falls under the INSPIRES program. It aims to develop new solutions and programmatic innovations for creating resilient spaces. More information about the program and its activities can be found on its website at https://web.sas.upenn.edu/mlp-devlab/.

**Gender differentiated credit scoring**, _U.S._Agency_for_International_Development | Bureau for Development, Democracy, and Innovation (DDI)_
> The University of California, Berkeley is creating a machine learning model to evaluate gender differentiated credit scoring for Rappicard customers in Mexico. By comparing this model to Rappi's current approach, they aim to assess if a gender-based model improves women's access to credit.

**Media Early Warning System (MEWS)**, _U.S._Agency_for_International_Development | Bureau for Development, Democracy, and Innovation (DDI)_
> The Media Early Warning System (MEWS) is designed to identify and address malicious narratives by monitoring social media for changes in images and videos and analyzing trends. Its purpose is to detect and counteract any harmful narratives that may be spreading through these mediums.

**Mali: AI predictions for the optimization of the allocation of the distribution of COVID-19 vaccines**, _U.S._Agency_for_International_Development | Bureau for Global Health (GH)_
> AI technology was utilized in Mali to create a pandemic preparedness AI model for the distribution of COVID-19 vaccines. The model focused on targeting hotspots with positive cases and pregnant/breastfeeding women using DHIS2 data, serving as a proof-of-concept for optimizing vaccine allocation.

**Serbia: AI predictions for the utilization of hospital beds**, _U.S._Agency_for_International_Development | Bureau for Global Health (GH)_
> AI technology was used in Serbia to predict bed occupancy at hospitals using data from 2019, with an average error of around 20% by department. This proof-of-concept model was developed at the request of the Institute of Public Health to understand the potential of AI and its value in decision-making.

**Using ML for predicting treatment interruption among PLHIV in Nigeria**, _U.S._Agency_for_International_Development | Bureau for Global Health (GH)_
> An algorithm was developed using data from the SIDHAS project in Nigeria to predict the likelihood of treatment interruption among newly initiated ART patients. The algorithm was successfully integrated into the LAMIS system and the results are shared with healthcare staff to provide more intensive support to high-risk patients. A qualitative assessment was also conducted to gather healthcare workers' perceptions of ML and identify additional support needed to incorporate ML into their routine work.

**Breakthrough RESEARCH’s Social Media Listening**, _U.S._Agency_for_International_Development | Bureau for Global Health (GH)_
> Breakthrough RESEARCH conducted social media listening in Nigeria, analyzing 12,301 social media posts to examine gender-related online conversations over the past five years. By using Crimson Hexagon's machine learning algorithm, the posts were categorized by topic, allowing researchers to uncover trends, misinformation, attitudes, and social norms. This method provided a more comprehensive way to monitor and track social media conversations compared to traditional research methods in public health and SBC programs.

**Indonesia: AI predictions for improving forecasts for TB drugs**, _U.S._Agency_for_International_Development | Bureau for Global Health (GH)_
> Indonesia plans to utilize AI technology to create a forecasting model for TB drugs. This will help the Ministry of Health in accurately estimating the required quantities of these drugs each year through their integrated data platform, SatuSehat.

**NASA SERVIR - Bias Correcting Historical GEOGloWS ECMWF Streamflow Service (GESS) data using Machine Learning (ML) Techniques**, _U.S._Agency_for_International_Development | Bureau for Latin America and the Caribbean_
> NASA SERVIR is using machine learning techniques, specifically the Long Short Term Memory (LSTM) Model, to bias correct historical streamflow data from the GEOGloWS ECMWF Streamflow Service (GESS). This correction allows for more accurate local information from the globally available GESS discharge data, which is used to forecast flow on every river worldwide.

**NASA SERVIR - Using artificial intelligence to forecast harmful algae blooms in Lake Atitlán, Guatemala**, _U.S._Agency_for_International_Development | Bureau for Latin America and the Caribbean_
> NASA SERVIR has developed an application that utilizes artificial intelligence and machine learning to predict daily algal blooms in Lake Atitlán, Guatemala. The forecasting system is being used by Lake Authorities to enhance their Harmful Algal Blooms Alert System, with support from National Geographic and Microsoft.

**NASA SERVIR - Mapping urban vulnerability using AI techniques**, _U.S._Agency_for_International_Development | Bureau for Latin America and the Caribbean_
> NASA SERVIR aims to enhance urban vulnerability assessment in major cities by developing AI techniques to map informal settlements using satellite imagery. The goal is to create standardized and reproducible methods that can be applied in various population centers.

## Social_Security_Administration

**CDR Model**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The CDR Model utilizes machine learning to pinpoint disability cases that are more likely to show medical improvement and highlights them for a continuing disability review. By applying advanced techniques, this model helps identify cases that may no longer require disability benefits.

**SSI Redetermination Model**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The SSI Redetermination Model utilizes machine learning to pinpoint supplemental security income cases that are likely to have significant overpayments due to changes in financial eligibility. These cases are then flagged for technician review, allowing for appropriate action to be taken.

**Modernized Development Worksheet (MDW)**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The Modernized Development Worksheet (MDW) utilizes artificial intelligence (AI) and natural language processing to quickly categorize textual data from claim development tasks. This streamlines the technician review process and improves efficiency.

**PATH Model**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The PATH Model is a machine learning-based approach that predicts which cases are more likely to receive an allowance at the hearing level. It uses this prediction to refer those cases to administrative law judges or senior adjudicators for a prioritized review.

**Medicare Part D Subsidy Model**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The Medicare Part D Subsidy Model employs machine learning to pinpoint cases with potentially inaccurate Medicare Part D subsidies and highlights them for technician evaluation.

**Rep Payee Misuse Model**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The Rep Payee Misuse Model employs machine learning to predict the likelihood of representative payees misusing resources. It then alerts technicians to investigate those specific cases.

**Anomalous iClaim Predictive Model**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The anomalous iClaim predictive model is a machine learning model that detects high-risk iClaims. Once identified, these claims are forwarded to Operations for further evaluation before any further action is taken to process them.

**Pre-Effectuation Review / Targeted Denial Review Models**, _Social_Security_Administration | Office of Analytics, Review, and Oversight_
> The Pre-Effectuation Review and Targeted Denial Review models utilize machine learning to pinpoint cases that are most likely to have errors in determining disability eligibility. These models then recommend these cases for thorough quality review checks to ensure accuracy.

**Insight**, _Social_Security_Administration | Office of Analytics, Review, and Oversight; Office of Hearing Operations, Office of Disability Systems_
> Insight is decision support software that aids Disability Program adjudicators in making high-quality, efficient, and consistent decisions. It analyzes the text of disability decisions and other case data to provide real-time alerts and reference information to adjudicators. With interactive tools, adjudicators can expedite their work and address any issues before the case progresses further. Insight utilizes natural language processing and artificial intelligence techniques to power its features.

**Intelligent Medical Language Analysis Generation (IMAGEN)**, _Social_Security_Administration | Office of Disability Determinations, Office of Disability Information Systems_
> IMAGEN is an IT Modernization Disability Analytics & Disability Decision Support (ADDS) Product that will enhance the analysis of medical records to improve disability determinations and decisions. It will utilize machine learning technologies like Natural Language Processing (NLP) and predictive analytics to transform text into data and support fraud prevention and detection efforts.

**Handwriting recognition from forms**, _Social_Security_Administration | Office of Disability Information Systems, Office of Hearing Operations, Office of Appellate Operations_
> AI technology is used to recognize and interpret handwritten entries on standard forms. This is done to support robotic process automation and can also be used independently.

**Duplicate Identification Process (DIP)**, _Social_Security_Administration | Office of Disability Information Systems, Office of Hearing Operations, Office of Appellate Operations_
> The Duplicate Identification Process (DIP) aims to improve the efficiency of identifying and flagging duplicate cases for hearings. It utilizes artificial intelligence software and image recognition technology to accurately identify duplicates in accordance with SSA policy.

**Quick Disability Determinations Process**, _Social_Security_Administration | Office of Retirement of Disability Programs_
> The Quick Disability Determinations (QDD) process uses a computer model to analyze initial disability applications and identify cases with a high likelihood of approval. This helps speed up the processing of these cases by prioritizing them. The model is regularly updated to improve its accuracy and efficiency.

**Mobile Wage Reporting (MOBWR)**, _Social_Security_Administration | Office of Systems_
> Mobile Wage Reporting (MOBWR) utilizes AI to quickly extract text and data from scanned images or documents such as pay stubs or payroll information. This technology allows for expedited processing of these documents.

## Department_of_Treasury

**CX Analytics**, _Department_of_Treasury_
> IRS' Customer Experience (CX) Analytics is a tool that analyzes customer service data to find problems, anomalies, and areas for improvement across different service channels. It uses various sources of customer-related information to identify issues and suggest ways to enhance the overall customer experience.

**Inventory Item Replenishment MLR Modeling POC - Phase 1a**, _Department_of_Treasury_
> The Bureau of Engraving and Printing conducted a proof of concept (POC) to test the use of predictive analytics in predicting processing lead times for their supply chain management. The POC involved developing a Multiple Linear Regression (MLR) model that utilizes historical data to predict delivery times for items. This model helps the Bureau determine if an item will be delivered on time or if there will be a delay, allowing them to make proactive decisions to avoid inventory shortages.

**Collection Chat Bot**, _Department_of_Treasury_
> The Collection Chat Bot will contain a Natural Language Understanding (NLU) model, which will be housed in the eGain intent engine. This NLU will analyze text input from customers, known as utterances, and match them with a relevant intent, providing the appropriate knowledge article in response.

**Inventory Item Replenishment MLR Modeling POC - Phase 2**, _Department_of_Treasury_
> The Bureau of Engraving and Printing wanted to use their new Cloudera Data Science Workbench application to predict if an item will be delivered on time by the vendor. The model uses historical data to make binary predictions, allowing the Bureau to prepare for potential inventory shortages if the model predicts a delay.

**NRP Redesign**, _Department_of_Treasury_
> The NRP Redesign aims to use advanced AI machine learning techniques to accurately estimate compliance baselines for tax gap estimation, improper payments reporting, workload identification and selection models, and policy analysis. The system will rely on existing NRP data that meets acceptable precision and quality standards to ensure reliable output.

**TAS Virtual Assistant**, _Department_of_Treasury_
> The TAS Virtual Assistant Chatbot collects information from taxpayers and guides them to useful resources on the IRS and TAS public websites. It helps users find the information they need efficiently.

**Collection Voice Bot**, _Department_of_Treasury_
> The Nuance Natural Language Understanding (NLU) model will be integrated into the Automated Collections IVR (ACI) main menu. This model will analyze customer speech input and determine the intent, guiding them through the appropriate call path.

**Predictive equipment maintenance system**, _Department_of_Treasury_
> The text discusses the implementation of a predictive equipment maintenance system, with the goal of improving equipment uptime, safety, and reducing maintenance costs. The research is focused on determining the feasibility of such a system and identifying the necessary tools for its successful implementation.

**DATA Act**, _Department_of_Treasury_
> The DATA Act Bot uses natural language processing to compare IRS Federal Procurement Data System reporting with contract documents, ensuring that information such as dollar amounts, dates, and location of work matches. The validation models' performance for each data element is measured using F1 scores.

**Inventory Item Replenishment MLR Modeling Pilot - Phase 1b**, _Department_of_Treasury_
> In order to demonstrate the effectiveness of predictive analytics, the Bureau of Engraving and Printing developed a Logistic Regression model to predict whether items would be delivered by the specified "Need by Date". By analyzing historical data, the model provides binary predictions that determine whether an item will be delivered on-time or if there will be a delay, allowing the Office of Supply Chain Management to make proactive decisions to prevent inventory shortages.

**Appeals Case Memorandum**, _Department_of_Treasury_
> The Appeals Case Memorandum (ACM) uses natural language processing to extract and organize unstructured text from IRS ACM documents. It identifies key information and presents it in a structured format for review by IRS staff. The results include detailed analysis on text relationships, issues, and citation narrative text paragraphs, providing insights on commonly adjusted issues during the appeals process.

**Coin quality inspection system**, _Department_of_Treasury_
> The coin quality inspection system aims to automate the process of visually inspecting coins for defects on production lines. Currently, operators manually inspect coins for quality, leading to potential errors and waste. The goal is to enhance the quality control process by researching and implementing feasible automated tools.

**Account Management Chatbot**, _Department_of_Treasury_
> The Account Management Chatbot uses a natural language understanding model to understand user queries and match them with specific intents. It then provides relevant knowledge articles as responses.

**Taxpayer Accessibility - Machine Translation (MT)**, _Department_of_Treasury_
> Taxpayer Accessibility Machine Translation (MT) is a SaaS product that uses Amazon Translate, a neural machine translation service. It allows integration of existing translations and workflows through a centralized repository of IRS glossaries, providing more accurate English-Spanish translations that reflect native language.

## Department_of_Transportation

**Remote Oceanic Meteorological Information Operations (ROMIO)**, _Department_of_Transportation | ANG_
> ROMIO is an operational demonstration that assesses the possibility of sending convective weather information to aircraft in remote oceanic regions. It utilizes converted weather satellite data, lightning, and weather prediction model data to identify areas of thunderstorm activity and cloud top heights. Additionally, AI is employed to enhance the accuracy of the output by comparing past activity with ground truth data.

**Determining Surface Winds with Machine Learning Software**, _Department_of_Transportation | ANG_
> Machine learning software has been proven effective in analyzing camera images of a wind sock to accurately determine surface wind speed and direction in areas without weather observing sensors. This technology provides reliable information in remote locations where traditional weather monitoring systems are not available.

**Surface Report Classifiier (SCM/Auto-Class)**, _Department_of_Transportation | ATO_
> The Surface Report Classifier (SCM/Auto-Class) is a system that categorizes surface incident reports based on the type of event, such as runway incursion or taxiway excursion. It also further classifies runway incursions based on severity, using categories A, B, C, D, and E.

**Automated Delay detection using voice processing**, _Department_of_Transportation | ATO_
> Automated delay detection can be achieved through voice processing technology. This method is necessary to accurately track delays caused by various factors, such as vectoring, which are currently not reported or accounted for. Implementing voice detection would enable automated detection of these delay events.

**Regulatory Compliance Mapping Tool**, _Department_of_Transportation | AVS_
> The Regulatory Compliance Mapping Tool (RCMT) is used by the AVS International office to identify compliance with ICAO Standards and Recommended Practices (SARPs). RCMT analyzes thousands of pages of documents to find SARPs and evaluate FAA Orders, suggesting evidence of compliance based on the evaluation. It uses Natural Language Processing (NLP) and AI technology to extract the meaning of the text and establish matches between ICAO SARPs and FAA text.

**JASC Code classification in Safety Difficulty Reports (SDR)**, _Department_of_Transportation | AVS_
> AVS and George Mason University collaborated to develop a method using Natural Language Processing and Machine Learning to predict joint aircraft system codes (JASC) chapter codes from service difficulty reports (SDR). This method can be used to verify the accuracy of codes provided in SDR entries or assign codes when none were given.

**Course Deviation Identification for Multiple Airport Route Separation (MARS)**, _Department_of_Transportation | Aviation Safety (AVS)_
> The MARS program aims to establish reduced separation standards between PBN routes in terminal airspace, potentially allowing for more airports in busy metropolitan areas. To build a safety case, collision risk models are needed, including one that analyzes the behavior of aircraft that deviate from the correct navigation procedure. Previous research has utilized Machine Learning to identify similar events during departure procedures by filtering incident data.

**Machine Learning for Occupant Safety Research**, _Department_of_Transportation | NSR Human Injury Research Division_
> Machine learning models can be used to predict head kinematics in vehicle crashes using deep learning techniques. These models can extract 3D kinematics from 2D crash videos, providing a reliable method for calculating head kinematics when sensors are not available or when sensor data is of low quality. The output of these models is the angular velocity, which can be used to predict the likelihood of injury.

**Machine Learning for Occupant Safety Research**, _Department_of_Transportation | NSR Human Injury Research Division_
> This text describes the use of deep learning in predicting two important parameters, Delta-V and PDOF, for occupant safety research. By analyzing real-world crash images, deep learning models can accurately predict these parameters without the need for WinSmash software or estimations by crash examiners. This not only improves efficiency but also provides rapid results within milliseconds.

**Offshore Precipitation Capability (OPC)**, _Department_of_Transportation | NextGen (ANG)_
> OPC is a system that combines data from various sources to create a radar-like view of precipitation. By utilizing machine learning techniques and historical data, OPC enhances the precision of identifying and measuring the intensity of precipitation in specific locations.

**Automatic Track Change Detection Demonstration and Analysis**, _Department_of_Transportation | Office of Research, Development and Technology_
> The text describes the Automatic Track Change Detection system, which uses a DeepCNet-based neural network to identify and classify track-related features in line-scan images from rail-bound inspection systems. This system can detect changes in the track's status or between different inspections based on geolocation and provide notifications.

**Development of Predictive Analytics Using Autonomous Track Geometry Measurement System (ATGMS) Data**, _Department_of_Transportation | Office of Research, Development and Technology_
> The Autonomous Track Geometry Measurement System (ATGMS) data can be used to develop predictive analytics for analyzing and predicting track locations of concern. By utilizing track geometry measurements and exceptions, the system can generate inspection reports that include trends in track geometry measures and estimate the time to failure, allowing for efficient maintenance and safety measures.

**Crushed Aggregate Gradation Evaluation System**, _Department_of_Transportation | Office of Research, Development and Technology_
> The Crushed Aggregate Gradation Evaluation System is a computer-based tool that uses deep learning computer vision algorithms to analyze the particle size grading in aggregate materials. It takes images of ballast cross sections as input and provides an output known as the ballast fouling index, which indicates the level of contamination in the ballast.

**PHMSA Rule Making**, _Department_of_Transportation | PHMSA Office of Chief Counsel (PHC)_
> The PHMSA is implementing the use of ChatGPT, an artificial intelligence system, to support its rulemaking processes. ChatGPT will provide sentiment analysis, relevance analysis, synopses of posted comments, cataloging of comments, and identification of duplicate comments. This will lead to increased efficiency, reduced effort, and the ability to handle high levels of public interest or scrutiny in rulemaking.

## U.S._General_Services_Administration

**Key KPI Forecasts for GWCM**, _U.S._General_Services_Administration | FAS_
> The Key KPI Forecasts for GWCM involves using monthly historical data to predict KPIs for the upcoming fiscal year. The initial focus is on total agency/category spend, and if the pilot program succeeds, the same approach can be applied to other KPIs.

**Acquisition Analytics**, _U.S._General_Services_Administration | FAS_
> Acquisition Analytics utilizes comprehensive transaction data to categorize each transaction within the Government-wide Category Management Taxonomy.

**City Pairs Program Ticket Forecast and Scenario Analysis Tools**, _U.S._General_Services_Administration | FAS_
> The City Pairs Program Ticket Forecast and Scenario Analysis Tools use air travel purchase data to predict future ticket sales for the City Pair Program. It provides forecasts for the current and upcoming fiscal year, including monthly predictions and breakdowns by DOD vs Civilian, Agency, and Region.

**Category Taxonomy Refinement Using NLP**, _U.S._General_Services_Administration | FAS_
> This text suggests that NLP (Natural Language Processing) can be used to improve the categorization of products by extracting tokens from their descriptions. This can help accurately determine the target markets for specific Product Service Codes (PSCs).

**Contract Acquisition Lifecycle Intelligence (CALI)**, _U.S._General_Services_Administration | FAS (QP0A)_
> The CALI tool is an automated machine learning evaluation tool used in the Source Selection process. It streamlines the evaluation of vendor proposals by analyzing documents and metadata in four key areas. It is currently being trained with sample data from the Multiple Award Schedule program.

**Chatbot for Federal Acquisition Community**, _U.S._General_Services_Administration | FAS / GSA IT (IC)_
> The GSA FAS NCSC plans to implement a chatbot to improve customer experience by automating responses to frequently asked questions. This will reduce the need for live chat agents and free up resources for other customer service initiatives, although customers can still request to speak with a live agent if desired.

**Document Workflow / Intelligent Data Capture and Extraction**, _U.S._General_Services_Administration | GSA IT (IC)_
> GSA aims to improve its document workflow platform by implementing intelligent data capture and extraction. This involves capturing and transferring important data from both structured and unstructured documents, such as PDF files, to the appropriate process, workflow, or decision engine.

**Service Desk Virtual Agent (Curie)**, _U.S._General_Services_Administration | GSA IT (IDT)_
> Curie is a virtual agent that utilizes machine learning to offer predictive outcomes for chat interactions. It is a natural language chatbot, also known as a virtual assistant, designed to enhance the customer service experience for employees' IT service needs by using knowledge-based articles.

**Service Desk Generic Ticket Classification**, _U.S._General_Services_Administration | GSA IT (IDT)_
> A model is being developed to classify generic Service Desk tickets and automate the re-routing process to the appropriate team. Currently, this re-routing is done manually, but the model aims to automate it for the top 5 most common ticket types.

**Solicitation Review Tool (SRT)**, _U.S._General_Services_Administration | OGP_
> The Solicitation Review Tool (SRT) collects SAM.gov data on ICT solicitations and uses machine learning algorithms to determine compliance. A Natural Language Processing model is used to identify solicitation containing compliance language. Agencies are required to review and validate the SRT predictions, and random manual reviews are conducted by the GSA on a monthly basis.

**Classifying Qualitative Data**, _U.S._General_Services_Administration | TTS_
> USAGov and USAGov en Español gather qualitative data from various sources such as survey comments, web searches, and call center chat transcripts. This data is then categorized by topic to identify areas that require product improvements or enhancements.

**IAE FSD CCAI Virtual Agent**, _U.S._General_Services_Administration | TTS/IAE_
> The IAE FSD CCAI Virtual Agent, known as SAM, is an AI-powered virtual assistant that utilizes manual learning to comprehend customer requirements and deliver suitable responses. SAM is designed to understand natural language and effectively address customer needs.

## National_Archives_and_Records_Administration

**AI Pilot Project to Screen and Flag for Personally Identifiable Information (PII) in Digitized Archival Records**, _National_Archives_and_Records_Administration | Information Service_
> The NARA Information Service team is collaborating with other departments on a pilot project to use AI tools to identify and redact personally identifiable information (PII) in digitized archival records. The project will screen both existing and upcoming digitized records for PII, and the agency plans to develop a user interface-driven tool for preliminary scans and add custom entities for detection.

**Auto-fill of Descriptive Metadata for Archival Descriptions**, _National_Archives_and_Records_Administration | Information Service_
> Auto-fill of descriptive metadata for archival descriptions is the process of automatically completing the necessary information for records that will be made public. The National Archives has released millions of pages of records with minimal descriptive metadata, so self-describing records use the document's content and available metadata to predict values for the descriptive information.

**Automated Data Discovery and Classification Pilot**, _National_Archives_and_Records_Administration | Information Service_
> NARA is planning to conduct a pilot to test automated data discovery and classification using AI/ML techniques. They will use a vendor's COTS solution that allows customers to search and discover full documents, rather than individual sensitive data elements. If the vendor's solution does not understand a specific document type, NARA can train the algorithm to find all documents of that type by providing a learning set of examples.

**Semantic Search for National Archives Catalog - an Artificial Intelligence (AI) / Machine Learning (ML) Pilot Program**, _National_Archives_and_Records_Administration | Information Service_
> The National Archives and Records Administration (NARA) is using semantic search, an AI/ML pilot program, to make it easier and faster for researchers and historians to find records and documents in the NARA Catalog. Semantic search understands the user's intent and provides more accurate and relevant search results, while also identifying relationships between records and documents to facilitate new insights and discoveries.

**Freedom of Information Act (FOIA) Discovery AI Pilot**, _National_Archives_and_Records_Administration | NGC FOIA Office_
> The Freedom of Information Act (FOIA) Discovery AI Pilot aims to use AI techniques to improve the response to FOIA requests. The AI system will provide a search technique based on content similarity and redact personal information and additional details based on the nature of the request and the requester.

## U.S._Office_of_Personnel_Management

**Human Resource Apprentice (HRA)**, _U.S._Office_of_Personnel_Management | HRS/FSC/ASMG & OCIO/FITBS_
> The Human Resource Apprentice (HRA) is responsible for assessing whether it is feasible, valid, and affordable to provide AI-supported assistance to HR Specialists in USA Staffing for reviewing applicants. Additionally, the Office of Personnel Management (OPM) will evaluate the prototype to ensure it does not introduce any biases or unfairness in the hiring process. The main objective for OPM is to determine if the AI solution can deliver quicker and more accurate evaluations of applicant qualifications compared to experienced HR Specialists.

**Similar Job Recommendations**, _U.S._Office_of_Personnel_Management | HRS/USAJOBS_
> USAJOBS intends to enhance job matching by utilizing natural language processing. This technology will assist users in finding relevant job opportunities that align with their interests and preferences.

**Skills matching on Open Opportunities**, _U.S._Office_of_Personnel_Management | HRS/USAJOBS_
> The website Open Opportunities utilizes a third-party vendor called Skills engine to match users with suitable opportunities. This is done by analyzing user input text and opportunity descriptions to generate personalized recommendations.

**Retirement Services (RS) Chat Bot**, _U.S._Office_of_Personnel_Management | RS/RO_
> The Retirement Services (RS) Chat Bot is a computer program that uses AI and natural language processing to understand and respond to customer questions about Survivor Benefits. It began with 13 questions but has been expanding based on user feedback.

## Department_of_Justice

**Drug Signature Program Algorithms**, _Department_of_Justice | Drug Enforcement Administration_
> The DEA's Special Testing and Research Laboratory uses AI/ML techniques to create algorithms for their Drug Signature Program. These algorithms use statistical analysis tools to automatically determine the geographical region where samples of heroin and cocaine originated, and can also identify anomalies and results with low confidence.

**Complaint Lead Value Probability**, _Department_of_Justice | Federal Bureau of Investigation_
> The Threat Intake Processing System (TIPS) database uses artificial intelligence algorithms to quickly and accurately identify, prioritize, and process actionable tips. This AI technology helps the FBI and law enforcement respond to the most urgent threats by triaging immediate threats for immediate attention and human review based on algorithm scores.

**Intelligent Records Consolidation Tool**, _Department_of_Justice | Justice Management Division_
> The Office of Records Management Policy uses an AI and NLP tool to compare records schedules and identify similar items. This tool saves time for the Records Manager by automatically suggesting schedule consolidation and providing a dashboard for easy review. Additionally, the tool's approach can be applied to other domains that need text similarity analysis.

**Privileged Material Identification**, _Department_of_Justice | Tax Division_
> The application scans documents to identify attorney/client privileged information by searching for specific keywords input by the system operator. This helps to identify and protect sensitive and confidential information.

## U.S._Environmental_Protection_Agency

**Use of random forest model to predict exposure pathways**, _U.S._Environmental_Protection_Agency_
> The study used a random forest model to predict the likelihood of chemicals being associated with different exposure pathways. The model achieved a balanced accuracy of 73-81% in identifying these pathways. By combining this information with other predictors of human intake rates, a consensus model was created that predicted relevant pathways, median intake rates, and credible intervals for a large number of chemicals.

**Records Categorization**, _U.S._Environmental_Protection_Agency_
> The records management technology team is using machine learning to predict how long records should be kept. This prediction model will be added to a records management application to assist users in applying the appropriate retention schedules to new records.

**Enforcement Targeting**, _U.S._Environmental_Protection_Agency_
> The EPA and the University of Chicago collaborated to develop a proof-of-concept that enhances the enforcement of environmental regulations through facility inspections. The use of predictive analytics led to a significant 47% improvement in identifying violations of the Resource Conservation and Recovery Act.

## Department_of_Housing_and_Urban_Development

**Consolidated Plan Pilot Analysis**, _Department_of_Housing_and_Urban_Development_
> In March 2023, PD&R initiated a pilot project to analyze HUD's Consolidated Plans. These plans, which are publicly available on HUD's website, are currently reviewed by HUD staff for compliance, but there is a lack of capacity to analyze commonalities or trends within the plans. The pilot project aims to create a database and chat-bot that will allow HUD staff to query features of the Consolidated Plans, potentially informing grantees, technical assistance, programmatic adjustments, and the utilization of advanced data science tools.

## Department_of_Education

**Aidan Chat-bot**, _Department_of_Education | Federal Student Aid_
> Aidan Chat-bot is a virtual assistant developed by FSA that utilizes natural language processing to provide answers to financial aid queries and assist users in accessing federal aid information on StudentAid.gov. Since its launch, Aidan has engaged with over 2.6 million individuals, generating more than 11 million user messages.
