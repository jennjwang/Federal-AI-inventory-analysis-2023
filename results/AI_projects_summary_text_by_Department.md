# AI projects across Departments

## Interior

**Land Use Plan Document and Data Mining and Analysis R&D**

_Interior | BLM_
> The Land Use Plan Document and Data Mining and Analysis R&D project aims to uncover patterns and conflicts in resource management planning rules by analyzing unstructured planning documents. The project's outputs help identify proposed action locations that require exclusion, restrictions, or stipulations based on the conflicts found in the planning documents.

**Improved Processing and Analysis of Test and Operating Data from Rotating Machines**

_Interior | BOR_
> This project aims to improve the analysis of DC ramp test data from rotating machines. Currently, engineering expertise is required to recognize characteristic curves from the test plots, but by using machine learning and AI tools, such as linear regression, computer software can analyze the plots and identify the curves, leading to faster and more reliable analysis.

**Improving UAS-derived photogrammetric data and analysis accuracy and confidence for high-resolution data sets using artificial intelligence and machine learning**

_Interior | BOR_
> The main idea of the response is that artificial intelligence and machine learning can be used to improve the accuracy and efficiency of analyzing high-resolution photogrammetric data obtained from unmanned aerial systems (UAS). By developing a standard reference protocol and using these technologies, it will be possible to unlock the potential information in the data and provide detailed analysis of Reclamation's assets, leading to better-informed decision making.

**Data Driven Streamflow Forecasting**

_Interior | BOR_
> The response states that Reclamation, along with partners, conducted a year-long evaluation of streamflow forecasting technologies and held a prize competition focused on 10-day streamflow forecasts. The top performing forecast product was from UpstreamTech, an AI/ML forecasting company, and several competitors from the prize competition also performed well. Reclamation plans to further evaluate the UpstreamTech forecast products and other top performers from the competition.

**Photogrammetric Data Set Crack Mapping Technology Search**

_Interior | BOR_
> The project aims to use photogrammetric products to analyze and map cracks on Reclamation facilities. This process, which has traditionally been time-consuming and required difficult access, can now be done using drones or other devices. By implementing a standardized protocol and utilizing machine learning and AI, the collected data will be used to make informed decisions about Reclamation assets.

**Seasonal/Temporary Wetland/Floodplain Delineation using Remote Sensing and Deep Learning**

_Interior | BOR_
> The goal of this study was to investigate whether advancements in machine learning, particularly deep learning with convolutional neural networks, can enhance the mapping of seasonal and temporary wetlands and floodplains using high-resolution remote sensing data. If successful, these improved mappings could be used to manage protected species and provide valuable information for decision-making in operations and planning.

**Data Driven Sub-Seasonal Forecasting of Temperature and Precipitation**

_Interior | BOR_
> Reclamation has conducted two year-long prize competitions to develop data-driven methods for predicting temperature and precipitation over a 2-6 week period in the western US. Participants in these competitions outperformed forecasts from NOAA, and Reclamation is now collaborating with Scripps Institute of Oceanography to refine and pilot the most promising methods. Improving sub-seasonal forecasts can have a significant impact on water management outcomes.

**Well Activity Report Classification**

_Interior | BSEE_
> The main idea of the response is that the research involves using self-supervised deep neural networks to identify classification systems for significant well events. This research is conducted by analyzing data from well Activity Reports.

**Sustained Casing Pressure Identification**

_Interior | BSEE_
> The response explains that sustained casing pressure (SCP) in well platforms is usually caused by gas migration through leaking cement sheaths or defects in tube connections, downhole accessories, or seals. Identifying wells with SCP quickly is crucial to preventing safety issues and accidents on the platforms.

**Level 1 Report Corrosion Level Classification**

_Interior | BSEE_
> The Level 1 surveys obtained from BSEE provide information on the condition of well platforms, including the coating and structural condition. These reports are used to assess safety concerns and determine if additional audits are needed. An automated screening system that can identify areas with excessive corrosion could significantly speed up the report processing time.

**Data–driven prospectivity modelling of sediment–hosted Zn–Pb mineral systems and their critical raw materials**

_Interior | USGS_
> The main idea of the response is that a data-driven approach using various regional data and machine learning models was used to create a prospectivity map for Clastic Dominated (CD) and Mississippi Valley Type (MVT) deposits in three countries. This approach involved loading the data into Uber's H3 cube and training Weights of Evidence and Gradient-Boosting Machine models.

**Updating Real-time Earthquake Shaking, Ground Failure, and Impact products with remote sensing and ground truth observations**

_Interior | USGS_
> The main idea of the response is that a breakthrough has been made in rapidly assessing and modeling the ground failure and loss caused by earthquakes using remote sensing and ground truth observations. The use of a machine learning framework and Bayesian causal inference allows for accurate and high-resolution estimates of multi-hazards and damage, improving prediction abilities and revealing causal dependencies. This advancement is expected to result in more accurate and actionable images, maps, and products for post-earthquake assessment.

**Coast Train**

_Interior | USGS_
> The Coast Train Coast Train dataset is a collection of orthomosaic and satellite images of coastal, estuarine, and wetland environments along with corresponding thematic label masks. The dataset includes spatial and time-series data and contains over 1.2 billion labelled pixels, representing more than 3.6 million hectares.

**Water Mission Area Regional Drought Early Warning System**

_Interior | USGS_
> The main goal of this project is to develop and test machine learning models for predicting and forecasting daily hydrologic drought in the Colorado River Basin. The project involves using gridded meteorologic forcing data and daily streamflow data to build random forest and neural network models. The project is being developed on AWS and in cooperation with CHS, and the USGS HPC systems are also being utilized.

**Spot Elevation OCR from historical topo maps**

_Interior | USGS_
> The main goal of this project is to create a database of spot elevations for summit locations in the contiguous United States. The focus is on extracting this information from historical topographic maps.

**Water Mission Area Drought Prediction Project**

_Interior | USGS_
> The Water Mission Area Drought Prediction Project aims to create a method for predicting daily hydrologic drought by using machine learning models calibrated on streamflow and meteorological data. The models will be developed for specific locations and then applied to similar ungaged basins through a "donor model" approach. The project will utilize the USGS HPC systems for model development and running.

**Estimating stream flow from images in headwaters**

_Interior | USGS_
> The main goals of this project are to use timelapse images captured by affordable cameras to estimate the amount of water flowing in small, unmonitored stream networks. They also aim to create a web-based platform that allows easy access and exploration of the images, climate data, and the model itself. Users can contribute data by uploading imagery and flow data to the database.

**Forecasting Water Temperature in the Delaware River Basin**

_Interior | USGS_
> The researchers developed a process-guided deep learning and data assimilation approach to forecast water temperature in the Delaware River Basin. Their model used real-time observations and an autoregressive technique to improve near-term forecasts, with an average root mean squared error ranging from 1.1 to 1.9°C for 1-day-ahead and 7-day-ahead forecasts.

**Economic valuation of fisheries in the Delaware River**

_Interior | USGS_
> The main goal of this study is to connect data on the flow of the Delaware River with models of trout population dynamics, changes in fish catch, and the economic benefits of recreational fishing. This will be done by developing trout population models using observational data, literature estimates, and existing models.

**Seabird and Marine Mammal Surveys Near Potential Renewable Energy Sites Offshore Central and Southern California**

_Interior | USGS_
> The Seabird Studies Team at the Western Ecological Research Center conducted aerial surveys of the ocean off central and southern California to count and classify marine birds and mammals. To process the large volume of images, they used machine learning techniques and created a labeled training dataset. They are currently reviewing the model's output and will generate maps of species distribution and abundance to inform planning for offshore wind energy development.

**Deep Learning for Automated Detection and Classification of Waterfowl, Seabirds, and other Wildlife from Digital Aerial Imagery**

_Interior | USGS_
> The project involves using AI to automate the detection and classification of wildlife from aerial imagery. The first stage is a detector that identifies wildlife, while the second stage is a classification algorithm that categorizes the wildlife. The project uses various tools and databases to develop and train the algorithms and store the annotated data and image metadata.

**Mapping benthic algae along the Buffalo National River from remotely sensed data**

_Interior | USGS_
> The main idea is that the study aims to map benthic algae along the Buffalo National River in northern Arkansas using orthophotos and multispectral images. The researchers collected field data to train a classification algorithm that can distinguish between different levels of algal density.

**Prediction of Flood Flow Metrics for Minimally Altered Catchments**

_Interior | USGS_
> The main idea of the response is that a system is being developed to predict flood flow metrics for stream reaches based on watershed characteristics and long-term meteorological data. The system will use gage data from the Delaware River Basin and the Colorado River Basin to train models that can estimate flood flow metrics in ungaged reaches. The current focus is on minimally altered catchments, but future years may include predictions for altered catchments. The models will be built using R packages on the USGS Tallgrass supercomputer.

**Multi-task deep learning for daily streamflow and water temperature**

_Interior | USGS_
> The main idea of this response is that a multi-task deep learning model was developed to predict daily average streamflow and daily average stream water temperature. The model used meteorological variables as input data and a multi-task scaling factor to control the contribution of the auxiliary variable's error. The training data consisted of streamflow and water temperature observations collected by the USGS and made available through the NWIS and CAMELS dataset. The models were developed using Python, TensorFlow, and Snakemake.

**AI system to recognize individual fish and disease**

_Interior | USGS_
> This study aims to develop an AI system that can identify individual fish and detect diseases from images. If successful, this system could replace traditional methods used for estimating fish abundance and movement, leading to cost reductions for fisheries managers. Additionally, disease detection from images could offer new ways to assess fish health status and trends.

**ML-Mondays course on applications of deep learning to image analysis**

_Interior | USGS_
> The ML-Mondays course focuses on the applications of deep learning in image analysis, particularly in image segmentation, classification, and object detection. The course utilizes Python, Keras, and Tensorflow ML libraries, providing software, data, documentation, slides, and a website for further information.

**Process-Guided Deep Learning for Dissolved Oxygen Predictions on Stream Networks**

_Interior | USGS_
> The main ideas in the response are: 1) A model has been developed to predict daily dissolved oxygen concentrations in stream locations using meteorological inputs and static catchment attributes. 2) The model is trained using dissolved oxygen concentrations collected by the USGS and made available through the National Water Information System. 3) The work is being conducted using Python and R, with deep learning models written in TensorFlow, data preparation in R, and the modeling workflow scripted via Snakemake.

**Walrus Haulout Camera Trap Image Classification**

_Interior | USGS_
> This project aims to classify walrus haulout camera trap images using codes that were previously developed for wildlife underpass camera trap image classification. The system is able to determine the probability of an image containing walruses and various human disturbances, such as boats and aircraft. Training, validation, and testing datasets with human-assigned labels are used to train and evaluate the models, which can then be used to predict classes on unlabeled images from ongoing camera monitoring efforts. The training process utilizes a convolutional neural network (CNN) approach based on TensorFlow and is performed on the USGS Tallgrass supercomputer.

**A machine learning approach to developing ground motion models from simulated ground motions**

_Interior | USGS_
> The researchers used a machine learning approach to develop a ground motion model (GMM) using simulated ground motions from the Southern California CyberShake study. They found that their synthetic-based GMM had similar results to empirically based GMMs derived from a global data set and records of a recent earthquake, suggesting that machine learning can effectively utilize synthetic data to guide future parameterization of GMMs.

**Application of machine learning to ground motion-based earthquake early warning**

_Interior | USGS_
> The main idea of the response is that machine learning is being applied to predict ground shaking during earthquakes based on initial observations from seismic stations. The initial test dataset is from an area of induced seismicity in Oklahoma, but future datasets will include seismic data from California and possibly Japan. The plan is to eventually transition from running the codes on a desktop to a cloud computing platform like AWS.

**Process-Guided Deep Learning Predictions of Lake Water Temperature**

_Interior | USGS_
> The main idea of this response is that a process-guided deep learning model has been developed to predict lake water temperatures. The model is trained using a two-stage process, with pre-training using process-based modeling outputs and then fine-tuning using lake temperature observations. The models are designed to fit observations and conserve energy, and the software used for modeling is the General Lake Model (GLM version 2) software.

**Characterization of Sub-surface drainage (tile drains) from satellite imagery**

_Interior | USGS_
> The response discusses the importance of understanding the changes in sub-surface drainage (tile drains) over time in order to analyze the effects on streamflow and water quality. The method proposed involves using satellite imagery to delineate tile drains and create an up-to-date geospatial layer. The process involves using a UNet model trained on panchromatic images and a combination of python scripting and Jupyter notebook.

**Where’s the Rock: Using Neural Networks to Improve Land Cover Classification**

_Interior | USGS_
> The main idea of the response is that the researchers are using a neural network to differentiate between exposed bare rock and soil covered areas in land cover classification. They have trained and tested the neural network using a dataset from the Sierra Nevada Mountains and are aiming to create a machine learning approach to accurately map soil vs. rock-covered areas for various applications such as landslide hazard mapping and calculating water fluxes.

**Walrus Object Detection in Drone/Satelite Imagery**

_Interior | USGS_
> The main idea of this response is that a system is being developed to detect and count individual walruses in drone imagery. The system will use convolutional neural networks trained on a supercomputer to identify and create bounding boxes around the walruses. This will support population research conducted by the Alaska Science Center.

**AI applications to mapping surface water**

_Interior | USGS_
> The research is focused on using artificial neural networks to identify surface water in different areas by training them with annotated hydrography data. The input training data includes various types of remotely sensed data and surface flow models. The researchers are utilizing open-source tools in a high-performance computing environment for their work.

**The National Landcover database**

_Interior | USGS_
> The National Landcover database (NLCD) uses artificial intelligence and machine learning to generate landcover data for all 50 states. This data is used by various users and federal agencies to estimate wildlife habitat, urban runoff, population growth, and other related factors.

**Explainable AI and interpretable machine learning**

_Interior | USGS_
> The main focus of this work is to develop expertise and resources for Explainable AI (XAI) in WMA PUMP Projects. The goal is to develop models that can predict stream temperature, discharge, dissolved oxygen, and other characteristics, and provide interpretable metrics that help understand the reasons behind the predictions and the physical processes captured by the models.

**Individual Mountain Lion ID from Camera Data**

_Interior | USGS_
> The main idea of this response is that a system is being developed to identify individual mountain lions using facial images. The system will use a convolutional neural network architecture and will help researchers mark and estimate the population of mountain lions.

**Fouling Identification Neural Network (FINN)**

_Interior | USGS_
> The Fouling Identification Neural Network (FINN) is an end-to-end system that uses supervised learning to predict and detect sensor fouling at USGS stream gages. The system operates in real-time on Amazon Web Services, providing predictions every 30 minutes based on raw data from the USGS AQUARIUS database. The system produces values indicating the likelihood of current fouling and the likelihood of fouling occurring in the next 24 hours, and these values are displayed on a Tableau dashboard connected to AWS.

**Fish and Climate Change Database (FiCli)**

_Interior | USGS_
> The Fish and Climate Change Database (FiCli) is a database that compiles peer-reviewed literature on the impact of climate change on inland fishes globally. The creators are looking to automate parts of the review process to make it more efficient in updating and maintaining the database.

**Leveraging Deep Learning to Improve Earthquake Monitoring**

_Interior | USGS_
> The USGS National Earthquake Information Center has used deep learning to enhance their earthquake monitoring system. By training AI models to analyze waveform data, they have improved automatic earthquake detection and characterization. The models utilized a training dataset that combined the NEIC's earthquake catalog with archived waveform recordings, and were developed using Python, Keras, and Tensorflow.

**Using Gradient Boosting Method and Feature Selection to Reduce Aleatory Uncertainty of Earthquake Ground-Motion Models**

_Interior | USGS_
> The researchers developed ground-motion models for peak ground acceleration (PGA) and peak ground velocity (PGV) using a gradient boosting method. They found that the optimal model for predicting PGA and PGV uses four explanatory variables: M, Rjb, VS30, and Ztor. The reduction in total variability is primarily due to the inclusion of source parameters as explanatory variables.

**Prediction of Salt Front Location in the Delaware River Estuary**

_Interior | USGS_
> The researchers are developing a machine learning model to predict the location of the salt front in the Delaware River Estuary. The model will use data on river discharge, tidal forcings, and meteorological data from various points in the estuary. The model will be compared with a process-based hydrodynamic model, COAWST, and is being built using tools from pyTorch.

**Artificial Intelligence for Environment & Sustainability (ARIES)**

_Interior | USGS_
> ARIES is an international research project that uses semantics and machine reasoning to enable AI-assisted modeling of human-natural systems. It aims to create a seamless knowledge commons of scientific artifacts that can be easily accessed and utilized by humans and machines through the linked data paradigm and artificial intelligence. The project's software, k.LAB, supports the creation and maintenance of a distributed semantic web platform for storing and connecting scientific information.

**Predicting Water Temperature Dynamics of Unmonitored Lakes With Meta‐Transfer Learning**

_Interior | USGS_
> The study proposes a method called meta-transfer learning to predict water temperature dynamics in unmonitored lakes. Different model types, including process-based models, neural networks, and process-guided neural networks, were trained on well-observed lakes and used to make predictions in unobserved lakes. The results showed that process-guided deep learning models performed better in transferring knowledge than process-based and pure machine learning approaches.

**TerrainFeatures detection and recognition**

_Interior | USGS_
> The main objective of this project is to use deep learning tools to extract and recognize terrain features. The focus is on developing techniques to detect and identify various characteristics of the terrain using advanced technology.

**Prediction of Inland Salinity in the Delaware River Basin**

_Interior | USGS_
> The main idea of the response is that a system is being developed to predict specific conductance (SC) in inland stream reaches in the Delaware River Basin (DRB). The system will use watershed characteristics, land use, and meteorological data to make predictions in locations and time periods where data is not available. The model will be built using pyTorch on the USGS Tallgrass supercomputer.

**Prediction of Regolith Thickness in the Delaware River Basin**

_Interior | USGS_
> This project aims to predict the thickness of the regolith layer in the Delaware River Basin by utilizing data collected from private well drillers. By training a Random Forest model, the researchers hope to create a data product that can be used to support groundwater and hydrologic modeling in the basin.

**Mapping river bathymetry from remotely sensed data**

_Interior | USGS_
> The researchers are using high frequency satellite images to estimate water depth in river channels. They are able to improve accuracy by averaging multiple scenes collected on the same day or within a couple of days. They have also developed a neural network regression approach for this purpose, using field measurements of water depth from five different rivers as training data. The method is implemented in MATLAB using the Deep Learning Toolbox.

**Prediction of Lake Water Temperature using Lake Attributes**

_Interior | USGS_
> The response states that a system is being developed to predict lake water temperature based on lake characteristics. The system will use lake temperature observations, meteorological data, and lake characteristics as training data to develop models using Python packages such as PyTorch on the USGS Tallgrass supercomputer.

**Waterfowl Lifehistory and Behavior Classification**

_Interior | USGS_
> The response discusses a model that accurately classifies waterfowl behavior into 8 different life history states using GPS relocations and potentially habitat data. This tool can help waterfowl researchers and managers quickly assess and notify important events in real-time, improving outcomes and reducing costs.

**Using Artificial Neural Networks to Improve Earthquake Ground-Motion Models**

_Interior | USGS_
> The response discusses the use of Artificial Neural Networks (ANNs) to enhance earthquake ground-motion models. The model utilizes data on location, magnitude, and local geological structure to estimate peak ground-motion from earthquakes. The model was built using Python, specifically Keras with TensorFlow.

**Stream physical habitat characterization in the Chesapeake Bay Watershed**

_Interior | USGS_
> The project aims to use a large dataset of rapid habitat assessment data collected in the Chesapeake Bay Watershed to predict stream habitat conditions for unmeasured stream reaches in the region. The model can generate predictions for multiple aspects of physical habitat condition and can be updated quickly with new data.

**Fluvial Fish Native Distributions for the Conterminous United States using the NHDPlusV2.1 and Boosted Regression Tree (BRT) Models**

_Interior | USGS_
> The main idea of the response is that species distribution models have been developed for 271 fluvial fish species in their native ranges in the conterminous United States. Boosted Regression Tree (BRT) models were used to predict the presence or absence of each species in stream segments based on landscape data and anthropogenic impacts. R Version 4.0.2 or newer with specific packages were used for modeling.

**Evaluating fish movement in restored coastal wetlands using imaging sonar and machine learning models**

_Interior | USGS_
> A collaboration between USGS, USFWS, and the University of Michigan is developing a machine learning model (MLM) that can analyze terabytes of hydroacoustic data on fish movement in restored coastal wetlands more efficiently. The model will identify, track, and quantify fish movement by reading sonar image files, converting them to a universal file format, and using TensorFlow-based convolutional neural networks for object detection. The model's output will provide valuable information for wetland managers and researchers in estimating fish habitat use and supporting the restoration of coastal wetland habitats.

**Prediction of Water Temperature in the Delaware River Basin**

_Interior | USGS_
> The researchers developed a machine learning model called the recurrent graph convolutional network (RGCN) to predict water temperature in the Delaware River Basin. They trained the RGCN using predictions from a process-based model that predicts stream flow and temperature called the Precipitation Runoff Modeling System with the coupled Stream Network Temperature Model (PRMS-SNTemp).

**Wildlife Underpass Camera Trap Image Classification, San Diego CA**

_Interior | USGS_
> This software system uses wildlife camera trap images to classify them into different taxonomic classes based on the species present in each image. By developing AI systems that can accurately perform this task, it can reduce the costs and increase the volume of data for analysis. The system uses a convolutional neural network approach and is trained on the USGS Tallgrass supercomputer.

**River Image SEnsing**

_Interior | USGS_
> The RISE project aims to create a camera system that can be integrated into the USGS Water Mission Area's streamgage monitoring network. This system will not only capture images and videos, but also use AI/ML modeling techniques to generate time-series data of surface water levels from still camera images.

**Integrating machine learning phase pickers into the Southern California Seismic Network earthquake catalog**

_Interior | USGS_
> The main idea of this response is that the researchers are testing a machine learning model called Generalized Phase Detection (GPD) to improve the earthquake catalog of the Southern California Seismic Network. The model uses seismic data from multiple stations to detect and pick the arrival times of P and S waves, which are then used to estimate earthquake locations, origin times, and magnitudes. The researchers are also developing a cloud-native software architecture to apply the GPD model in real-time using Amazon Web Services.

**Understanding the 2020-2021 Puerto Rico Earthquake sequence with deep learning approaches**

_Interior | USGS_
> The researchers used deep learning approaches to analyze the 2020-2021 earthquake sequence in southwestern Puerto Rico. They employed various deep learning models, such as EQTransformer, EikoNet, and HypoSVI, to detect earthquakes, determine their location, and understand the fault system and triggering mechanisms. These machine learning methods provided more data on small earthquakes and improved depth estimates, allowing for a better understanding of the active faults and physical processes involved.

**ARMI Amphibian Species ID from Acoustic Data**

_Interior | USGS_
> The USGS Amphibian and Reptile Monitoring Initiative (ARMI) aims to help reverse amphibian population declines by providing scientific information to managers. They are using acoustic monitoring of amphibian vocalizations, but the process of reviewing and identifying species vocalizations is time-consuming. To address this issue, they plan to develop convolutional neural networks (CNNs) that can classify species vocalizations in audio recordings, starting with bullfrog vocalizations as a prototype project. They will use the TensorFlow Python API and train the models on the USGS Tallgrass supercomputer.

**Process guidance for learning groundwater influence on stream temperature predictions**

_Interior | USGS_
> The response discusses a process for predicting daily average stream temperature using meteorological drivers in the Delaware River Basin. The training data used are water temperature observations collected by the UGSS. The focus is on developing a custom loss function to account for the influence of groundwater on stream temperature by considering the phase lag and amplitude dampening effect of groundwater.

**Process-guided deep learning for predicting stream temperature in out-of-bound conditions**

_Interior | USGS_
> This study uses meteorological data to predict stream temperature in the Delaware River Basin. The researchers compare the performance of two deep learning models that incorporate process guidance through pretraining on process-based modelling outputs, in order to understand their limitations in accurately predicting stream temperature under changing climate and precipitation conditions.

**Global Inland Fisheries Risk Index**

_Interior | USGS_
> The researchers used both manual and machine learning methods to analyze a large amount of literature on global inland fisheries. Their goal was to understand the impact of human-driven environmental changes on these fisheries and develop a risk index to assess the level of stressors they face.

**PRObability of Streamflow PERmanence**

_Interior | USGS_
> The PROSPER modeling framework uses sparse streamflow observation data and hydroclimatic data to predict the annual probability of streamflow permanence at different resolutions. The models were developed using various tools and resources, including FCPGTools, R, and USGS HPC resources.

## HHS

**Auto-generation Synonyms**

_HHS | AHRQ_
> The main idea is that auto-generation synonyms are being used to enhance search results by adding synonyms to search queries. These synonyms are implemented behind the scenes to improve the overall search experience.

**Relevancy Tailoring**

_HHS | AHRQ_
> The main idea of this response is that relevancy tailoring involves adjusting the ranking of search results to ensure that the most relevant results are displayed at the top of the list. This process helps users find the most useful and accurate information quickly and easily.

**Suggested Related Content**

_HHS | AHRQ_
> The suggestion is to display related searches that could offer the user additional valuable information. This feature aims to enhance the user's experience and provide them with more relevant content.

**Chatbot**

_HHS | AHRQ_
> The main idea is that a chatbot is an interactive interface that can understand and respond to queries in real time using natural language processing. It is capable of understanding plain language and providing appropriate responses.

**Auto Tagging**

_HHS | AHRQ_
> The main idea is that auto tagging is a process of suggesting content tags automatically by evaluating how existing content is tagged using machine-driven algorithms. This approach eliminates the need for manual tagging and allows for more efficient categorization of content.

**Did you mean**

_HHS | AHRQ_
> The main idea of the response is that the speaker is suggesting the implementation of spelling corrections and reformatted search queries based on Google Analytics data.

**Automated Suggestions**

_HHS | AHRQ_
> The main idea of the response is that there is a feature or tool that automatically suggests or fills in queries as they are being typed. This suggests that the system or platform has some sort of predictive or auto-fill functionality to assist users in typing their queries more quickly or accurately.

**Item Nonresponse Detection in Open-text Response Data**

_HHS | CDC | NCHS_
> The National Center for Health Statistics (NCHS) is creating a model to detect item nonresponse in open-text survey data. The model uses Natural Language Processing (NLP) and is trained to identify instances of gibberish, uncertainty, refusals, and high-risk responses, with the goal of improving survey data and questionnaire design.

**Sequential Coverage Algorithm (SCA) in Record Linkage**

_HHS | CDC | NCHS_
> The National Center for Health Statistics (NCHS) Data Linkage Program has incorporated the Sequential Coverage Algorithm (SCA) into their linkage programs. This supervised machine learning algorithm is used to create blocking groups or joining methods for large datasets, enhancing the efficiency of the blocking process.

**ICD-10 Coding of Cause of Death reported on Death Certificates (MedCoder)**

_HHS | CDC | NCHS_
> The response states that MedCoder uses ICD-10 codes to match the cause of death descriptions provided on death certificates. This includes codes for both the underlying cause and contributing causes of death.

**Chatbot – Voice**

_HHS | CMS_
> The Chatbot - Voice CMS/OSFLO is an automated phone response system designed to help the CMS Badging Help Desk. It provides general information about badging and allows help desk personnel to focus on more complex issues faced by employees and contractors.

**Reasonable Accommodation RPA Bot**

_HHS | CMS_
> The Reasonable Accommodation RPA Bot is designed to gather HR data regarding staffing changes and generate information for the Reasonable Accommodation staff to ensure that disability reasonable accommodations are properly implemented. This bot helps streamline the process of managing accommodations for employees with disabilities.

**Priority Score Model - ranks providers within the Fraud Prevention System using logistic regression based on program integrity guidelines.**

_HHS | CMS_
> The Priority Score Model is a system that uses logistic regression to rank providers within the Fraud Prevention System (FPS) based on program integrity guidelines. It takes inputs such as Medicare Claims data, Targeted Probe and Educate (TPE) Data, and Jurisdiction information to determine the ranking of providers within the FPS system.

**Data Lake/Load-Extract-Load-Transform (L-ETL)**

_HHS | CMS_
> The main idea in this response is that CMS is using a Security Data Lake to modernize its load-extract-load-transform (L-ETL) pipelines and data tooling. This will enhance Agency security by bringing together more system, telemetry, and program data in one place with a unifying governance model. While there is no ML/AI work being done currently, the goal is to build a modern data platform that will allow for machine learning model development in the future.

**Priority Score Timeliness - forecast the time needed to work on an alert produced by Fraud Prevention System (Random Forest, Decision Tree, Gradient Boost, Generalized Linear Regression)**

_HHS | CMS_
> The main idea of this response is to prioritize alerts produced by the Fraud Prevention System based on the forecasted time needed to work on them. The inputs for this prioritization include Medicare Claims data, TPE Data, and Jurisdiction information, and the output is the forecasted time needed to work on each alert using various machine learning algorithms.

**Chatbot – Text**

_HHS | CMS_
> The Chatbot (text) is designed to help the Security team by automating email responses to general physical security questions. This allows the help desk team to focus on addressing more complex issues for employees and contractors.

**Provider Education 90 Day - reviews claims for provider before and after education for statistical change in their claim submission patterns**

_HHS | CMS_
> The Provider Education 90 Day program aims to analyze the claim submission patterns of healthcare providers before and after their education. It uses Medicare Claims data, TPE data, and jurisdiction information to review and identify any statistical changes in their claim submissions. The output of the program is a comprehensive assessment of the providers' claim submission patterns.

**Predictive Intelligence - Incident Assignment for Quality Service Center (QSC).**

_HHS | CMS_
> The main idea of this response is that Predictive Intelligence (PI) is used in the Quality Service Center (QSC) to assign incidents. The solution analyzes the short description provided by the end user and assigns the ticket to the appropriate assignment group based on previously submitted incidents. The solution is re-trained every 3-6 months with incident data.

**Feedback Analysis Solution (FAS)**

_HHS | CMS_
> The Feedback Analysis Solution (FAS) is a system that uses publicly available data to review and analyze public comments and other information from stakeholders. It utilizes Natural Language Processing (NLP) and machine learning (ML) tools to aggregate and sort comments, identify duplicates, and extract topics, themes, and sentiment from the dataset.

**Rapid Authority to Operate (ATO)**

_HHS | CMS_
> The Rapid Authority to Operate (ATO) System uses natural language processing (NLP) to analyze system security plans and identify commonly used technology components in Federal Information Security Management Act (FISMA) systems. This helps the Centers for Medicare and Medicaid Services (CMS) identify similar approaches to solving control areas within the Acceptable Risk Safeguards (ARS) and streamline the generation of system security plans for new systems.

**Artificial Intelligence-based Deduplication Algoirthm for Classfication of Duplicate Reports in the FDA Adverse Event Reports (FAERS)**

_HHS | FDA_
> The response describes an artificial intelligence-based deduplication algorithm that is used to identify duplicate reports in the FDA Adverse Event Reporting System (FAERS). The algorithm processes both structured and unstructured data, extracts relevant clinical features, and uses a probabilistic record linkage approach to score pairs of reports. The algorithm aims to facilitate the identification of potential duplicate reports during case series evaluation for safety concerns.

**Opioid Data Warehouse Term Identification Novel Synthetic Opioid Detection and Evaluation Analytics**

_HHS | FDA_
> The Opioid Data Warehouse has developed a system called Term Identification and Novel Synthetic Opioid Detection and Evaluation Analytics. This system utilizes social media and forensic chemistry data to identify new references to drug products in social media text. It uses the FastText library to create vector models of known NSO-related terms, providing similarity scores and expected prevalence estimates for future data gathering efforts.

**Advanced Semantic Search and Indexing of Text for Tobacco Applications (ASSIST4Tobacco)**

_HHS | FDA_
> The main idea is that ASSIST4Tobacco is a new tool that uses semantic indexing and artificial intelligence to search tobacco authorization applications. It utilizes a language model to understand relationships between words and concepts in the text, enabling more thorough search capabilities.

**BHW Community Need Analysis Platform**

_HHS | HRSA_
> The BHW Community Need Analysis Platform is being developed to assess the healthcare needs of a population in primary care with behavioral health integration. It uses a machine learning-based automated clustering engine to analyze relevant datasets and provide dynamic assessments. The output of this tool will be used in the evaluation process for grant proposals for the Notice of Funding Opportunity (NOFO).

**Electronic Handbooks (EHBs) AI Chatbot**

_HHS | HRSA_
> The main ideas in this response are that an AI chatbot has been developed and deployed for HRSA EHBs (Electronic Handbooks) grantees. The chatbot allows grantees to communicate using natural conversational expressions and provides knowledge- and action-based responses. It is integrated with existing EHBs application UI and Salesforce for automated ticket creation, and has the ability to improve its responses over time.

**Leveraging AI/ML for classification and categorization of scientific concepts**

_HHS | NIH | NHLBI_
> The main idea is that AI and machine learning can be used to classify and categorize scientific concepts. This is done by using a text classification model and concept extraction on publications and grants abstracts, resulting in category labels and a list of concepts.

**Splunk IT System Monitoring Software**

_HHS | NIH | NIEHS_
> Splunk is an IT system monitoring software that uses machine learning to collect and analyze system logs from various IT infrastructure systems and endpoints. It helps with auditing and monitoring tasks.

**COVID-19 Pandemic Vulnerability Index Dashboard**

_HHS | NIH | NIEHS_
> The COVID-19 Pandemic Vulnerability Index Dashboard is a tool that provides risk profiles for every county in the United States. It generates PVI scorecards that summarize and visualize the overall disease risk based on the latest data, enabling users to assess the vulnerability of different areas to the COVID-19 pandemic.

**Grant Application Subject-Matter Classification Tool**

_HHS | NIH | NIEHS_
> The response explains that there is a tool being developed that uses natural language processing to classify grant applications for review assignment purposes. This tool aims to streamline the process of categorizing grant applications based on their subject matter.

**Leveraging AI for Business Process Automation**

_HHS | NIH | NIGMS_
> The National Institute of General Medical Sciences (NIGMS) has developed a method to automate the referral of grant applications using artificial intelligence (AI). They are using natural language processing (NLP) and machine learning (ML) algorithms to parse grant applications and determine the appropriate project officer candidates for grant assignment. This automation process, previously done manually, has significantly reduced the amount of time and effort required. NIGMS has collaborated with the Electronic Records Administration group to incorporate this technique into the Internal Referral Module, making it available for broader use across the National Institutes of Health (NIH).

**National Institute of General Medical Sciences (NIGMS) AI Supported Searches, Information Systems and Tools System Acronym: NIGMS ASSIST)**

_HHS | NIH | NIGMS_
> The National Institute of General Medical Sciences has developed a system called NIGMS ASSIST, which utilizes artificial intelligence and natural language processing to provide relevant data to program staff. The system includes modules for identifying investigators and matching program officers based on scientific data input. It is supported by Oracle, SQL server, and Python analytics.

**MetaMap to identity potential terms for indexing MEDLINE articles**

_HHS | NIH | NLM_
> The main idea of the response is that MetaMap is a program that allows access to the concepts in the unified medical language system (UMLS) Metathesaurus from biomedical text. It uses natural language processing (NLP) to link the text of biomedical literature to the knowledge and synonymy relationships in the Metathesaurus. The program provides a flexible architecture to explore mapping strategies and generate potential indexing terms.

**Determining selection for indexing MEDLINE articles using Neural Network Architecture with a Convolutional Neural Network (CNN)**

_HHS | NIH | NLM_
> The main idea of the response is that the researchers have developed a classifier using a Convolutional Neural Network (CNN) to automate the selection of indexed articles for MEDLINE. The classifier combines predictions from traditional machine learning algorithms and uses a sigmoid activation function to generate a probability of an article being in-scope for MEDLINE.

**Pangolin lineage classifications to support accessing and analysis of SARS-CoV-2 sequence Data.**

_HHS | NIH | NLM_
> The Pango nomenclature, known as Pango lineages, is being used globally to track the transmission and spread of SARS-CoV-2 and its variants. Researchers and public health agencies can access and analyze the sequence data using the Pango lineage classifications. The tool requires conda on a MacOS or Linux system and FASTA-formatted sequence data, and it uses a classification tree called PangoLEARN to group similar sequences for lineage assignment within NCBI Virus.

**SingleCite: Improving single citation search in PubMed**

_HHS | NIH | NLM_
> The main idea of the response is that SingleCite is an automated algorithm that improves single citation search in PubMed. It establishes a query-document mapping by using a regression function to predict the probability of a retrieved document being the target. SingleCite has shown superior performance in benchmarking experiments and is able to rescue queries that would otherwise fail.

**Best Match: New relevance search for PubMed**

_HHS | NIH | NLM_
> The main idea of this response is that PubMed has developed a new relevance search algorithm called Best Match in order to help users find the most relevant papers in the rapidly growing field of biomedical literature. The algorithm uses machine-learning technology and user feedback to improve retrieval performance and provide an improved user experience.

**National Library of Medicine NLM-Chem: towards automatic chemical indexing in PubMed articles**

_HHS | NIH | NLM_
> The National Library of Medicine has developed NLM-Chem, an automatic tool for finding chemical names in biomedical literature. This tool aims to improve the efficiency of chemical indexing by replacing manual indexing by expert indexers. The tool has been tested on evaluation datasets and will be integrated into the production MEDLINE indexing pipeline.

**Computed Author: author name disambiguation National Institutes of Health (NIH) NLM for PubMed**

_HHS | NIH | NLM_
> The National Institutes of Health (NIH) NLM has developed a machine-learning method to disambiguate author names in PubMed queries. The method uses agglomerative clustering to group papers belonging to the same authors, resulting in higher accuracy than other methods. This disambiguation method has been integrated into PubMed to improve author name searches.

**Providing MeSH Check Tag of NLM’s Medical Text Indexer (MTI) ons using Support Vector Machines (SVM)**

_HHS | NIH | NLM_
> The response discusses the use of Support Vector Machines (SVM) in providing MeSH CheckTags to the NLM Medical Text Indexer (MTI) program. These CheckTags are a small set of MeSH Descriptors that indicate Species, Sex, and Age in MEDLINE articles. SVM machine learning algorithm is used to provide confidence scores for these CheckTags.

**National Library of Medicine NLM-Gene: towards automatic gene indexing in PubMed articles**

_HHS | NIH | NLM_
> The National Library of Medicine has developed an automatic tool called NLM-Gene to assist in the manual process of gene indexing in biomedical literature. The tool uses natural language processing and deep learning methods to find gene names. It has been tested on evaluation datasets and will be integrated into the MEDLINE indexing pipeline.

**Query View Report (QVR) LIKE**

_HHS | NIH | OER_
> The QVR's LIKE feature utilizes the NIH RCDC indexing results to compare scientific terms related to a project, person, or publication. This allows for the discovery of projects, individuals, or publications that are scientifically similar.

**NIH Grants Virtual Assistant**

_HHS | NIH | OER_
> The National Institutes of Health (NIH) has developed a virtual assistant chat bot to help users find grant-related information using open educational resources (OER). This chat bot will provide assistance in navigating the vast amount of grant information available through the NIH.

**Research, Condition, and Disease Categorization (RCDC)**

_HHS | NIH | OER_
> The main idea of the response is that RCDC is an electronic tool that uses AI/NLP to categorize projects based on grant applications, R&D contracts, intramural projects, and inter-agency agreements. It does this by identifying concepts in the extracted text from the source project, person, or publication, normalizing the text, extracting concepts and matching them to the RCDC thesaurus. The system then ranks the concepts based on their frequency of occurrence and outputs the projects into their respective areas of science.

**Program Class Code (Area of Science) Referral for NIAID**

_HHS | NIH | OER_
> The response explains that the RPAB is responsible for assigning program classifications to grant applications submitted to NIAID. They currently manually assign approximately 6,000+ grant applications, but the AI project aims to automate this process by categorizing the applications into their respective Program Class Codes (PCCs).

**Internal Referral Module (IRM) NLP**

_HHS | NIH | OER_
> The Internal Referral Module (IRM) NLP module automatically refers grant applications to Program Officers based on an analysis of the applications' title, abstract, specific aims, and Public Health Relevance. This process has been highly accurate and has successfully eliminated the referral bottleneck.

**Text Analytics Portal**

_HHS | NIH | OIG_
> The text analytics portal is designed for individuals who do not have a background in analytics but need to analyze text documents. It provides a range of tools such as search, topic modeling, and entity recognition to help users quickly examine and understand text data.

**Grants Analytics Portal**

_HHS | NIH | OIG_
> The Grants Analytics Portal is a tool that utilizes AI to provide HHS OIG staff with efficient access to grants data. It allows them to quickly find relevant findings, compare data between different divisions, identify trends, and assess potential anomalies in grantees.

**Machine learning system to predict translational progress in biomedical research**

_HHS | NIH | OPA_
> The NIH Office of Portfolio Analysis has developed a machine learning system that can predict whether a scientific paper is likely to be cited by a future clinical trial or guideline. By analyzing postpublication data, the system can accurately predict a paper's eventual citation with an 84% accuracy rate. This system allows for the assessment and prediction of translational progress in biomedicine based on early reactions from the scientific community.

**Semantic analysis of scientific documents (word2vec OPA )**

_HHS | NIH | OPA_
> The NIH Office of Portfolio Analysis has created a neural network method called word2vec OPA for analyzing scientific content. This approach converts words in scientific texts into numbers and summarizes documents based on their semantic meaning. The method is customizable for different types of scientific texts, such as grants and articles.

**Person-level disambiguation for PubMed authors and NIH grant applicants**

_HHS | NIH | OPA_
> The NIH developed a disambiguation solution to accurately link researchers to their grants and publications. They used a neural network model trained on ORCID identifiers to determine if author-publication pairs refer to the same person, considering factors such as institutional affiliation, co-authorship, and article-affiliated Medical Subject Heading terms.

## Commerce

**Consolidated Screening List**

_Commerce | International Trade 
Administration (ITA)_
> The Consolidated Screening List (CSL) is a compilation of 13 export screening lists from the Departments of Commerce, State, and Treasury. It allows for a fuzzy name search, meaning that users can search for entities without knowing the exact spelling of their name, which is useful for finding names translated into English from non-Latin alphabet languages.

**B2B Matchmaking**

_Commerce | International Trade 
Administration (ITA)_
> The B2B matchmaking system uses algorithms and AI technology to analyze data about event participants and identify suitable matches based on their specific needs and available opportunities. It generates suggested B2B matches and provides a match strength scorecard as an output.

**Market Diversification Toolkit**

_Commerce | International Trade 
Administration (ITA)_
> The Market Diversification Toolkit is a tool that uses current trade patterns to identify potential new export markets for users. It applies a machine learning algorithm to compare and recommend markets that should be considered for further market research. Users can customize the tool's indicators and export the data for further analysis.

**AD/CVD Self Initiation**

_Commerce | International Trade 
Administration (ITA)_
> The AD/CVD program investigates allegations of dumping and countervailing duties. Self-initiation allows the ITA to monitor trade patterns and start investigations before harmed US entities are even aware of the harm. This proactive approach helps identify and address potential offenses more efficiently.

**Chatbot Pilot**

_Commerce | International Trade 
Administration (ITA)_
> The response mentions the implementation of a chatbot pilot embedded into trade.gov to assist clients with frequently asked questions, locating information, suggesting events and services. Clients would interact with the chatbot by asking questions or responding to prompts, and the chatbot would scan content libraries and input from staff to provide relevant answers and suggestions based on the client's persona.

**Azure Chatbot**

_Commerce | Minority Business Development 
Administration (MBDA)_
> The main idea of the response is that Azure Chatbot is being used to automate and improve user responses to potential questions on the MBDA website. The solution uses AI, machine learning, and natural language processing to provide effective chatbot responses.

**ProbSR (probability of subfreezing roads**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> ProbSR is a machine-learned algorithm that predicts the probability of roads being subfreezing on a scale of 0 to 100%.

**Coastal Change Analysis Program (C-CAP)**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The Coastal Change Analysis Program (C-CAP) has been using geographic object-based image analysis and machine learning algorithms to classify coastal land cover from high-resolution imagery. They have recently started using a convolutional neural network (CNN) approach for deriving the impervious surface component of their land cover products. Prior to this, C-CAP focused on developing moderate resolution land cover using Landsat data and employed Classification and Regression Trees for data development in 2002.

**Robotic microscopes and machine learning algorithms remotely and autonomously track lower trophic levels for improved ecosystem monitoring and assessment**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that robotic microscopes and machine learning algorithms are being used to track and monitor phytoplankton, which are important for marine food webs and can be impacted by changes in the ocean. These technologies are being deployed in various areas to assess phytoplankton communities and their changes in relation to ocean and climate variability.

**Ice seal detection and species classification in multispectral aerial imagery**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main objective of this project is to enhance the detection and classification process of ice seals in aerial imagery by reducing false positive rates and maintaining high accuracy. Additionally, the project aims to minimize the need for post-survey review, which requires extensive labor.

**The Development of ProbSevere v3 - An improved nowcasting model in support of severe weather warning operations**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main ideas of the response are that ProbSevere v3 is an improved nowcasting model that uses various data sources to predict severe weather events. The project aims to enhance the operational version of ProbSevere by incorporating additional data sets and improved machine learning techniques. The development of ProbSevere v3 has been successfully demonstrated and a proposal has been submitted for an operational update, with funding provided by GOES-R.

**First Guess Excessive Rainfall Outlook**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response describes a machine learning product called the First Guess Excessive Rainfall Outlook that provides predictions for excessive rainfall. It is developed using the ERO dataset and atmospheric variables, and it offers forecasts for Day 1, 2, and 3.

**Coral Reef Watch**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> NOAA Coral Reef Watch has been using remote sensing, modeling, and in situ data to operate a Decision Support System to help resource managers, researchers, decision makers, and stakeholders prepare for and respond to coral reef ecosystem stressors caused by climate change and warming oceans. They offer the world's only global early-warning system for coral reef ecosystem changes and provide information, early warnings, and outlooks of stressful environmental conditions at targeted reef locations worldwide. Their products primarily focus on sea surface temperature but also incorporate other variables such as light and ocean color.

**A Hybrid Statistical-Dynamical System for the Seamless Prediction of Daily Extremes and Subseasonal to Seasonal Climate Variability**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response explains the development and testing of a hybrid statistical-dynamical prediction system that can generate seamless probabilistic forecasts for daily extremes and subseasonal to seasonal temperature and precipitation. The system incorporates a Bayesian statistical method for post-processing seasonal forecasts and uses machine learning techniques. The goal is to improve the accuracy and suitability of climate variability predictions.

**VIAME: Video and Image Analysis for the Marine Environment Software Toolkit**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> VIAME is an open-source software toolkit called Video and Image Analysis for the Marine Environment. It allows users to use deep-learning algorithms to automatically annotate imagery using a graphical user interface. It is available for free to NOAA users and is supported by the NOAA Fisheries Office of Science and Technology.

**SUVI Thematic Maps**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The GOES-16 Solar Ultraviolet Imager (SUVI) is an operational solar extreme-ultraviolet imager produced by NOAA. The SUVI Level 2 Thematic Map files are produced by NOAA's National Centers for Environmental Information and are processed from Level 2 HDR composite SUVI images. These files are experimental and will be improved in future releases, and users can contact the NCEI SUVI team for assistance. The SUVI Thematic Maps product is a Level 2 data product that uses a machine learning classifier to generate a pixel-by-pixel map of important solar features from all six SUVI spectral channels.

**Using community-sourced underwater photography and image recognition software to study green sea turtle distribution and ecology in southern California**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> This project aims to study the distribution and ecology of green sea turtles in southern California, specifically in the La Jolla Cove area. It involves engaging with local photographers to collect underwater images of the turtles, which are then analyzed using facial recognition software to identify individual turtles. Through this method, researchers can gather information on population size, residency patterns, and foraging habits of the green turtles.

**Replacing unstructured WW3 in the Great Lakes with a Recurrent neural network and a boosted ensemble decision tree**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response discusses the investigation of replacing unstructured wave models in the Great Lakes with two AI models: a Recurrent Neural Network (RNN) and a boosted ensemble decision tree. These models were trained on two decades of wave observations in Lake Erie and compared to the current operational wave model.

**Fisheries Electronic Monitoring Image Library**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The Fisheries Electronic Monitoring Library (FEML) is a centralized database that stores electronic monitoring (EM) data pertaining to marine life. It serves as a repository for this information.

**The VOLcanic Cloud Analysis Toolkit (VOLCAT): An application system for detecting, tracking, characterizing, and forecasting hazardous volcanic events**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The VOLCAT is an application system that uses AI-powered satellite applications to detect, track, characterize, and forecast hazardous volcanic events, particularly volcanic ash which is a major aviation hazard. The goal of the project is to further develop the VOLCAT products and transition them to the NESDIS Common Cloud Framework to meet new International Civil Aviation Organization requirements.

**Deep learning algorithms to automate right whale photo id**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response states that deep learning algorithms have been developed to automate the identification of right whales in photos. These algorithms have been expanded to include different viewpoints and body parts, and are now being used on the Flukebook platform for both North Atlantic and southern right whales. Additionally, a paper about this system is currently under review at Mammalian Biology.

**CoralNet: Ongoing operational use, improvement, and development, of machine vision point classification**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main ideas in this response are that CoralNet is a software used for annotating benthic photo quadrats using machine vision point classification. The development of classifiers has reduced the need for human annotation and there is ongoing development and improvement of CoralNet.

**AI-based automation of acoustic detection of marine mammals**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that AI-based automation of acoustic detection of marine mammals is important for adapting mitigation measures in response to climate change. The INSTINCT software has been developed for training and deploying machine learning models for this purpose, and it has been successfully used in several analyses. However, further development and integration of AI methods require a skilled operator familiar with INSTINCT, machine learning, and the acoustic repertoire of Alaska region marine mammals.

**An Interactive Machine Learning Signals in Passive Acoustic Recordings Toolkit for Classifying Species Identity of Cetacean Echolocation**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response states that there is a project to develop automated machine learning tools for classifying the species identity of toothed whales and dolphins using acoustic recordings. The tools will be used to analyze long-term recordings from passive acoustic moored instruments in the Gulf of Mexico to study environmental processes affecting marine mammal density and distribution. The project is funded from June 2018 to May 2021.

**Steller sea lion automated count program**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that the NOAA Fisheries Alaska Fisheries Science Center's Marine Mammal Laboratory conducts annual aerial surveys to monitor the endangered western Steller sea lion population in Alaska. They currently use manual counting methods but are working with Kitware to develop automated detection and image registration pipelines to improve the process. This data is important for understanding the species and ecosystem and informing sustainable fishery management decisions.

**Developing automation to determine species and count using optical survey data in the Gulf of Mexico**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main ideas of the response are: 1) The project aims to develop an image library of landed catch from optical survey data in the Gulf of Mexico, 2) The project plans to use machine learning and deep learning techniques to automatically identify and count species from underwater imagery, and 3) The project also aims to develop algorithms that can process imagery in near real time and transfer the information to a central database.

**NN Radiation**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that there is a development of fast and accurate neural network longwave (LW) and shortwave (SW) radiations for the Global Forecast System (GFS) and Global Ensemble Forecast System (GEFS). The success and stability of this approach have been demonstrated in previous versions of GFS, and it will now be extended to the current versions of GFS and GEFS.

**Drought outlooks by using ML techniques**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that machine learning techniques, including simple neural networks and deep learning techniques, are used with NCEP models to predict precipitation and temperature over the contiguous United States (CONUS) for the first five weeks. These techniques are employed to provide drought outlooks.

**EcoCast: A dynamic ocean management tool to reduce bycatch and support sustainable fisheries**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The EcoCast is an operational tool that uses boosted regression trees to model the distribution of swordfish and bycatch species in the California Current. It is designed to reduce bycatch and support sustainable fisheries by providing dynamic ocean management.

**Automated detection of hazardous low clouds in support of safe and efficient transportation**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> This project focuses on the automated detection of hazardous low clouds to ensure safe and efficient transportation. It involves maintaining and sustaining the operational GOES-R fog/low stratus products, which are derived from satellite imagery and machine learning. These products are regularly used by the NWS Aviation Weather Center and Weather Forecast Offices.

**Picky**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response discusses the use of CNN (Convolutional Neural Network) to identify objects of a specific size in side scan imagery. By presenting users with a probability, the automation of contact picking in the field becomes possible. Side scan imagery, being a one-channel intensity image, is suitable for basic CNN techniques.

**Steller sea lion brand sighting**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that researchers are trying to detect and identify branded Steller sea lions from remote camera images in the western Aleutian Islands. The purpose of this is to make the photo processing more efficient and reduce the amount of effort needed to review the images.

**Passive acoustic analysis using ML in Cook Inlet, AK**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> Passive acoustic analysis using machine learning is being used to detect and classify the signals emitted by beluga whales in Cook Inlet, AK. The results of this analysis are being used to understand the seasonal distribution, habitat use, and impact of human disturbance on these whales. The project also aims to expand its analysis to other cetacean species and anthropogenic noise.

**Using k-means clustering to identify spatially and temporally consistent wave systems**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response discusses the use of k-means clustering to identify consistent wave systems in terms of spatial and temporal aspects. This postprocessing technique has been evaluated and implemented into operations by NWS marine forecasters nationwide as of February 3, 2021.

**FathomNet**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> FathomNet is a platform that offers training data for machine learning algorithms to analyze visual data. They have used interns and college class curriculums to annotate and localize imagery from NOAA videos, which helps in training their own algorithms.

**First Guess Excessive Rainfall Outlook**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The given response describes a machine learning product called First Guess Excessive Rainfall Outlook. This product is designed to provide a first estimate of excessive rainfall outlook based on atmospheric variables learned from the ERO (Excessive Rainfall Outlook). It specifically focuses on predicting excessive rainfall for the Day 4-7 time frame.

**Fast tracking the use of VIAME for automated identification of reef fish**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that the use of VIAME for automated identification of reef fish is being fast-tracked. Image libraries are being compiled to create automated detection and classification models for the SEAMAP Reef Fish Video survey. The models are performing well, and automated analysis will be incorporated into the video reads in the spring as part of a supervised annotation process.

**BANTER, a machine learning acoustic event classifier**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that BANTER is a machine learning acoustic event classifier. It uses a hierarchical random forest approach and is supervised in nature.

**ANN to improve CFS T and P outlooks**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The researchers propose using Artificial Neural Networks (ANN) to enhance the accuracy of Week 3-4 precipitation and temperature forecasts in the Climate Forecast System (CFS). By incorporating ANN into the forecasting model, they aim to improve the outlooks for these weather variables.

**Edge AI survey payload development**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea of the response is that there is continued development and support for a multispectral aerial imaging payload that can run detection models in real-time. The goal is to reduce the amount of data and processing time, in order to expedite the analysis and population assessment for arctic mammals.

**ENSO Outlooks using observed/analyzed fields**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The response mentions the use of an LSTM model to forecast ONI values in the tropical Pacific up to one year in advance. There is also a plan to improve these forecasts by adding a CNN layer that utilizes reforecast data.

**NN training software for the new generation of NCEP models**

_Commerce | National Oceanic and 
Atmospheric Administration 
(NOAA)_
> The main idea in this response is the development of NN (Neural Network) training software for the new generation of NCEP (National Centers for Environmental Prediction) models. The goal is to optimize the NCEP EMC (Environmental Modeling Center) Training and Validation System to effectively handle the high spatial resolution model data generated by these new operational models.

**WAWENETS**

_Commerce | National Telecommunications and 
Information Administration (NTIA)_
> The WAWENETS algorithm is used to estimate the speech quality and intelligibility of telecommunications recordings. It takes a digital file as input and produces a single number indicating the quality or intelligibility of the speech.

**Data Science: Clutter**

_Commerce | National Telecommunications and 
Information Administration (NTIA)_
> The Institute for Telecommunication Sciences (ITS) is using AI, specifically a convolutional neural network (CNN), to automatically identify and classify clutter obstructed radio frequency propagation paths. Clutter refers to vegetation, buildings, and other structures that cause radio signal loss. The CNN is trained using lidar data and radio frequency propagation measurements, and can predict clutter classification labels for new radio path lidar data.

**Enriched Citation**

_Commerce | United States Patent and Trade 
Office (USPTO)_
> The response describes an Enriched Citation Data dissemination system that can identify references or prior art cited in patent application office actions. The system can extract information from unstructured office actions and provide it through a structured API, including bibliographic information, cited claims, and relevant sections relied upon by the examiner.

**Inventor Search Assistant (iSAT)**

_Commerce | United States Patent and Trade 
Office (USPTO)_
> The Inventor Search Assistant (iSAT) Service is designed to assist inventors in conducting a novelty search by providing them with relevant documents, figures, and classification codes based on a short description of their invention. The system generates a user-selectable list of recommended resources to help inventors get started with their research.

**AI retrieval for TM design coding and Image search**

_Commerce | United States Patent and Trade 
Office (USPTO)_
> The response states that an AI retrieval system is being used for trademark design coding and image search. The purpose of this system is to help examiners identify similar trademark images, suggest the correct assignment of design codes, and determine the potential acceptability of goods and services. The system is expected to use both incoming and registered trademark images to output design codes and related images.

**AI retrieval for patent search**

_Commerce | United States Patent and Trade 
Office (USPTO)_
> The main idea of the response is that there is an AI system being developed to enhance patent search tools for examiners. This system can analyze published and unpublished applications and suggest additional areas to search for relevant documents, allowing users to sort the recommendations based on similarity to specific concepts.

**AI use for CPC classification**

_Commerce | United States Patent and Trade 
Office (USPTO)_
> The given response discusses the use of AI in a CPC classification system. This system helps in categorizing patent applications based on the cooperative patent classification scheme, assigning work, and recommending symbols for AI search. Additionally, it mentions the use of a back-office processing system that takes incoming patent applications as input and provides classification symbols as output.

## Energy

**Objective-Driven Data Reduction for Scientific Workflows**

_Energy | Brookhaven National Laboratory_
> This project focuses on developing theories and algorithms for reducing scientific data in workflows. The reduction is driven by specific objectives and involves various models, including data-driven AI models.

**Protocol Analytics to enable Forensics of Industrial Control Systems**

_Energy | Idaho National Laboratory_
> The research aims to develop methods and technologies that will allow existing cybersecurity tools to defend industrial control systems (ICS) networks and enable cybersecurity analysts to detect compromise before any harm can be done. The focus is on analyzing captured communication signals to determine the protocol being used, and machine learning will be employed to identify unknown protocols. The findings will be used to create a prototype device.

**Data-driven failure diagnosis and prognosis of solid-state ceramic membrane reactor under harsh conditions using deep learning technology with internal voltage sensors**

_Energy | Idaho National Laboratory_
> This research aims to use deep learning technology and internal voltage sensors to diagnose and predict failure in solid-state ceramic membrane reactors under harsh conditions. By collecting data on current and impedance during operation, artificial intelligence will be utilized to analyze the data and anticipate reactor failure.

**Evaluating thermal properties of advanced materials**

_Energy | Idaho National Laboratory_
> The response states that the standard technique for measuring thermal diffusivity, laser flash, is improved by modifying the experimental setup and using a machine learning tool. This tool, which includes a finite element model and algorithms, allows for the analysis of thermal properties of a material from a single laser flash measurement.

**Artificial Intelligence Enhanced Advanced Post Irradiation Examination**

_Energy | Idaho National Laboratory_
> The project aims to use artificial intelligence technology to expedite the development of nuclear fuel by studying the post irradiation examination of uranium-10wt.% zirconium (UZr) metallic fuel. The project will analyze microstructural image and thermal conductivity data from UZr, create a dataset for the microstructural patterns of irradiated UZr, and train machine learning and deep learning models to understand the relationships between various factors in fuel properties.

**Machine Learning Interatomic Potentials for Radiation Damage and Physical Properties in Model Fluorite Systems**

_Energy | Idaho National Laboratory_
> This project aims to use machine learning interatomic potentials to investigate the impact of radiation damage on the physical properties of calcium fluoride and uranium dioxide. The developed potentials will be validated through electron irradiation experiments and thermal conductivity measurements, providing a high-throughput method for developing and qualifying new nuclear fuels.

**Automated Type and Data Structure Resolution**

_Energy | Idaho National Laboratory_
> The research focused on automating the identification and labeling of type and structure data, making it usable in other tools and research areas. Initially, heuristic methods were used, but later a machine learning approach was adopted to scale the process.

**Advanced Machine Learning-based Fifth Generation Network Attack Detection System**

_Energy | Idaho National Laboratory_
> The main goal of the project is to develop a machine learning-based system that can detect attacks in the fifth generation (5G) cellular network. This system aims to enhance security for mission-critical applications such as automated vehicles, drones, connected health, and emergency response operations that rely on the 5G network.

**Artificial Intelligence Based Process Control and Optimization for Advanced Manufacturing**

_Energy | Idaho National Laboratory_
> This project aims to develop artificial intelligence (AI) based control algorithms to intelligently control and optimize advanced manufacturing processes. By using deep reinforcement learning and physics-informed reduced order models (ROMs), the algorithms will be able to make adaptive processing decisions in a simulation environment, reducing the need for a trial and error approach.

**Accelerating deployment of nuclear fuels through reduced-order thermo- physical property models and machine learning**

_Energy | Idaho National Laboratory_
> This project aims to develop a tool that combines reduced-order models, machine learning algorithms, fuel performance methods, and thermal property characterization equipment to accelerate the discovery, development, and deployment of nuclear fuels. The tool will focus on describing thermal conductivity, specific heat, thermal expansion, and self-diffusion coefficients as they relate to temperature and irradiation.

**Passive Strain Measurements for Experiments in Radiation Environments**

_Energy | Idaho National Laboratory_
> This project aims to develop passive instrumentation to measure permanent strains caused by radiation and extract important parameters using modeling, simulation, and machine learning algorithms. The researchers plan to conduct an irradiation experiment using anisotropic materials to understand the directional deformation caused by neutron radiation. The results will be used to predict the material's response in future applications as a probe material.

**Accelerating and Improving the Reliability of Low Failure Probability Computations to Support the Efficient Safety Evaluation and Deployment of Advanced Reactor Technologies**

_Energy | Idaho National Laboratory_
> The project aims to use artificial intelligence and Monte Carlo algorithms to improve the efficiency and reliability of computational simulations for estimating low failure probabilities in advanced reactor technologies. By reducing the number of finite element evaluations, the project will enable the nuclear engineering community to conduct probabilistic failure analyses and uncertainty quantification studies more efficiently in the design and optimization of these technologies.

**Nuclear-Renewable-Storage Digital Twin: Enhancing Design, Dispatch, and Cyber Response of Integrated Energy Systems**

_Energy | Idaho National Laboratory_
> This project aims to develop a simulation framework that combines high-fidelity physics models with grid monitoring data to make real-time decisions for integrated energy systems (IES) operation. The framework will use learning-based algorithms to detect and respond to component contingencies caused by extreme events like cyber-attacks or extreme weather, in order to mitigate their impacts.

**Advances in Nuclear Fuel Cycle Nonproliferation, Safeguards, and Security Using an Integrated Data Science Approach**

_Energy | Idaho National Laboratory_
> This research aims to develop a digital twin of a centrifugal contactor system in the nuclear fuel cycle. The digital twin will receive data from sensors, simulate the chemical separations component, and use machine learning to analyze the data for anomalies, failures, and trends. The research will also involve the use of advanced artificial intelligence and data analysis techniques guided by a team of nuclear safeguards experts.

**Secure Millimeter Wave Spectrum Sharing with Autonomous Beam Scheduling**

_Energy | Idaho National Laboratory_
> The approach described in the response utilizes millimeter wave beam directionality and autonomous beam scheduling to support secure spectrum sharing for 5G and ensure optimal performance for base stations. Measurements and predictive analytics are used to develop autonomous beam scheduling algorithms, which will benefit mission critical communications, emergency response operations, and secure communication for critical infrastructure without the need for expensive licensed bands.

**Deep Reinforcement Learning and Decision Analytics for Integrated Energy Systems**

_Energy | Idaho National Laboratory_
> The project aims to develop a deep reinforcement learning approach that can effectively manage distributed or tightly coupled multi-agent systems in energy systems. This approach will utilize deep neural networks for system representation, modeling, and learning, allowing for complex optimization of nonlinear systems over various timescales.

**Tailoring the Properties of Multiphase Materials Through the Use of Correlative Microscopy and Machine Learning**

_Energy | Idaho National Laboratory_
> This research combines machine learning techniques with microscopy to identify and correlate microstructural features in a multiphase alloy that has high strength and fracture toughness. The goal is to develop a machine learning tool that can be used to identify key microstructural features and correlate them with mechanical properties in different alloys.

**Infrastructure eXpression**

_Energy | Idaho National Laboratory_
> The project developed a framework and process to translate industrial control system features into a machine-readable format for use with automated cyber tools. The research also examined current and evolving standards for usability with diverse grid architectures and aims to prioritize future research steps for automated threat response and improved cyber incident consequence models. Additionally, the project aims to enhance national capabilities for sharing actionable threat intelligence at machine speed.

**Automated Infrastructure & Dependency Detection via Satellite Imagery and Dependency Profiles**

_Energy | Idaho National Laboratory_
> The use of computer vision techniques for processing satellite imagery has improved in recent years, making it possible to analyze critical infrastructure and interdependency data more effectively. By combining advanced computer vision techniques with a functional taxonomic approach to critical infrastructure and unique geo-spatial and dependency datasets, researchers can achieve innovative and state-of-the-art image processing results that enhance national critical infrastructure security and defense capabilities.

**Interdependent Infrastructure Systems Resilience Analysis for Enhanced Microreactor Power Grid Penetration**

_Energy | Idaho National Laboratory_
> The main idea of this response is that the project aims to develop machine learning methodologies to assess the resilience of integrated energy systems and their vulnerabilities to threats. This analysis will help understand the impact of microreactors and distributed energy resources on the reliability and resiliency of energy systems.

**Support Vector Analysis for Computational Risk Assessment, Decision-Making, and Vulnerability Discovery in Complex Systems**

_Energy | Idaho National Laboratory_
> This project aimed to improve probabilistic risk assessment by using a support vector machine and PRA software to automatically detect vulnerabilities in system design and identify previously unknown issues. This approach eliminates the need for training data that would typically only be available after system failures occur, reducing human error and costs.

**Scalable Framework of Hybrid Modeling with Anticipatory Control Strategy for Autonomous Operation of Modular and Microreactors**

_Energy | Idaho National Laboratory_
> The main idea of this research is to develop and validate scalable models that can make predictions and decisions faster than real-time. The focus is on achieving autonomous operation of microreactors through a hybrid modeling approach that combines physics-based and artificial intelligence techniques. Additionally, a distributed anticipatory control strategy will be developed to analyze the risk of cascading failures when deploying emerging reactors in a full feeder microgrid.

**Adaptive Fingerprinting of Control System Devices through Generative Adversarial Networks**

_Energy | Idaho National Laboratory_
> This research project aims to use generative adversarial networks to automate the training of electromagnetic-based anomaly detection systems for legacy industrial control systems devices and Industrial Internet of Things. By doing so, it would reduce the manual labor and operational costs associated with protecting legacy control systems from intrusion.

**Resilient Attack Interceptor for Intelligent Devices**

_Energy | Idaho National Laboratory_
> The Resilient Attack Interceptor for Intelligent Devices approach aims to develop external monitoring methods to safeguard industrial internet of things devices. This will be done by analyzing naturally occurring physical aspects and detecting any abnormal functionality that may indicate a potential attack.

**Unattended Operation through Digital Twin Innovations**

_Energy | Idaho National Laboratory_
> The main idea of the response is that the team believes that artificial intelligence can be used to predict events by integrating data from test bed sensors and physics-based models. They also aim to develop a framework for future digital twins by integrating software, AI, and sensor data, and to train AI models on determining important attributes for intelligent autonomous control and cybersecurity in digital twins.

**Accelerated Nuclear Materials and Fuel Qualification by Adopting a First to Failure Approach**

_Energy | Idaho National Laboratory_
> This response highlights the use of physics-based multi-scale modeling and AI techniques to speed up nuclear materials research and qualification. By applying AI to combinatorial-based materials research, the focus can be narrowed down to a smaller number of candidates with the desired properties.

**Automated Malware Analysis Via Dynamic Sandboxes**

_Energy | Idaho National Laboratory_
> The main idea of this project is to develop a framework for automated malware analysis using dynamic sandboxes. The framework aims to provide capabilities for analyzing industrial control system malware, outputting results in a machine-readable format, and sharing threat information. The ultimate goal is to enable further analysis through machine learning and facilitate timely and automated analysis of malware samples.

**Promoting Optimal Sparse Sensing and Sparse Learning for Nuclear Digital Twins**

_Energy | Idaho National Laboratory_
> The main ideas are that the project aims to address the efficient use of limited experimental data for training and demonstrating nuclear digital twins (NDTs). This involves developing sparse data reconstruction methods and using NDT models to determine the sensor requirements for demonstration experiments. The project also highlights the importance of leveraging sparse sensing and sparse learning for stronger prediction, diagnostics, and prognostics capabilities.

**Microstructurally-driven Framework for Optimization of In-core Materials**

_Energy | Idaho National Laboratory_
> The research aims to develop a methodology that combines machine learning models, ion irradiation and creep testing techniques, and advanced characterization to understand the relationship between alloy composition, thermomechanical processing, microstructure, and swelling and creep behavior. The project aims to provide a rapid development process for in-core materials and important information on alloy design for optimized swelling and creep behavior for advanced reactor development.

**Development of a multi-sensor data science system used for signature development on solvent extraction processes conducted within Beartooth facility**

_Energy | Idaho National Laboratory_
> The main idea is that a multi-sensor data science system will be developed to analyze solvent extraction processes. The system will use non-traditional and traditional measurement sources, along with machine learning techniques, to discover signals and improve target metals recovery, identify process faults, account for special nuclear material, and make real-time decisions.

**Secure and Resilient Machine Learning System for Detecting Fifth Generation (5G) Attacks including Zero-Day Attacks**

_Energy | Idaho National Laboratory_
> This project aims to develop a machine learning system that can effectively detect 5G attacks, including zero-day attacks. The system will have high classification speed and accuracy, and it will use field programmable gate array based deep autoencoders to address the vulnerability to zero-day attacks.

**Red Teaming Artificial Intelligence**

_Energy | Idaho National Laboratory_
> This research aims to improve the assessment of the security of machine learning and artificial intelligence systems through reverse engineering, exploitation, risk assessment, and vulnerability remediation. By addressing gaps in understanding and developing risk evaluation metrics, this research can contribute to better cybersecurity practices in the field of AI.

**Smart Contingency Analysis Neural Network for in-depth Power Grid Vulnerability Analyses**

_Energy | Idaho National Laboratory_
> The response discusses the limitations of typical contingency analysis for power utilities and introduces a new approach using a machine learning framework and resilience-chaos plots. This new method reduces computational expense and can accurately discover n-2 contingencies by 50%.

**Spectral Observation Convolutional Neural Network**

_Energy | Idaho National Laboratory_
> The main idea of the response is that the project developed a method to analyze radiation spectra using deep learning, which can overcome the weaknesses of existing spectroscopic techniques and enhance difficult measurements. The method was tested and operated on the International Space Station's supercomputer, demonstrating its performance in low-wattage computing situations and hazardous radiological environments.

**Signal Decomposition for Intrusion Detection in Reliability Assessment in Cyber Resilience**

_Energy | Idaho National Laboratory_
> The main idea of this response is that the project aims to research and implement machine learning and artificial intelligence algorithms for signal decomposition in order to detect false data injection in physical processes. The project also aims to develop an advanced library that combines these algorithms with high-fidelity model comparisons for improved detection of malicious tampering.

**Combinatorial Evaluation of Physical Feature Engineering and Deep Temporal Modeling for Synchrophasor Data at Scale**

_Energy | Office of Electricity_
> The response discusses the application of big data, artificial intelligence, and machine learning to analyze phasor measurement unit (PMU) data. The goal is to enhance grid operation and management by identifying and improving existing knowledge, as well as discovering new insights and tools.

**Big Data Synchrophasor Monitoring and Analytics for Resiliency Tracking (BDSMART)**

_Energy | Office of Electricity_
> The main idea of BDSMART is to utilize big data, AI, and machine learning technology to analyze PMU data in order to enhance grid operation and management. The goal is to gain new insights and tools for improving existing knowledge and resiliency tracking.

**Open-Source High-Fidelity Aggregate Composite Load Models of Emerging Load Behaviors for Large-Scale Analysis (GMLC 0064)**

_Energy | Office of Electricity_
> The main ideas of the response are that machine learning methods are used to estimate load composition data and motor protection profiles for different climate regions in the Western US, and a deep learning algorithm is applied to calibrate the parameters of the WECC composite load model to match responses with a detailed feeder model.

**A Robust Event Diagnostic Platform: Integrating Tensor Analytics and Machine Learning Into Real-time Grid Monitoring**

_Energy | Office of Electricity_
> The main idea of the response is to explore the use of big data, artificial intelligence, and machine learning technology and tools on phasor measurement unit data to improve grid operation and management. The goal is to identify existing knowledge, discover new insights, and develop better tools for monitoring the grid in real-time.

**Discovery of Signatures, Anomalies, and Precursors in Synchrophasor Data with Matrix Profile and Deep Recurrent Neural Networks**

_Energy | Office of Electricity_
> The response discusses the use of big data, artificial intelligence, and machine learning technology to analyze phasor measurement unit data. The goal is to identify and improve existing knowledge, as well as discover new insights and tools for better grid operation and management.

**Robust Learning of Dynamic Interactions for Enhancing Power System Resilience**

_Energy | Office of Electricity_
> The main idea of the response is to explore the use of big data, AI, and machine learning on PMU data to enhance power system resilience. The goal is to improve existing knowledge, discover new insights, and develop tools for better grid operation and management.

**MindSynchro**

_Energy | Office of Electricity_
> The main idea is to explore the use of big data, artificial intelligence, and machine learning technology on PMU data in order to improve existing knowledge and discover new insights for better grid operation and management.

**The Grid Resilience and Intelligence Platform (GRIP)**

_Energy | Office of Electricity_
> The Grid Resilience and Intelligence Platform (GRIP) utilizes artificial intelligence (AI) to create metrics that measure the effects of extreme weather events on the power grid. By analyzing utility data and utilizing physical models, GRIP can predict the potential grid impacts caused by major storms.

**PMU-Based Data Analytics Using Digital Twin Phasor Analytics Software**

_Energy | Office of Electricity_
> The response discusses the use of big data, AI, and machine learning technology in analyzing PMU data to enhance grid operation and management. The goal is to improve existing knowledge, uncover new insights, and develop tools for better grid management.

**Machine Learning Guided Operational Intelligence**

_Energy | Office of Electricity_
> This response suggests using big data, artificial intelligence, and machine learning to analyze phasor measurement unit data in order to enhance grid operation and management. The goal is to identify and enhance existing knowledge while also discovering new insights and tools for improved efficiency.

## Homeland_Security

**Vessel Detection**

_Homeland_Security | Customs and Border Protection_
> The main idea of the response is that integrated technologies and analytics can enhance maritime detection and the sensor network. Machine-assisted and AI-enhanced detection and tracking can improve the detection of illicit vessels in areas with high volumes of legitimate trade and recreational water vessel traffic. The use of AI algorithms and sharing of detections of Items of Interest (IoI) can help in tracking and maintaining these IoIs across multiple sensors.

**Use of technology to identify proof of life**

_Homeland_Security | Customs and Border Protection_
> The main idea is that the use of technology, specifically Artificial Intelligence and mobile device cameras, is being utilized in the CBP One app to detect proof of life or "Liveness Detection." This technology is important for reducing fraudulent activity and ensuring that the submitted data is from the actual person in front of the camera.

**AI Curated Synthetic Data**

_Homeland_Security | Customs and Border Protection_
> AI Curated Synthetic Data is a technology that generates artificial data for computer vision, specifically in the field of anomaly detection in complex environments. It creates synthetic X-ray scan images and virtual 3D assets to improve the development of algorithms for detecting narcotics and contraband in vehicles and cargo.

**Integrated Digital Environment**

_Homeland_Security | Customs and Border Protection_
> The Integrated Digital Environment is a system that uses AI/ML models to analyze end user activity data and identify opportunities for improving workflows and application usage. It provides analytics, dashboarding, and workflow suggestions to support analysts and allows for customization and automation of agency applications for greater connectivity and security.

**Autonomous Maritime Awareness**

_Homeland_Security | Customs and Border Protection_
> The Autonomous Maritime Awareness system utilizes surveillance towers, ocean data solutions, unmanned autonomous surface vehicles (ASV), and AI to detect, identify, and track objects of interest in a maritime environment. The system uses low-cost surveillance towers equipped with radars and cameras, as well as ruggedized ASVs powered by wind, solar, or onboard engines. Both systems employ AI/ML to autonomously detect and track objects, and can send alerts to monitoring agencies for potential interdictions or intelligence collection.

**Geospatial imagery utilizing annotation**

_Homeland_Security | Customs and Border Protection_
> The response describes the use of geospatial imagery and annotation through Synthetic Aperture Radar (SAR) satellites, which can capture images of any location on Earth regardless of cloud cover or time of day. This technology utilizes AI, including machine vision and object detection, to identify airframes, military vehicles, and marine vessels, as well as detect changes for disaster response missions.

**Data and Entity Resolution**

_Homeland_Security | Customs and Border Protection_
> The response explains that Data and Entity Resolution is a process that uses Machine Learning to merge and identify connections between different data sources. It also mentions that there is a user-friendly tool available for non-technical users to train the models continuously.

**Automated Item of Interest Detection - ICAD**

_Homeland_Security | Customs and Border Protection_
> The main idea is that the ICAD system uses automated software called Matroid to analyze and annotate photographs taken by field imaging equipment. The software determines if the images contain human subjects and can also recognize objects, people, and events in any image or video stream. The goal is to expand the software's capabilities to include vehicles and subjects with long-arm rifles, while excluding items of little interest such as animals.

**AI for Autonomous Situational Awareness**

_Homeland_Security | Customs and Border Protection_
> The AI for autonomous situational awareness system aims to detect and track illicit cross-border traffic using IoT sensor kits in remote locations. It utilizes motion image/video systems enhanced with Artificial Intelligence to detect vehicles and determine their direction, with high-resolution cameras capturing images upon motion sensor triggers. The captured images are then processed by AI models to classify objects and provide imagery for re-identification, creating a low-cost and low-power system for covert detection and situational awareness.

**RVSS Legacy Overhauled System Project (INVNT)**

_Homeland_Security | Customs and Border Protection_
> The RVSS Legacy Overhauled System Project (INVNT) has developed software called Video Computer Aided Detection (VCAD) that allows users to create and share vision detectors. These detectors are trained computer vision models that can recognize objects, people, and events in images or videos. The software provides various reports and alert notifications to help users identify important events and trends, and it also offers an API and language-specific clients for integration with other CBP applications.

**Entity Resolution**

_Homeland_Security | Customs and Border Protection_
> The use of third-party global trade data enhances investigations into targeted entities by providing additional information and network analysis on trade flows and associated risks. AI/ML models are utilized to manage the data and perform various functions to support the software's knowledge graph and user interface.

**Autonomous Aerostat**

_Homeland_Security | Customs and Border Protection_
> The Autonomous Aerostat Aerostat capability utilizes three tethers and advanced weather sensors to autonomously launch and recover aerostats based on weather conditions. This eliminates the need for on-site staff and allows for autonomous operation, saving time and manpower.

**Autonomous Surveillance Towers (Anduril)**

_Homeland_Security | Customs and Border Protection_
> The Autonomous Surveillance Towers by Anduril are equipped with artificial intelligence technology that allows them to detect, identify, and track items of interest without the need for a dedicated operator. These towers are easily deployable and can be relocated in less than a day by a small team. The system is solar-powered with battery backup and has a hybrid command and control capability accessible through various devices. The tower scans constantly, with the radar detecting and recognizing movement, and the camera slewing autonomously to the object of interest. The system alerts the user and tracks the object autonomously, providing near real-time photos that can be accessed through the User Interface.

**Cyber Threat Intelligence Feed Correlation**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The Cyber Threat Intelligence Feed Correlation utilizes AI to quickly correlate information from multiple feeds, enhancing the quality of externally shared information. The AI algorithm can learn and improve its efficiency in performing the task, and customized algorithms can be developed for continuous monitoring of threat actors' tactics, techniques, and procedures (TTPs).

**Security Information and Event Management (SIEM) Alerting Models**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The main idea is that security analysts in Security Operations Centers receive large amounts of log data and use manual and automatic methods to detect and correlate potential cyber attacks. However, these methods are not always comprehensive, so analysts use automated tooling with mathematically and probabilistically based models to refine alerts and detect anomalies in a timely manner.

**Malware Reverse Engineering**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The main idea of the response is that malware reverse engineering is crucial for CISA's cyber defense mission. Threat Focused Reverse Engineering (TFRE) is a method that combines advanced engineering, formal methods, and deep learning techniques to improve cyber threat intelligence. The use of scalable, automated tools is necessary to disrupt sophisticated adversaries' malware development lifecycle, and new techniques are needed to target adversaries, support analysts, and create advanced tools for end users. TFRE includes tool hardening, enhanced computational abilities, understanding of deployment environments, and other important capabilities.

**AI Security and Robustness**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The AI Security and Robustness Frameworks, processes, and testing tools have been developed to govern the acquisition, development, deployment, and maintenance of AI technologies. These tools, which use Machine Learning and Natural Language Processing, help ensure the trustworthy, robust, and secure operation of AI systems by speeding up data processing and enhancing the assessment of AI technology within the agency.

**Critical Infrastructure Anomaly Alerting**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The Cyber Sentry program monitors critical infrastructure networks and uses advanced anomaly detection and machine learning to analyze cyber-physical data from IT and OT networks, including ICS/SCADA. The Critical Infrastructure Anomaly Alerting model assists threat hunting analysts by providing AI-assisted processing of this information.

**Cyber Incident Reporting**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> Cyber incident handling specialists use advanced automation tools that leverage Machine Learning and Natural Language Processing to process and filter data received from various threat intelligence and cyber incident channels. These tools assist in increasing the accuracy and relevance of data presented to human analysts and decision-makers, as well as aggregating information in reports for further analysis.

**AIS Scoring and Feedback**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The main idea of the response is that AIS Automated Scoring & Feedback (AS&F) uses descriptive analytics to classify indicators of compromise (IOCs) based on organizational-centric intelligence data. It determines if an indicator is present in a known-good list, if there are sightings of the indicator, if it has been verified by an analyst, and if there are other reports about the indicator. AIS participants can use the opinion and confidence values to filter and prioritize actions and investigations.

**Operational Activities Explorer**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The Operations Center in CISA utilizes an artificial intelligence-powered dashboard to assist duty officers and analysts in understanding ongoing operational activities. This AI system combines real-time event data, historical cybersecurity information, and previous response activity to suggest appropriate actions and engagement strategies with other government entities and critical infrastructure owners based on potential impacts to national critical functions.

**Cyber Vulnerability Reporting**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The use of advanced automation tools, such as Machine Learning and Natural Language Processing, is crucial for vulnerability analysts to process and aggregate data from various reporting channels. These tools improve the accuracy and relevance of the filtered information, making it easier for human analysts and decision-makers to analyze and make informed decisions. Additionally, Machine Learning techniques aid in aggregating information from databases like KEV and CVE for further analysis and presentation.

**Advanced Analytic Enabled Forensic Investigation**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The Cybersecurity and Infrastructure Security Agency (CISA) uses advanced analytics and forensic specialists to investigate cyber events in government departments and agencies, as well as other partners. These specialists use artificial intelligence tools to analyze data and detect anomalies and potential threats more efficiently.

**Advanced Network Anomaly Alerting**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The main ideas in the response are that Advanced Network Anomaly Alerting Threat hunting and Security Operations Center (SOC) analysts receive large amounts of data from the National Cybersecurity Protection System's (NCPS) Einstein sensors. They use a combination of manual and automated techniques to detect network attacks and refine their alerts, leveraging mathematical and probabilistic models to ensure timely and accurate detection of anomalies.

**Automated Indicator Sharing (AIS) Automated PII Detection**

_Homeland_Security | Cybersecurity and Infrastructure Security Agency_
> The main idea of the response is that the Automated PII Detection and Human Review Process uses analytics and natural language processing to automatically detect potential personally identifiable information (PII) in submissions. If PII is flagged, it is reviewed by analysts who can confirm or deny its identification and redact it if necessary. The system learns from feedback and helps ensure compliance with privacy requirements.

**Text Analytics for Survey Responses (TASR)**

_Homeland_Security | HQ_
> The DHS OCHCO is using Text Analytics for Survey Responses (TASR) to analyze and extract important topics/themes from open-ended survey responses. These results are then used by DHS Leadership to improve employee satisfaction and meet their basic needs.

**RelativityOne**

_Homeland_Security | HQ Enforcement, Intelligence and Analysis, Science and Technology_
> RelativityOne is a document review platform that helps streamline the process of reviewing and producing large volumes of documents in various legal contexts, such as litigation and FOIA requests. It aims to improve efficiency and effectiveness in these tasks.

**Barcode Scanner**

_Homeland_Security | Immigration and Customs Enforcement_
> The Barcode Scanner is a project developed by the DHS HSI Innovation Lab / RAVEn to scan and populate information from MRZ and PDF417 barcodes into text fields on the RAVEn GO's Encounter Card. This project supports ICE's mission to enforce and investigate violations of U.S. laws and allows for the analysis of trends and criminal patterns. For more information, refer to the DHS/ICE/PIA-055 Privacy Impact Assessment.

**Mobile Device Analytics**

_Homeland_Security | Immigration and Customs Enforcement_
> Mobile Device Analytics (MDA) has been developed to help investigators analyze large amounts of data extracted from mobile devices. The goal of MDA is to improve the efficiency of agents in identifying relevant evidence and criminal networks. The project also involves the development of machine learning for object detection in photos and videos.

**Email Analytics**

_Homeland_Security | Immigration and Customs Enforcement_
> The Email Analytics application allows users to analyze email data obtained legally. It uses AI to classify spam messages, extract information about names, organizations, and locations, and includes machine translation capabilities.

**Normalization Services**

_Homeland_Security | Immigration and Customs Enforcement_
> HSI uses Artificial Intelligence to normalize and correct data entry errors, identify purposeful misidentification, and connect information across datasets. This streamlines the process of investigations and reduces the number of resource hours needed. The services provided include normalizing addresses, inferring ID types, categorizing name parts, and validating phone numbers. These services are part of the Repository for Analytics in a Virtualized Environment (RAVEn), which supports ICE's mission to enforce and investigate violations of U.S. laws.

**Facial Recognition Service**

_Homeland_Security | Immigration and Customs Enforcement_
> The Facial Recognition Service is utilized by HSI agents and analysts to identify individuals involved in various crimes such as child exploitation, human rights violations, and war crimes. It is a part of the DHS HSI Innovation Lab project known as RAVEn, which helps support ICE's mission to enforce and investigate violations of U.S. laws. RAVEn also enables the use of analytical tools to analyze trends and identify criminal patterns as needed by HSI.

**Machine Translation (Previously Language Translator)**

_Homeland_Security | Immigration and Customs Enforcement_
> Systran's machine translation service, previously known as Language Translator, offers translation for over 100 language combinations. The Innovation Lab currently has licenses for translating various languages, including Chinese, Spanish, Arabic, and more, into English. Systran can translate plain text, word documents, and PDFs, and provides a web-based UI and API endpoint for easy access.

**Sentiment Analysis - Surveys**

_Homeland_Security | United States Citizenship and Immigration Services_
> The Sentiment Analysis - Surveys system uses statistical analysis and Natural Language Processing (NLP) modeling software to assign sentiments to survey results. This helps survey administrators analyze both quantitative and qualitative data from employee satisfaction surveys. The system is currently available on demand.

**Predicted to Naturalize**

_Homeland_Security | United States Citizenship and Immigration Services_
> The Predicted to Naturalize model predicts when Legal Permanent Residents would be eligible to naturalize and provides their current address. The model can be utilized to send correspondence to USCIS customers regarding their resident status and notify others about potential USCIS benefits.

**Identity Match Option (IMO) Process with DBIS Data Marts**

_Homeland_Security | United States Citizenship and Immigration Services_
> The Identity Match Option (IMO) is a process used by USCIS to determine a single identity for each applicant or beneficiary across multiple systems. This process combines data from various systems to create comprehensive immigration histories for individuals, allowing for analysis, fraud detection, and resolution of data quality issues.

**Person-Centric Identity Services A-Number Management Model**

_Homeland_Security | United States Citizenship and Immigration Services_
> The main idea of the response is that Person-Centric Identity Services aims to be the trusted source of biographical and biometric information for immigration history and status. The A-Number Management model is used to train and evaluate the datasets, ensuring accurate matching of records and maintaining high confidence and trust in the PCIS entity resolution. Machine learning is employed to handle fuzzy matches and variations in data quality.

**I-539 approval prediction**

_Homeland_Security | United States Citizenship and Immigration Services_
> The project aims to create a machine learning model that can predict the approval of I-539 applications through eProcessing. This has the potential to enhance the efficiency of the approval process and improve the overall experience for applicants.

**Topic Modeling on Request For Evidence data sets**

_Homeland_Security | United States Citizenship and Immigration Services_
> The main idea of the response is that topic modeling on Request For Evidence (RFE) data sets involves creating models that can identify lists of topics and documents related to each topic. Topic modeling is a useful tool for organizing, understanding, searching, and summarizing text data, as it helps in discovering hidden themes in a collection and classifying documents based on these themes.

**Person-Centric Identity Services Deduplication Model**

_Homeland_Security | United States Citizenship and Immigration Services_
> The Person-Centric Identity Services (PCIS) aims to be a trusted source of biographical and biometric information for individuals' immigration history. The de-duplication model helps merge data from different systems to create a complete picture of a person, using machine learning to identify and match records with high accuracy. This allows PCIS to gather an individual's immigration history efficiently without the need for extensive research across multiple systems.

**I-485 Family Matching**

_Homeland_Security | United States Citizenship and Immigration Services_
> The I-485 Family Matching system uses artificial intelligence to match family members to their underlying I-485 petitions, which can be difficult due to limited available data. It aims to improve the reliability of matching by leveraging AI and can also help identify and group I-485s filed by family members and gather related forms for faster and more accurate processing.

## VA

**Disentangling dementia patterns using artificial intelligence on brain imaging and electrophysiological data**

_VA_
> The main idea of this response is that researchers are using artificial intelligence to analyze brain imaging and electrophysiological data in order to identify patterns of dementia. They are also investigating the potential of these imaging modalities as biomarkers for different types of dementia and epilepsy disorders, with the help of retrospective chart reviews conducted by the VA.

**Reinforcement learning evaluation of treatment policies for patients with hepatitis C virus**

_VA_
> A machine learning model is being used to evaluate treatment policies for patients with hepatitis C virus. The model is focused on predicting disease progression among veterans with this virus.

**Predicting corticosteroid free endoscopic remission with Vedolizumab in ulcerative colitis**

_VA_
> The study aimed to predict the likelihood of achieving corticosteroid-free remission with Vedolizumab in ulcerative colitis patients. The researchers used random forest modeling on a group of 594 patients and found that baseline data or data through week 6 of therapy could be used to construct predictive models.

**Prediction of health outcomes, including suicide death, opioid overdose, and decompensated outcomes of chronic diseases.**

_VA_
> This response states that a tool can predict health outcomes such as suicide death, opioid overdose, and decompensated outcomes of chronic diseases by using electronic health records as inputs. The tool can analyze both structured and unstructured data to generate deep phenotypes and predictions of these outcomes.

**Nediser reports QA**

_VA_
> Nediser is an AI system designed to assist radiologists in analyzing X-ray properties. It can perform various tasks such as selecting normal templates, detecting hardware, evaluating patella alignment and leg length, and measuring Cobb angles.

**Gait signatures in patients with peripheral artery disease**

_VA_
> Machine learning is being used to enhance the treatment of functional issues in patients with peripheral artery disease (PAD). By analyzing previously collected biomechanics data, researchers are able to identify gait signatures specific to PAD patients and assess the effectiveness of using limb acceleration measurements to model important biomechanics measures from PAD data.

**VA-DoE Suicide Exemplar Project**

_VA_
> The VA-DoE Suicide Exemplar project is using artificial intelligence to enhance the VA's capacity to identify veterans who are at risk of suicide. This project involves three different initiatives that all involve partnerships with the Department of Energy.

**SoKat Suicidial Ideation Detection Engine**

_VA_
> The SoKat Suicide Ideation Engine (SSIE) is a tool that uses natural language processing (NLP) to better detect suicidal thoughts among veterans. It focuses on analyzing survey data collected by the Office of Mental Health Veteran Crisis Line support team.

**AI Cure**

_VA_
> AICURE is a mobile application designed to track and ensure adherence to prescribed medication in clinical or pharmaceutical drug studies. It uses artificial intelligence to monitor and analyze patient behavior and provide accurate data to sponsors.

**Predicting hospitalization and corticosteroid use as a surrogate for IBD flares**

_VA_
> The study focuses on predicting hospitalization and corticosteroid use as indicators of irritable bowel disease (IBD) flare-ups. The researchers used data from over 20,000 patients from the Veterans Health Administration and employed random forest models to analyze longitudinal lab data and other factors to make these predictions.

**VA /IRB approved research study for finding colon polyps**

_VA_
> The main idea of the response is that there is an approved research study that uses a randomized trial and artificial intelligence to find colon polyps. This study has been approved by the VA and IRB.

**Automated eye movement analysis and diagnostic prediction of neurological disease**

_VA_
> Automated eye movement analysis using artificial intelligence can help improve the accuracy and quality of diagnostic predictions for neurological diseases such as traumatic brain injury, Parkinson's, and stroke. This technology uses previously collected data to screen for markers of these diseases.

**Assessing lung function in health and disease**

_VA_
> This response suggests that artificial intelligence can be used by health professionals to assess lung function in both healthy individuals and those with diseases. It mentions that the AI can help identify predictors of normal and abnormal lung function as well as sleep parameters.

**Use of machine learning to predict surgery in Crohn’s disease**

_VA_
> The study used machine learning to analyze patient data from 2001 to 2015 to predict surgery in Crohn's disease patients. The analysis focused on patient demographics, medication use, and laboratory values to model future surgical outcomes within one year.

**Seizure detection from EEG and video**

_VA_
> Machine learning algorithms are being used to automatically detect seizures without the need for human intervention. These algorithms analyze EEG and video data from a VHA epilepsy monitoring unit to identify and classify seizure activity.

**Provider directory data accuracy and system of record alignment**

_VA_
> The main ideas in the response are that AI can be used to improve the accuracy of provider directory data and align it with the system of record. AI can also be used for intelligent identity resolution, linking, clinical decision support, discrepancy detection, and resolution. Additionally, AI adapters can be used for inference, and AI has long-term storage capabilities for machine learning and business intelligence applications.

**Acute kidney injury (AKI)**

_VA_
> This project aims to detect acute kidney injury (AKI) using artificial intelligence in collaboration with Google DeepMind. The AI system can identify AKI, ranging from minor loss of kidney function to complete kidney failure, and can also detect AKI caused by other illnesses.

**Machine learning (ML) for enhanced diagnostic error detection and ML classification of protein electrophoresis text**

_VA_
> Researchers are using machine learning techniques to improve diagnostic error detection and classification of protein electrophoresis text. They are collecting true/false positive annotations through chart review and constructing a vector embedding of patient records to retrieve similar unlabeled records. The aim is to use machine learning as a filter to improve specificity after rules-based retrieval, focusing on high-value structured data relevant to stroke risk and potentially prior text notes.

**Predictor profiles of OUD and overdose**

_VA_
> This response states that machine learning prediction models are being used to evaluate risk factors for opioid use disorder (OUD) and overdose in Post-9/11 Veterans. Various modeling approaches are being employed to develop predictor profiles for OUD and overdose.

**Extraction of family medical history from patient records**

_VA_
> This pilot project aims to extract family medical history data from patient records of African American Veterans aged 45-50. The goal is to identify individuals who are at risk of prostate cancer but have not undergone screening for this disease.

**Digital command center**

_VA_
> The Digital Command Center is a system that aims to gather all data within a medical center and use predictive and prescriptive analytics to help leaders improve the performance of the hospital. By consolidating information and applying advanced analytics, the system enables better optimization and decision-making.

**GI Genius (Medtronic)**

_VA_
> The Medtronic GI Genius is a technology that uses artificial intelligence to assist in the detection of colon polyps. It is designed to improve the accuracy and effectiveness of colonoscopy procedures by providing real-time analysis and highlighting potential polyps for further examination.

**Social determinants of health extractor**

_VA_
> The response discusses the use of an AI called social determinants of health extractor to identify social determinants of health (SDOH) information from clinical notes. These extracted variables can then be used in health-related analysis to determine if SDOH can contribute to disease risks or healthcare inequality.

**Prediction of Veterans' Suicidal Ideation following Transition from Military Service**

_VA_
> Machine learning is being used to predict veterans' suicidal thoughts after they leave the military. The prediction is based on data collected through a web-based survey that tracks veterans' experiences within the first three years of leaving military service.

**CuraPatient**

_VA_
> CuraPatient is a remote tool powered by AI that enables patients to effectively manage their health conditions without needing to visit a healthcare provider. The tool provides features such as health tracking, program enrollment, insurance management, and appointment scheduling.

**Behavidence**

_VA_
> The main idea is that Behavidence is a mental health tracking app specifically designed for veterans. The app analyzes the phone usage of veterans and compares it to a digital phenotype of individuals who have confirmed mental health conditions.

**Computer-aided detection and classification of colorectal polyps**

_VA_
> This study focuses on using artificial intelligence models to enhance the clinical management of colorectal polyps. The models analyze video frames from colonoscopy videos to identify the presence of polyps and predict their likelihood of being cancerous.

**Automatic speech transcription engines to aid scoring neuropsychological tests.**

_VA_
> Automatic speech transcription engines are being used to analyze the cognitive decline of older VA patients by transcribing their digitally recorded speech responses using artificial intelligence-based technology. This eliminates the need for manual transcription of patient speech, making it easier to score neuropsychological tests.

**Interpretation/triage of eye images**

_VA_
> Artificial intelligence is being used to interpret eye images and assist in triaging eye patients in telehealth. By analyzing retina photos, it can assess health risks and improve the diagnosis of conditions like glaucoma, macular degeneration, and diabetic retinopathy.

**Prediction of biologic response to thiopurines**

_VA_
> Artificial intelligence is being used to predict how veterans with irritable bowel disease will respond to thiopurines, using data from the Computerized Patient Record System (CPRS) and Corporate Data Warehouse (CDW). This technology aims to improve the treatment outcomes for these individuals.

**Medication Safety (MedSafe) Clinical Decision Support (CDS)**

_VA_
> The Medication Safety (MedSafe) Clinical Decision Support (CDS) system utilizes VA electronic clinical data to analyze the current clinical management of diabetes, hypertension, and chronic kidney disease. It then provides evidence-based recommendations to primary care providers based on patient-specific information such as comorbidities, laboratory test results, medications, and history of adverse drug events. The system uses knowledge bases and an automated execution engine to generate these patient-specific recommendations.

**Machine learning tools to predict outcomes of hospitalized VA patients**

_VA_
> The response states that there is an IRB-approved study that intends to use machine learning tools to predict the health outcomes of Veterans Affairs (VA) patients. The study will specifically focus on predicting Alzheimer's disease, rehospitalization, and Chlostridioides difficile infection.

**Machine learning models to predict disease progression among veterans with hepatitis C virus**

_VA_
> A machine learning model is being utilized to forecast the progression of hepatitis C virus in veterans. This model aims to predict the course of the disease and potentially help in developing effective treatment plans.

**Using machine learning to predict perfusionists’ critical decision-making during cardiac surgery**

_VA_
> The study uses machine learning to create models that can predict the decision-making process of perfusionists during critical situations in cardiac surgery. The findings could potentially lead to the development of computerized tools that can enhance patient safety and improve surgical outcomes in the operating room.

**Predicting hepatocellular carcinoma in patients with hepatitis C**

_VA_
> This study aimed to determine if deep learning recurrent neural network models using raw longitudinal data from electronic health records were more effective than conventional regression models in predicting the risk of hepatocellular carcinoma in patients with hepatitis C-related cirrhosis. The study analyzed data from patients in the national Veterans Health Administration with at least 3 years of follow-up after their cirrhosis diagnosis.

**Artificial intelligence coach in cardiac surgery**

_VA_
> The artificial intelligence coach in cardiac surgery helps identify misalignment in team members' mental models during complex healthcare tasks, particularly in safety-critical domains like aviation and healthcare. By identifying model misalignment, the coach can assist in improving teamwork and enhancing human cognition in the operating room.

**Artificial Intelligence physical therapy app**

_VA_
> The main idea of the response is that there is an artificial intelligence physical therapy app that serves as a support tool for physical therapy. The app is capable of analyzing data from wearable sensors and providing feedback to the physical therapist in a format that can be easily understood.

**Precision medicine PTSD and suicidality diagnostic and predictive tool**

_VA_
> The response discusses a model for precision medicine that can be used as a diagnostic and predictive tool for PTSD and suicidality. This model aims to interpret real-time inputs to anticipate episodes of PTSD and suicidality, provide early and accurate diagnosis, and gain a deeper understanding of the effects of stress on the onset of PTSD, particularly in extreme situations.

**PredictMod**

_VA_
> PredictMod is an artificial intelligence system that analyzes the gut microbiome to determine if it can make predictions about diabetes. It utilizes AI technology to identify possible correlations between the gut microbiome and the development of diabetes.

**Screening for esophageal adenocarcinoma**

_VA_
> The main idea is that national VHA administrative data is being used to develop tools that can predict the risk of esophageal adenocarcinoma using electronic health records.

## Agriculture

**Detection of aquatic weeds**

_Agriculture | USDA | APHIS_
> The main focus of this response is on the detection and identification of aquatic weeds. It emphasizes the need to identify and locate these weeds in aquatic environments.

**Predictive modeling of invasive pest species and category at the port of entry using machine learning algorithms**

_Agriculture | USDA | APHIS_
> The main idea of the response is that machine learning algorithms are being used to predict and improve the detection of invasive and quarantine significant pests at the port of entry. These algorithms use inspection data to develop predictive models for identifying these pests.

**Approximate string or fuzzy matching, used to automate matching similar, but not identical, text in administrative documents**

_Agriculture | USDA | APHIS_
> The response discusses the concept of approximate string or fuzzy matching, which is used to automate the process of matching similar but not identical text in administrative documents. This matching algorithm helps classify similar strings into a single category, reducing the need for manual error-checking and information duplication.

**Detection of pre-symptomatic HLB infected citrus**

_Agriculture | USDA | APHIS_
> The main idea is to detect pre-symptomatic citrus trees infected with Huanglongbing (HLB) by identifying pixels in multispectral and thermal imagery that have the signature of HLB infection. This method allows for the detection of HLB before visible symptoms appear on the citrus trees.

**High throughput phenotyping in citrus orchards**

_Agriculture | USDA | APHIS_
> The main idea of the response is that high throughput phenotyping techniques can be used in citrus orchards to monitor the health of the orchard. This involves locating, counting, and categorizing citrus trees in order to assess their overall well-being.

**Artificial Intelligence for correlative statistical analysis**

_Agriculture | USDA | APHIS_
> The main idea of the response is that artificial intelligence (AI) techniques are used for correlative statistical analysis, specifically in modeling predictive relationships between variables. The response mentions various modeling approaches, such as random forest, artificial neural networks, k-nearest neighbor clustering, and support vector machines, that are commonly used for statistical prediction.

**Automated Detection & Mapping of Host Plants from Ground Level Imagery**

_Agriculture | USDA | APHIS_
> The main idea is the development of a system that can automatically detect and map host plants using ground-level imagery. The focus is on generating maps specifically for target trees using streetview images.

**Standardization of cut flower business names for message set data**

_Agriculture | USDA | APHIS_
> The response discusses the process of standardizing cut flower business names for message set data using natural language processing. It explains that data is cleaned by removing punctuation to make it easier to match and then calculates cosine similarity to find similar terms, ultimately providing the results.

**Training machine learning models to automatically read file attachments and save information into a more convenient Excel format.**

_Agriculture | USDA | APHIS_
> The main idea is that machine learning models can be trained to automatically read file attachments and extract information, such as form fields, to save it in an Excel format. This helps program managers who receive numerous PDF email attachments daily, as manually opening and copying the information is time-consuming. The use of artificial intelligence in document processing and information extraction enables automation in this task.

**NAL Automated indexing**

_Agriculture | USDA | ARS_
> The NAL Automated indexing Cogito software utilizes artificial intelligence to automatically annotate around 500,000 peer-reviewed journal articles each year. It specifically uses the National Ag Library Thesaurus concept space to annotate metadata in the Library's bibliographic citation database, including AGRICOLA, PubAg, and Ag Data Commons.

**ARS Project Mapping**

_Agriculture | USDA | ARS_
> The ARS Project Mapping NLP allows national program leaders to analyze and cluster research project plans, providing them with an interactive dashboard to identify synergies and patterns within and across different ARS research program portfolios. This tool aims to enhance collaboration and coordination among different research programs.

**4% Repair Dashboard**

_Agriculture | USDA | ARS_
> The 4% Repair Dashboard is a model that analyzes descriptions of expenses related to repairs and maintenance. It uses keywords in context to classify expenses as either "repair" or "not repair."

**Democratizing Data**

_Agriculture | USDA | ERS_
> The main idea of the response is that the project aims to utilize AI tools, machine learning, and natural language processing to analyze the usage of publicly-funded data and evidence in serving science and society. The goal is to democratize access to this information and make it more widely available.

**Westat**

_Agriculture | USDA | ERS_
> Westat held a competition to discover efficient ways of connecting USDA nutrition information with a large dataset of 750K food items. Participants used various AI techniques like NLP, random forest, and semantic matching to achieve this task.

**Retailer Receipt Analysis**

_Agriculture | USDA | FNS_
> The Retailer Receipt Analysis is a Proof of Concept that uses artificial intelligence to automate the manual process of reviewing FNS receipts and invoices. It aims to save staff time, ensure accuracy, and detect difficult patterns. The goal is to develop a review system that has an automated workflow, learns from analyst feedback, and can detect and visualize fraud patterns on retailer invoices and receipts.

**Nutrition Education & Local Access Dashboard**

_Agriculture | USDA | FNS_
> The Nutrition Education & Local Access Dashboard aims to provide a visual representation of FNS nutrition support at the county level, including nutrition education and local food access. The dashboard includes a clustering script that groups states based on various characteristics, enabling users to identify similar states for potential partnerships.

**Land Change Analysis Tool (LCAT)**

_Agriculture | USDA | FPAC_
> The Land Change Analysis Tool (LCAT) uses a random forest machine learning classifier to create detailed land cover maps from aerial and satellite imagery. They generate training data using a web application and process the data using a large docker cluster. The results are made publicly available through an image service, and they have already mapped a significant amount of land and generated a large number of training samples.

**OCIO/CDO Council Comment Analysis Tool**

_Agriculture | USDA | Federal CDO Council_
> The Comment Analysis pilot has demonstrated that a tool utilizing Natural Language Processing (NLP) can assist in the regulatory comment analysis process. The tools developed help reviewers identify comment topics and themes, as well as group similar comments together. These tools provide efficiency and cost savings for government agencies by streamlining the comment processing and reducing duplicated efforts.

**RMRS Raster Utility**

_Agriculture | USDA | Forest Service_
> The RMRS Raster Utility is a .NET library that helps with data acquisition, raster sampling, and statistical and spatial modeling. It aims to streamline raster analysis by reducing processing time and storage space, and it also incorporates machine learning techniques.

**Ecosystem Management Decision Support System (EMDS)**

_Agriculture | USDA | Forest Service_
> The Ecosystem Management Decision Support System (EMDS) is a spatial decision support system that operates within ArcGIS and QGIS. It allows users to create applications tailored to their specific needs using various AI engines, including logic processing, multi-criteria decision analysis, Bayesian networks, and Prolog-based decision trees.

**Wildland Urban Interface - Mapping Wildfire Loss**

_Agriculture | USDA | Forest Service_
> This study aims to explore the application of machine learning and object-based image classification techniques to identify buildings, building loss, and defensible space in wildland-urban interface areas before and after a wildfire. It serves as a proof-of-concept for using these methods to map wildfire damage and assess the effectiveness of defensible space measures.

**Landscape Change Monitoring System (LCMS)**

_Agriculture | USDA | Forest Service_
> The Landscape Change Monitoring System (LCMS) is a data system created by the USDA Forest Service that uses remote sensing to map and monitor changes in vegetation canopy cover, land cover, and land use. It uses a supervised classification process to analyze temporal change classifications and training data to identify areas of vegetation gain, loss, and changes in land cover and use.

**Forest Health Detection Monitoring**

_Agriculture | USDA | Forest Service_
> The main idea of the response is that machine learning models are being used to map and monitor forest mortality and defoliation across the United States. These models use training data from various sources and can process the output into vector polygons.

**CLT Knowledge Database**

_Agriculture | USDA | Forest Service_
> The CLT knowledge database is an information system that uses data aggregator bots to search the internet for relevant cross-laminated timber (CLT) information. It has cataloged over 3,600 publications on various aspects of CLT and aims to foster collaboration among stakeholders and support the increasing use of mass timber, benefiting forest health.

**Geospatial and Remote Sensing Training Courses**

_Agriculture | USDA | Forest Service_
> The response states that there are various courses available that focus on teaching software and scripting for machine learning in geospatial and remote sensing. These courses cover topics like change detection, using software packages like eCognition and Google Earth Engine, and utilizing Collect Earth Online.

**TreeMap 2016**

_Agriculture | USDA | Forest Service_
> TreeMap 2016 is a model that matches forest plot data to a grid, providing a detailed representation of the forests in the United States. It is being used in various sectors for projects such as fuel treatment planning and estimating carbon resources. The model utilizes a machine-learning algorithm and incorporates various predictor variables such as forest cover, height, topography, and disturbance history.

**List Frame Deadwood Identification**

_Agriculture | USDA | NASS_
> The deadwood model uses boosted regression trees with various inputs to create a propensity score indicating the likelihood of a farm operation being out of business. This model is used in conjunction with expert knowledge to develop a systematic process for identifying and removing deadwood.

**Cropland Data Layer**

_Agriculture | USDA | NASS_
> The Cropland Data Layer (CDL) uses a machine learning algorithm to classify the type of crop or activity in each 30 square meter pixel on the ground using satellite-based sensors. The algorithm is trained on USDA's Farm Services Agency data and other sources to ensure accuracy. The CDL has been produced since 2008 and is highly accurate for commodities like corn and soybeans.

**Census of Agricuilture Response Propensity Scores**

_Agriculture | USDA | NASS_
> The response propensity scores for the Census of Agriculture (COA) are calculated using random forest models that analyze historical data, control data, and other survey data. These scores aid in identifying the most effective methods for data collection.

**Climate Change Classification NLP**

_Agriculture | USDA | NIFA_
> The response describes a model that uses natural language processing techniques to classify NIFA funded projects as either climate change related or not. The model uses text fields such as the project's title, non-technical summary, objectives, and keywords as input features, and it assigns a dummy variable classification to determine if a project is climate change related or not.

**Operational water supply forecasting for western US rivers**

_Agriculture | USDA | NRCS_
> The USDA Natural Resources Conservation Service (NRCS) National Water and Climate Center has developed a next-generation prototype for generating operational water supply forecasts (WSFs) called the multi-model machine-learning metasystem (M4). This system integrates AI and other data-science technologies to improve the accuracy of spring-summer river flow volume forecasts. Through hindcasting and live operational testing, M4 has shown improved accuracy and the ability to provide geophysical explanations for its results, making it a valuable tool for water management in the western US.

**Ecological Site Descriptions (machine learning)**

_Agriculture | USDA | NRCS_
> The main idea of the response is that machine learning was used to analyze a large amount of data, including over 20 million records of soils data and 20,000 text documents of ecological state and transition information, in order to create ecological site descriptions.

**Digital Imagery (no-change) for NRI program**

_Agriculture | USDA | NRCS_
> The main idea of the response is that neural networks and AI technologies are being used to identify and detect no-changes in digital imagery for the NRI program. This suggests that advanced technologies are being utilized to analyze and monitor national resources.

**Conservation Effects Assessment Project**

_Agriculture | USDA | NRCS_
> The Conservation Effects Assessment Project aims to determine the conservation benefits at the field level. It achieves this by utilizing farmer survey data, APEX modeling results, and environmental data to predict these benefits.

**Artificial Intelligence SPAM Mitigation Project**

_Agriculture | USDA | OASCR_
> The main idea of the response is that an Artificial Intelligence (AI) solution, which combines Robotic Process Automation with AI/ML models, is being used to address the issue of spam and marketing emails that appear in civil rights complaints email channels. The solution aims to automatically classify and remove these unwanted emails, as they make up a significant portion of incoming OASCR emails.

**Acquisition Approval Request Compliance Tool**

_Agriculture | USDA | OCIO_
> The USDA has developed a natural language processing model to analyze text in procurement descriptions and determine the likelihood of an award being IT-related and requiring an Acquisition Approval Request (AAR). The model calculates the probability of IT-relatedness for procurements without an AAR number in the Integrated Acquisition System (IAS).

**Intelligent Ticket Routing**

_Agriculture | USDA | OCIO_
> The main idea of the response is that there is a system called Intelligent Ticket Routing that automatically routes BMC Remedy tickets to the appropriate work groups. This system utilizes multiple technologies and tools such as Python, JupyterHub, scikit-learn, GitLab, Flask, Gunicorn, Nginx, and ERMS.

**Predictive Maintenance Impacts**

_Agriculture | USDA | OCIO_
> The main idea of this response is that predictive maintenance can have an impact on infrastructure items. It mentions several tools and technologies that are used in the process, such as einblick, mysql, python, linux, and tableau.

**Video Surveillance System**

_Agriculture | USDA | OSSP_
> The Video Surveillance System (VSS) is a comprehensive system that includes various components such as cameras, network switches, and routers. It is designed to collect, manage, and present video from multiple sources in a clear and concise manner. The system allows for the integration of different video subsystems and provides advanced features for tracking targets and selecting cameras.

## State

**Supply Chain Fraud and Risk Models**

_State | A_
> A/LM plans to develop AI/ML models to detect potential fraud or malfeasance in their Integrated Logistics Management System (ILMS). These models will focus on key supply chain functions like Asset Management, Procure-to-Pay, and Fleet Management to enhance existing risk analytics.

**Product Service Code Automation ML Model**

_State | A_
> The company developed a machine learning model called Product Service Code Automation ML Model A/LM. This model is designed to scan unstructured procurement data entered by users, such as requisition titles and line descriptions, and automatically detect the types of commodities and services being purchased. The purpose of this model is to improve the categorization of procurement items.

**Within Grade Increase Automation**

_State | A_
> The response explains the process of using a Natural Language Processing (NLP) model and Intelligent Character Recognition (ICR) to extract values from the JF-62 form for within grade increase payroll actions. After that, Robotic Process Automation (RPA) is used to validate the data, create a formatted file, and proceed with approval and processing.

**Tailored Integration Logistics Management System (ILMS) Automated User Support Bot**

_State | A_
> ILMS has created an automated support desk assistant using ServiceNow Virtual Agent in order to make interactions with the support desk easier for customers and to reduce the workload of support desk agents by resolving simple issues. This system aims to streamline support desk operations and minimize costs.

**Conflict Forecasting**

_State | A_
> Conflict Forecasting CSO/AA is working on creating forecasting models to predict conflict outcomes such as interstate war, mass mobilization, and mass killings. These models utilize open-source data on political, social, and economic factors and employ statistical AI techniques like machine learning and clustering methods.

**Federal Procurement Data System (FPDS) Auto-Populate Bot**

_State | A_
> A bot was developed to automate data entry in the Federal Procurement Data System (FPDS), reducing workload for procurement staff and improving compliance with DATA Act reporting. Another bot was developed to automate closeout reminders for federal assistance grants, and plans are underway to automate receiving report validation and customer service inbox monitoring.

**Tailored Integration Logistics Management System (ILMS) User Analytics**

_State | A_
> The company A/LM plans to utilize the ILMS transactional data and planned transactions to create customized user experiences and analytics that cater to the specific needs of the users. By analyzing real system actions and clicks, they aim to gather valuable insights that can simplify user interactions and reduce the time taken to complete daily tasks.

**Verified Imagery Pilot Project**

_State | A_
> The response explains that the Bureau of Conflict and Stabilization Operations conducted a pilot project using a technology service called Sealr to verify the delivery of foreign assistance to conflict-affected areas where neither the U.S. Department of State nor their partner could access. Sealr utilizes blockchain encryption and artificial intelligence to secure and detect tampering of photographs taken on smartphones, making it a useful tool for remote monitoring of foreign assistance in dangerous or inaccessible areas.

**Automatic Detection of Authentic Material**

_State | CGFS_
> The Foreign Service Institute School of Language Studies is creating a tool to automatically find authentic texts in native languages that are classified by topic and proficiency level. This tool will be used to support the development of foreign language curriculum and language testing kits.

**Apptio**

_State | CSO_
> The Apptio Working Capital Fund (IRM/WCF) utilizes Apptio to invoice bureaus for combined services managed by the WCF. Apptio also allows for the creation of cost models, enabling bureaus to plan and allocate budgets for future fiscal years. Additionally, Apptio has the ability to predict future values using various formulas.

**ServiceNow AI-Powered Virtual Agent (Chatbot)**

_State | CSO_
> IRM's BMP Systems is looking to integrate ServiceNow's Virtual Agent into their applications to facilitate support and data requests for users. This AI-powered chatbot will be provided by ServiceNow's Platform as a Service.

**Automated Burning Detection**

_State | CSO_
> The Village Monitoring System program utilizes AI and machine learning to analyze moderate resolution commercial satellite imagery on a daily basis. By focusing on the near-infrared band, the program can detect anomalies or unusual patterns.

**Automated Damage Assessments**

_State | CSO_
> The Conflict Observatory program uses AI and machine learning to analyze satellite imagery and document war crimes and abuses in Ukraine. One of its main functions is to conduct automated damage assessments of various buildings, including critical infrastructure, hospitals, schools, and crop storage facilities.

**NLP for Foreign Assistance Appropriations Analysis**

_State | F_
> The main idea is that natural language processing (NLP) is being used for Foreign Assistance Appropriations Analysis (F/RA) to automate the extraction of earmarks and directives from the annual appropriations bill. This process used to be done manually before the implementation of NLP.

**eRecords M/L Metadata Enrichment**

_State | FSI_
> The Department's eRecords archive uses machine learning models to enhance metadata and aid in record discovery and review. These models include entity extraction, sentiment analysis, classification, and document type identification.

**Facebook Ad Test Optimization System**

_State | GPA_
> The main idea is that Facebook has developed a system called GPA that collects and analyzes data from multiple media sources to provide a comprehensive global view of media coverage. This system helps optimize Facebook's ad tests by providing up-to-date information on media coverage worldwide.

**Machine-Learning Assisted Measurement and Evaluation of Public Outreach**

_State | GPA_
> The response discusses the use of machine learning in GPA's production system to collect, analyze, and summarize the global digital content footprint of the Department. This technology assists in measuring and evaluating public outreach efforts more efficiently.

**Global Audience Segmentation Framework**

_State | GPA_
> The response mentions a prototype system that collects and analyzes daily media clips reports from around 70 Embassy Public Affairs Sections. This system is likely designed to help with global audience segmentation and understanding of different public affairs sections' media coverage.

**GPATools and GPAIX**

_State | GPA_
> GPATools and GPAIX are production systems developed by GPA for testing messages at a large scale across different foreign sub-audiences. The goal is to determine the most effective way to reach and engage with specific target audiences.

**Behavioral Analytics for Online Surveys Test (Makor Analytics)**

_State | IRM_
> The main idea of the response is that the Global Engagement Center (GEC) is conducting a Technology Testbed to test emerging technology applications against foreign disinformation and propaganda challenges. Makor Analytics, an AI quantitative research company, is involved in the pilot by using their proprietary behavioral analytics technology to analyze survey responses and provide target audience sentiment insights and recommendations. The pilot aims to provide additional information beyond self-reported data that reflects sentiment analysis in the country of interest.

**Fast Text Word Builder**

_State | IRM_
> Fast Text is an AI approach that helps identify similar terms and phrases based on a root word. It supports A&R in creating strong search queries for data collection.

**AI Capabilities Embedded in SMART**

_State | IRM_
> The SMART system on OpenNet has integrated AI capabilities to perform various tasks such as entity extraction, sentiment analysis, keyword extraction, and historical data analysis. These capabilities are used to identify objects within cables, analyze the sentiment in cables, extract keywords related to topics in cables, and provide recommendations for addressees and passlines when composing cables.

**Crisis Campaign Cable Analytics**

_State | M/SS_
> The main idea of the response is that Crisis Campaign Cable Analytics utilizes optical character recognition and natural language processing on Department cables to identify gaps and trends in crisis training. This aims to enhance post preparedness for crisis events.

**NLP to pull key information from unstructured text**

_State | PM_
> The main idea is that natural language processing (NLP) can be used to extract important information from unstructured text, specifically in this case, from multiple pages of a PDF document. The specific examples mentioned are extracting country names and agreement dates.

**K-Means clustering into tiers**

_State | PM_
> The main idea of the given response is to use k-means clustering to categorize countries into tiers based on data collected from open source and bureau sources. This approach allows for the grouping of countries based on similar characteristics or variables.

**Image Clustering**

_State | R_
> The main idea is that image clustering involves using a pretrained deep learning model to create image embeddings. These embeddings are then used in a hierarchical clustering process to find and group similar images together.

**forecasting**

_State | R_
> The main idea is that statistical models are being used to forecast future outcomes, such as COVID cases and violent events, by projecting expected results based on data analysis. These models have been applied to analyze tweets and predict the occurrence of violent events.

**Louvain Community Detection**

_State | R_
> The Louvain Community Detection algorithm is used to identify and group similar nodes in a social network into communities. This algorithm helps in clustering nodes based on their similarities and can be used to analyze the structure and relationships within a social network.

**TOPIQ**

_State | R_
> The TOPIQ GEC A&R's TOPIQ tool is designed to classify text into topics for analyst review and interpretation. It utilizes Latent Dirichlet Allocation (LDA), a natural language processing technique, to identify topics within a collection of documents and determine the probability of each document belonging to a specific topic.

**SentiBERTIQ**

_State | R_
> The SentiBERTIQ GEC A&R is an AI tool that uses deep contextual analysis to identify and extract subjective information from text. It was trained on a large dataset of labeled tweets in multiple languages and can assign sentiment and confidence intervals to text documents.

**Deepfake Detector**

_State | R_
> The response describes a deep learning model called Deepfake Detector that can classify an image as either real or fake based on the presence of a person's face. It specifically identifies deepfakes, which are synthetically generated faces often created using Generative Adversarial Networks (GANs).

**Topic Modeling**

_State | R_
> The main idea of the response is that topic modeling is a technique used to cluster text into themes based on the frequency of used words in documents. This technique has been applied to both digital media articles and social media posts, and it can be implemented using Python libraries.

**Text Similarity**

_State | R_
> The Text Similarity capability of GEC A&R identifies texts that are identical or very similar by calculating their cosine similarity. These texts are then grouped together based on their high cosine similarity and made available for analysts to review in more detail.

**Optical Character Recognition – text extraction**

_State | R_
> The main idea of the response is that Optical Character Recognition (OCR) can be used to extract text from images using standard Python libraries. The response also mentions that the inputs for this process have been websites, indicating that data can be collected from websites and the text can be extracted from images on those websites.

## Labor

**Audio Transcription**

_Labor_
> The main idea of the response is that audio transcription involves converting speech into written text. This process is done using natural language processing models and is primarily used for record-keeping purposes.

**Website Chatbot Assistant**

_Labor_
> The website chatbot assistant is designed to provide basic information about the program, offer guidance on who to contact for assistance, and can help users check the status of their petition cases. It serves as a helpful resource for users seeking information and support related to the program.

**Language Translation**

_Labor_
> The main idea is that language translation services are being provided for published documents and websites using natural language processing models.

**Automatic Data Processing Workflow with Form Recognizer**

_Labor_
> The main idea of the response is that there is an automatic data processing workflow in place that uses Form Recognizer technology to extract the necessary data from complex documents. This workflow eliminates the need for manual data extraction and streamlines the process.

**Text to Speech Conversion**

_Labor_
> The main idea of the response is that text to speech conversion can be enhanced through the use of neural networks, resulting in more realistic human-sounding applications. This is achieved by utilizing natural language processing models.

**Claims Document Processing**

_Labor_
> The main idea is to use custom natural language processing models to determine if a physician's note contains causal language. This process is part of claims document processing.

**Case Recording summarization**

_Labor_
> The main idea is that an open source large language model is used to summarize case recording documents that do not contain any personal identifiable information or sensitive data. The process involves human note takers reviewing the summaries, and the model is not hosted in the DOL technical environment.

**Data Ingestion of Payroll Forms**

_Labor_
> The main idea is that there is a custom machine learning model that can extract data from complex forms and tag the data entries to field headers. This model takes a document or scanned image of the form as input and produces a JSON response with key/value pairs extracted by running the form through the trained model.

**Electronic Records Management**

_Labor_
> The main idea of the response is that electronic records management is meeting the metadata standards set by NARA for permanent federal documents through the use of AI. This is achieved by using AI to identify data within the documents and also using natural language processing (NLP) to classify and summarize the documents.

**Scanner Data Product Classification**

_Labor_
> The Bureau of Labor Statistics (BLS) receives bulk data from corporations regarding the cost of goods and services they provide. To process this data efficiently, BLS is using machine learning to label the data with Entry Level Item (ELI) codes based on word frequency counts in item descriptions. Logistic regression is used to estimate the probability of each item being classified, and any classifications with low probabilities are flagged for human review.

**Official Document Validation**

_Labor_
> The response highlights the use of AI technology for validating official documents. It specifically focuses on the detection of mismatched addresses and garbled text in letters sent to benefits recipients.

**Form Recognizer for Benefits Forms**

_Labor_
> Form Recognizer for Benefits Forms is a custom machine learning model that can extract data from complex forms and tag the data entries to field headers. By inputting a document or scanned image of the form, the model generates a JSON response that includes key/value pairs extracted using the custom trained model.

**Automatic Document Processing**

_Labor_
> The main idea of the response is that there is a need for automatic processing of continuation of benefits forms in order to extract pre-defined selection boxes. This suggests that there is a desire to streamline the process of extracting specific information from these forms using automated methods.

**Expenditure Classification Autocoder**

_Labor_
> The Expenditure Classification Autocoder is a custom machine learning model that is used to assign expense descriptions from survey respondents to specific expense categories known as item codes. This model helps automate the process of classification and categorization of expenses for analysis and reporting purposes.

**DOL Intranet Website Chatbot Assistant**

_Labor_
> The DOL (Department of Labor) has implemented a chatbot assistant on their intranet websites to provide answers to common procurement questions and specific contract inquiries. This chatbot aims to assist employees in finding information quickly and efficiently.

**Hololens**

_Labor_
> The main idea is that inspectors can use Hololens AI to visually inspect high and unsafe areas without putting themselves in danger. This technology allows them to view and assess these areas from a safe location.

**Call Recording Analysis**

_Labor_
> The main idea of the response is the automatic analysis of recorded calls made to Benefits Advisors in the DOL Interactive Voice Response (IVR) center. This analysis likely involves evaluating the content and quality of the calls to gain insights and improve the efficiency or effectiveness of the service provided by the Benefits Advisors.

**OEWS Occupation Autocoder**

_Labor_
> The OEWS Occupation Autocoder is a tool that reads job titles in state submitted response files and assigns Standard Occupational Classification (SOC) codes along with their probabilities as recommendations for human coders. These codes above a certain threshold are then appended to the response files and sent back to states to assist with their SOC code assignment.

## USAID

**Long-term impacts of land-use/land-cover dynamics on surface water quality in Botswana’s reservoirs using satellite data and artificial intelligence methods: Case study of the Botswana’s Limpopo River Basin (1984-2019)**

_USAID | USAID | USAID/Bureau for Development, Democracy, and Innovation (DDI)_
> This study aims to analyze the long-term impacts of land-use and land-cover dynamics on surface water quality in Botswana's Limpopo River Basin. It will use satellite data and artificial intelligence methods to determine the relationships between land-use change, socioeconomic development indicators, climate change, and their effects on water quality and availability. The study will also develop empirical models for estimating water quality and mapping a water quality index using in-situ measurements and satellite imagery.

**Project Vikela**

_USAID | USAID | USAID/Bureau for Development, Democracy, and Innovation (DDI)_
> The main idea is that Project Vikela is using artificial intelligence (AI) to detect illegal rhino horn in airplane luggage X-Ray scanners. The goal is to prevent the smuggling of rhino horn by utilizing advanced technology.

**Media Early Warning System (MEWS)**

_USAID | USAID | USAID/Bureau for Development, Democracy, and Innovation (DDI)_
> The Media Early Warning System (MEWS) aims to identify narratives and trends in social media by detecting alterations in images and videos. The purpose of this system is to counteract malign narratives by monitoring and addressing them in a timely manner.

**Machine Learning for Peace**

_USAID | USAID | USAID/Bureau for Development, Democracy, and Innovation (DDI)_
> The main idea of the response is that Machine Learning for Peace Objective 1 is a part of the Illuminating New Solutions and Programmatic Innovations for Resilient Spaces (INSPIRES) initiative. It involves program activities and has a dedicated website for further information.

**Gender differentiated credit scoring**

_USAID | USAID | USAID/Bureau for Development, Democracy, and Innovation (DDI)_
> The University of California, Berkeley is creating a machine learning model to analyze gender differentiated credit scoring for Rappicard customers in Mexico. They aim to compare this model with Rappi's current method to evaluate if a gender differentiated approach can improve women's access to credit.

**Morogoro youth empowerment through establishment of social innovation (YEESI) lab for problem-centered training in machine vision**

_USAID | USAID | USAID/Bureau for Development, Democracy, and Innovation (DDI)_
> The main idea of the response is that a social innovation lab for a machine vision program will be established in Morogoro, Tanzania, to address agricultural problems faced by farmers in the region. The project aims to develop experts who can use machine vision to solve problems related to disease detection, weed classification, pest detection, crop seedlings, and crop vigor estimation. This will contribute to the improvement of agricultural practices and help farmers mitigate problems and improve crop performance.

**Breakthrough RESEARCH’s Social Media Listening**

_USAID | USAID | USAID/Bureau for Global Health (GH)_
> Breakthrough RESEARCH used social media listening and machine learning to analyze over 12,000 social media posts in Nigeria and examine how gender-related online conversations have changed in the past five years. By scraping publicly available social media content and using language detection, the algorithm categorized the posts into different topics, providing insights into conversation volume, misinformation, attitudes, and social norms. This approach allowed for a more comprehensive monitoring and tracking of social media conversations compared to traditional research methods in public health and SBC programs.

**Using ML for predicting treatment interruption among PLHIV in Nigeria**

_USAID | USAID | USAID/Bureau for Global Health (GH)_
> The main idea of the response is that machine learning (ML) is being used to predict the likelihood of treatment interruption among people living with HIV in Nigeria. The algorithm developed from the data is integrated into the Lafiya Management Information System (LAMIS) and used to provide more intensive follow-up support to patients at high risk of interrupting treatment. A qualitative assessment was also conducted to determine healthcare workers' perception of ML and identify additional support needed for incorporating ML into their routine work.

**Mali: AI predictions for the optimization of the allocation of the distribution of COVID-19 vaccines**

_USAID | USAID | USAID/Bureau for Global Health (GH)_
> In Mali, AI technology was utilized to create an AI model for distributing COVID-19 vaccines more efficiently. The model focused on prioritizing areas with high COVID-19 cases and pregnant/breastfeeding women using data from DHIS2. However, it should be noted that this model was only a proof-of-concept.

**Serbia: AI predictions for the utilization of hospital beds**

_USAID | USAID | USAID/Bureau for Global Health (GH)_
> AI technology was used in Serbia to predict bed occupancy at hospitals, with a median error of around 20%. This proof-of-concept model was developed to demonstrate the value of AI and will be followed by a focus on waiting list optimization for scheduled imaging diagnostics services as part of the national AI strategy.

**Indonesia: AI predictions for improving forecasts for TB drugs**

_USAID | USAID | USAID/Bureau for Global Health (GH)_
> In Indonesia, artificial intelligence (AI) technology will be utilized to create a forecasting model for tuberculosis (TB) drugs. This model aims to improve the accuracy of annual quantification exercises for the Ministry of Health (MoH) by integrating national data through the SatuSehat platform.

**NASA SERVIR - Mapping urban vulnerability using AI techniques**

_USAID | USAID | USAID/Bureau for Latin America and the Caribbean_
> NASA SERVIR is working on a project to enhance urban vulnerability assessment in major population centers. The focus is on developing techniques to utilize satellite imagery and artificial intelligence to map informal settlements, with the aim of creating replicable methods for future use.

**NASA SERVIR - Using artificial intelligence to forecast harmful algae blooms in Lake Atitlán, Guatemala**

_USAID | USAID | USAID/Bureau for Latin America and the Caribbean_
> NASA SERVIR is using artificial intelligence to predict harmful algae blooms in Lake Atitlán, Guatemala. The system uses machine learning and data from Earth observations and weather models to provide daily forecasts, which are used by Lake Authorities to inform their Harmful Algal Blooms Alert System. The project is supported by National Geographic and Microsoft.

**NASA SERVIR - Bias Correcting Historical GEOGloWS ECMWF Streamflow Service (GESS) data using Machine Learning (ML) Techniques**

_USAID | USAID | USAID/Bureau for Latin America and the Caribbean_
> The main idea of the response is that NASA SERVIR's GEOGloWS ECMWF Streamflow Service (GESS) uses machine learning techniques to bias correct historical flow data. The service serves as a platform for collaboration and engagement among different sectors to provide actionable water information. The application of Earth Observations (EO) has been a significant element in creating a system that forecasts flow on every river worldwide and provides a 40-year simulated historical flow.

## Transportation

**Determining Surface Winds with Machine Learning Software**

_Transportation | Federal Aviation Administration | ANG_
> The use of machine learning software has been successful in analyzing camera images of a wind sock to accurately determine surface wind speed and direction in remote areas without weather observing sensors. This AI capability proves to be a reliable method for obtaining crucial weather information in inaccessible locations.

**Remote Oceanic Meteorological Information Operations (ROMIO)**

_Transportation | Federal Aviation Administration | ANG_
> The main idea of ROMIO is to assess the possibility of sending convective weather information to aircraft flying over the ocean and remote areas. This is achieved by using converted weather satellite data, lightning and weather prediction model data to identify areas of thunderstorm activity and cloud top heights. Artificial intelligence is also employed to enhance the accuracy of the output by comparing it with ground truth data from previous activity.

**Surface Report Classifiier (SCM/Auto-Class)**

_Transportation | Federal Aviation Administration | ATO_
> The Surface Report Classifier (SCM/Auto-Class) is a system that categorizes surface incident reports based on event type, including Runway Incursion, Runway Excursion, and Taxiway Incursion/Excursion. It also further classifies runway incursions based on severity type, ranging from Category A to E.

**JASC Code classification in Safety Difficulty Reports (SDR)**

_Transportation | Federal Aviation Administration | AVS_
> The response explains that the team at George Mason University collaborated with AVS employees to use Natural Language Processing (NLP) and Machine Learning to predict JASC codes from narrative descriptions in safety difficulty reports (SDR). This method can be used to verify the accuracy of SDR entries or assign codes when they are missing.

**Regulatory Compliance Mapping Tool**

_Transportation | Federal Aviation Administration | AVS_
> The AVS International office needs to comply with ICAO Standards and Recommended Practices (SARPs) and must find means of compliance evidence scattered across thousands of pages of documents. The Regulatory Compliance Mapping Tool (RCMT) processes these documents through Natural Language Processing (NLP) to extract the meaning of the text and uses a recommender system to establish matches between the SARPs and FAA text.

**Course Deviation Identification for Multiple Airport Route Separation (MARS)**

_Transportation | Federal Aviation Administration | Aviation Safety (AVS)_
> The MARS program is working on developing a safety case for reducing separation standards between PBN routes in terminal airspace, which could help deconflict airports in high-demand areas. To build collision risk models, they need to identify rare events where aircraft fail to navigate procedures correctly, and prior work has used Machine Learning to filter incident data and identify similar events on departure procedures.

**Offshore Precipitation Capability (OPC)**

_Transportation | Federal Aviation Administration | NextGen (ANG)_
> The Offshore Precipitation Capability (OPC) uses various data sources including weather radar, lightning networks, satellite, and numerical models to create a radar-like representation of precipitation. By applying machine learning techniques using extensive satellite and model data, OPC enhances the accuracy of identifying and predicting the location and intensity of precipitation areas.

**Automatic Track Change Detection Demonstration and Analysis**

_Transportation | Federal Railroad Administration | Office of Research, Development and Technology_
> The response describes a deep neural network called DeepCNet that can identify and classify various features related to railroad tracks, such as fasteners and ties. The network is used for "change detection" applications, where it can detect and notify any changes in the track's status or between different inspections based on geolocation.

**Crushed Aggregate Gradation Evaluation System**

_Transportation | Federal Railroad Administration | Office of Research, Development and Technology_
> The response describes a system that uses deep learning computer vision algorithms to analyze the particle size grading of crushed aggregate. It takes images of ballast cross sections as input and produces a ballast fouling index as output.

**Development of Predictive Analytics Using Autonomous Track Geometry Measurement System (ATGMS) Data**

_Transportation | Federal Railroad Administration | Office of Research, Development and Technology_
> The development of predictive analytics using Autonomous Track Geometry Measurement System (ATGMS) data involves using machine-learning processes to analyze, predict, and report track locations of concern. By leveraging track geometry measurements and exceptions, an inspection report is generated that includes the trending of track geometry measures and the time to failure, helping with maintenance and safety limits.

**Automated Delay detection using voice processing**

_Transportation | Federal Transportation Administration | ATO_
> Automated delay detection using voice processing is necessary to accurately track and account for delays in air traffic control (ATC) and aircraft interactions. The current reporting system lacks the ability to detect certain delay events like vectoring, and voice detection would enable automated detection of these events.

**Machine Learning for Occupant Safety Research**

_Transportation | National Highway Traffic Safety Administration | NSR Human Injury Research Division_
> The main idea of the response is that machine learning, specifically deep learning, can be used to predict crash parameters such as Delta-V and PDOF directly from real-world crash images. This eliminates the need for manual calculations or estimations by crash examiners and provides rapid results for improved efficiency.

**Machine Learning for Occupant Safety Research**

_Transportation | National Highway Traffic Safety Administration | NSR Human Injury Research Division_
> The main idea of the response is that machine learning techniques, specifically deep learning models, can be used to predict head kinematics in vehicle crashes using videos. This is useful when there are no sensors available or when sensor data is of low quality, and the output of the model can be used to predict the likelihood of injury based on angular velocity.

**PHMSA Rule Making**

_Transportation | Pipeline and Hazardous Materials Safety Administration (PHMSA | PHMSA Office of Chief Counsel (PHC)_
> The response states that PHMSA plans to use ChatGPT, an artificial intelligence system, to support the rulemaking processes. ChatGPT will be used to analyze the sentiment and relevance of comments on proposed rules, provide a synopsis of the comments, catalog them, and identify duplicate comments. This is expected to bring efficiency, reduce effort, and enable scalability in handling public scrutiny or interest in rulemaking.

## SSA

**Medicare Part D Subsidy Model**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The Medicare Part D Subsidy Model utilizes machine learning to identify and highlight cases that may have incorrect Medicare Part D subsidies. These flagged cases are then reviewed by technicians for further assessment.

**Modernized Development Worksheet (MDW)**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The Modernized Development Worksheet (MDW) is a process that utilizes artificial intelligence (AI) to analyze textual data in claim development tasks. Through natural language processing, the AI categorizes the data into workload topics, making it easier for technicians to review the information quickly.

**SSI Redetermination Model**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The SSI Redetermination Model utilizes machine learning to pinpoint cases of supplemental security income that are likely to have significant overpayments due to alterations in financial eligibility. These cases are then flagged for review by technicians.

**PATH Model**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The PATH Model is a machine learning system that predicts which cases are likely to receive an allowance at the hearing level. It then refers these cases to administrative law judges or senior adjudicators for prioritized review, potentially speeding up the decision-making process.

**Pre-Effectuation Review / Targeted Denial Review Models**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The response mentions two review models, the Pre-Effectuation Review and the Targeted Denial Review, that utilize machine learning to identify cases that are more likely to have errors in determining disability eligibility. These cases are then referred for quality review checks.

**Anomalous iClaim Predictive Model**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The main idea of the response is that the anomalous iClaim predictive model is a machine learning model used to identify high-risk iClaims. These claims are then sent for further review before any further action is taken.

**Rep Payee Misuse Model**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The Rep Payee Misuse Model is a machine learning model that predicts the likelihood of representative payees misusing resources. It identifies cases that require further examination by a technician, potentially helping to prevent misuse of funds.

**CDR Model**

_SSA | SSA | Office of Analytics, Review, and Oversight _
> The CDR (Continuing Disability Review) Model is based on machine learning and aims to identify disability cases that have a higher chance of medical improvement. It uses this information to flag these cases for a review to assess if the individuals still qualify for disability benefits.

**Insight**

_SSA | SSA | Office of Analytics, Review, and Oversight; Office of Hearing Operations, Office of Disability Systems_
> Insight is decision support software used by Disability Program adjudicators to improve the quality, speed, and consistency of their decision making. It analyzes case data to provide real-time alerts and reference information, as well as interactive tools to streamline their work. The software utilizes natural language processing and artificial intelligence techniques to power its features.

**Intelligent Medical Language Analysis Generation (IMAGEN)**

_SSA | SSA | Office of Disability Determinations, Office of Disability Information Systems_
> The main idea of the response is that IMAGEN is an IT modernization product that will provide tools and services to improve the efficiency and consistency of disability determinations and decisions. It will transform text to data and leverage machine learning technologies to support other agency initiatives such as fraud prevention.

**Duplicate Identification Process (DIP)**

_SSA | SSA | Office of Disability Information Systems,  Office of Hearing Operations, Office of Appellate Operations_
> The Duplicate Identification Process (DIP) aims to assist users in efficiently identifying and marking duplicate cases, thereby reducing the time spent on reviewing cases for hearings. DIP utilizes image recognition technology powered by artificial intelligence software to accurately identify duplicates according to the policy set by the Social Security Administration (SSA).

**Handwriting recognition from forms**

_SSA | SSA | Office of Disability Information Systems,  Office of Hearing Operations, Office of Appellate Operations_
> The main idea of the response is that handwriting recognition from forms AI is used to perform OCR (Optical Character Recognition) on handwritten entries on specific standard forms. This use case supports both Robotic Process Automation efforts and can also be used as a standalone application.

**Quick Disability Determinations Process**

_SSA | SSA | Office of Retirement of Disability Programs _
> The Quick Disability Determinations (QDD) process uses a computer-based predictive model to identify cases where a positive disability determination is likely and medical evidence is easily accessible. By identifying these cases early on, the Social Security Administration can prioritize them and expedite the processing. The Agency continually updates and improves the QDD model to better identify strong candidates for faster processing.

**Mobile Wage Reporting (MOBWR)**

_SSA | SSA | Office of Systems_
> Mobile Wage Reporting (MOBWR) is a technology that utilizes artificial intelligence (AI) to extract text and data from scanned images or documents containing pay stubs or payroll information. This allows for quicker processing and analysis of the information.

## Treasury

**Large Corporate Compliance**

_Treasury_
> The main idea is that Large Corporate Compliance is a machine learning model specifically designed for classifying corporate taxes. It implies that the model utilizes machine learning techniques to accurately categorize and manage corporate tax data.

**NRP Redesign**

_Treasury_
> The NRP Redesign aims to implement innovative active learning methods that would estimate compliance baselines more efficiently, supporting tax gap estimation, improper payments reporting, workload identification and selection models, and policy analysis. However, the system inputs depend on existing NRP data that must meet a certain level of precision and quality for satisfactory data output.

**Inventory Item Replenishment MLR Modeling POC - Phase 2**

_Treasury_
> The response mentions the completion of phase 2 of a project involving the creation and assessment of a multiple linear regression model. This model aims to predict if an inventory item will be replenished within the standard Need By time of 128 days.

**Collection Voice Bot**

_Treasury_
> The main idea is that the NLU model will be integrated into the Automated Collections IVR (ACI) menu. It will analyze customer speech input and determine the intent, guiding the taxpayer to the appropriate call path based on that intent.

**Evaluate Multilingual BERT for Software Translation Use Case Evaluations**

_Treasury_
> The project is evaluating the effectiveness of training a multilingual BERT model on IRS corpora and using it to evaluate software translation of IRS content. The evaluation will use COMET, ROGUE, and BLEU measures, and will also assess the model's performance in classifying English-Only and Spanish-Only content.

**Collection Chat Bot**

_Treasury_
> The main idea is that the Natural Language Understanding (NLU) model will be integrated into the eGain intent engine. This NLU will analyze customer typed text input and match it to a specific intent, allowing it to provide the relevant knowledge article in response.

**Line Anomaly Recommender**

_Treasury_
> The Line Anomaly Recommender use case aims to find a model that combines two recommender system models in order to evaluate compliance risk and detect abnormal tax returns and line-item values. The pipeline capabilities provided by this model can enhance the IRS LB&I reviewers' understanding with the help of advanced deep learning techniques for identifying anomalies.

**LB&I Text Analytics (including Appeals Case Management)**

_Treasury_
> The main idea is that LB&I (Large Business and International) Text Analytics has developed TaxBERT, a tax domain-specific BERT model, by training it using a large number of tax-related documents. This model can be used for targeted analytics in the tax domain.

**SBSE Issue Recommender**

_Treasury_
> The SBSE Issue Recommender has developed an AI-based recommender system that can detect potential non-compliance issues. This system improves the efficiency and scalability of training returns selection and field work processes.

**Large Partnership Compliance**

_Treasury_
> The main idea of Large Partnership Compliance is to use a machine learning model to analyze partnership data and assess the risk of potential non-compliance. This model helps in categorizing and scoring the data based on the level of risk involved.

**Projected Contract Award Date Web App**

_Treasury_
> The IRS has developed a web app that uses a machine learning model to predict when procurement requests will become signed contracts. The tool provides valuable insight for the IRS and other federal agencies on when contracts are likely to be signed, potentially impacting $600 billion in government contracts.

**Inventory Item Replenishment MLR Modeling POC - Phase 1**

_Treasury_
> The main idea of this response is to build and evaluate a multiple linear regression model to predict whether an inventory item will be received before or after the need by date. The goal is to determine the likelihood of receiving items on time in the future.

## GSA

**Service Desk Virtual Agent (Curie)**

_GSA_
> The Service Desk Virtual Agent, known as Curie, is a chatbot that utilizes machine learning to offer predictive responses during chats. This virtual assistant is designed to enhance the customer service experience for employees seeking IT support by providing relevant information from knowledge-based articles.

**City Pairs Program Ticket Forecast and Scenario Analysis Tools**

_GSA_
> The City Pairs Program Ticket Forecast and Scenario Analysis Tools use segment-level City Pair Program air travel purchase data to generate predictions for air travel purchases in the current and upcoming fiscal year. These forecasts are provided at different levels, such as DOD vs Civilian, Agency, and Region.

**Document Workflow / Intelligent Data Capture and Extraction**

_GSA_
> The General Services Administration (GSA) is working on improving their document workflow platform to be more accurate and scalable. They aim to implement intelligent data capture and extraction techniques to efficiently transfer important data from PDF files to the appropriate processes, workflows, or decision engines.

**Category Taxonomy Refinement Using NLP**

_GSA_
> The response describes a method called Category Taxonomy Refinement using Natural Language Processing (NLP) to extract tokens from product descriptions. This technique helps accurately identify and categorize products to determine their intended markets based on Product Service Codes (PSCs).

**Service Desk Generic Ticket Classification**

_GSA_
> The main idea is that the company is developing a model to classify generic Service Desk tickets and automatically route them to the appropriate team. This will automate the current manual process of re-routing tickets. The initial focus of the model will be on the top 5 most common ticket types.

**Acquisition Analytics**

_GSA_
> The main idea is that acquisition analytics uses detailed transaction data to classify each transaction within the Government-wide Category Management Taxonomy. This helps in organizing and managing government acquisitions more effectively.

**Key KPI Forecasts for GWCM**

_GSA_
> The main idea is that Key KPI Forecasts for GWCM takes historical data to create near-term forecasts for the upcoming fiscal year. The pilot program is currently focused on total agency/category spend and if successful, the same methodology can be applied to other KPIs.

**Solicitation Review Tool (SRT)**

_GSA_
> The Solicitation Review Tool (SRT) uses SAM.gov data to compile a database of Information and Communications Technology (ICT) solicitations. It then applies machine learning algorithms, starting with a Natural Language Processing model, to determine if a solicitation contains compliance language. Agencies are asked to validate the SRT predictions, and random manual reviews are conducted by GSA monthly.

**IAE FSD CCAI Virtual Agent**

_GSA_
> The main idea of the response is that the virtual agent, named SAM, uses manual learning and natural language processing to understand customer needs and provide appropriate responses. It is part of the IAE FSD CCAI system.

**Contract Acquisition Lifecycle Intelligence (CALI)**

_GSA_
> The main idea of the response is that CALI is an automated machine learning tool that is used to evaluate vendor proposals against solicitation requirements in order to support the Source Selection process. It processes and analyzes documents in four key areas and allows evaluation members to review and submit evaluation results. CALI is currently being trained with sample data from the EULAs under the Multiple Award Schedule program.

**Chatbot for Federal Acquisition Community**

_GSA_
> The main idea is that the introduction of a chatbot will help the GSA FAS NCSC streamline customer experience and automate answering commonly asked questions. This will reduce the need for live chat agents and allow resources to be dedicated to other customer service initiatives, while still giving customers the option to connect with a live agent if they prefer.

**Classifying Qualitative Data**

_GSA_
> USAGov and USAGov en Español gather qualitative data from various sources such as survey comments, web searches, and call center chat transcripts. This data is classified by topic to identify areas that require product updates or enhancements.

## Justice

**Drug Signature Program Algorithms**

_Justice | Drug Enforcement Administration_
> The DEA's Special Testing and Research Laboratory employs AI and machine learning techniques to classify the geographic origin of samples in their Heroin and Cocaine signature programs. They use statistical analysis tools to automatically detect anomalies and low confidence results.

**Complaint Lead Value Probability**

_Justice | Federal Bureau of Investigation_
> The Complaint Lead Value Probability Threat Intake Processing System (TIPS) database employs artificial intelligence algorithms to efficiently identify and process actionable tips. By prioritizing immediate threats through AI triage, the system assists FBI field offices and law enforcement in responding to the most serious threats promptly. Tips with the highest algorithm score receive priority for human review.

**Intelligent Records Consolidation Tool**

_Justice | Justice Management Division_
> The Office of Records Management Policy has developed an Intelligent Records Consolidation Tool that uses AI and Natural Language Processing to assess the similarity of records schedules. The tool generates clusters of similar items, reducing the time spent on manual review by the Records Manager. The tool also provides recommendations for schedule consolidation and can be applied to other domains that require text similarity analysis.

**Privileged Material Identification**

_Justice | Tax Division_
> The response discusses a specific application that scans documents to identify attorney/client privileged information. The application uses keyword input provided by the system operator to perform this task.

## EPA

**Enforcement Targeting**

_EPA_
> The response highlights that the EPA's Office of Compliance and the University of Chicago collaborated to create a proof-of-concept aimed at enhancing the enforcement of environmental regulations through facility inspections. Through the use of predictive analytics, the initiative achieved a significant 47% improvement in identifying violations of the Resource Conservation and Recovery Act.

**Use of random forest model to predict exposure pathways**

_EPA_
> The study used a random forest model to predict the probability of a chemical being associated with different exposure pathways. They then created a consensus model that combined multiple predictors of exposure to estimate intake rates for a large number of chemicals, even with limited exposure information. The approach identified certain chemicals with potentially high intake rates, while also providing confidence intervals for the estimated intake rates of other compounds.

**Records Categorization**

_EPA_
> The records management technology team is utilizing machine learning to forecast the retention schedule for records. This model will be integrated into a records management application to assist users in applying the appropriate retention schedules for new records.

## Education

**Aidan**

_Education | Education | Office of Federal Student Aid (FSA)_
> Aidan FSA is a virtual assistant that utilizes natural language processing to provide information and answer commonly asked questions about financial aid. Over the span of two years, Aidan has assisted over 2.6 million customers and processed more than 11 million user messages.
